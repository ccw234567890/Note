# 🧠 PyTorch 数学运算核心笔记

> [!abstract|bg-blue] **本页核心内容概览**
> - **基础运算**: 逐元素的加、减、乘、除。
> - **矩阵乘法**: 区分 `torch.mm`, `torch.matmul` 和 `@` 的使用场景。
> - **幂运算**: `pow`, `sqrt`, `rsqrt` 以及指数 `exp` 和对数 `log`。
> - **近似取整**: `floor`, `ceil`, `round`, `trunc`, `frac` 的区别。
> - **数值裁剪**: `clamp` 及其在梯度裁剪中的重要应用。

---

## 📘 1. 基础运算 (Element-wise Operations)

> [!NOTE|bg-green] **按元素运算**
> PyTorch 的基础算术运算 (`+`, `-`, `*`, `/`) 默认都是按元素 (element-wise) 进行的。这意味着张量中对应位置的元素会分别进行计算。

> [!EXAMPLE|bg-gray] **代码示例: 运算符 vs 函数**
> ```python
> # 创建两个随机张量
> a = torch.rand(3, 4)
> b = torch.rand(3, 4)
> 
> # 使用运算符
> c = a + b 
> 
> # 使用函数
> d = torch.add(a, b)
> 
> # 验证两者结果是否完全相等
> torch.all(torch.eq(c, d))
> # Out: tensor(1, dtype=torch.uint8) -> 表示结果为 True
> ```
> **结论**: 使用 `a + b` 这样的运算符和 `torch.add(a, b)` 这样的函数是完全等价的。选择哪种取决于代码风格和可读性偏好。

---

## 📗 2. 矩阵乘法 (Matrix Multiplication)

这是神经网络中最为核心的运算之一。

### a) 2D 矩阵乘法

> [!INFO|bg-cyan] **三种实现方式**
> 1.  `torch.mm(a, b)`: **最严格**，只接受两个 2D 张量。
> 2.  `torch.matmul(a, b)`: **更通用**，支持 2D 和更高维度的广播矩阵乘法。
> 3.  `a @ b`: `torch.matmul` 的运算符简写，**代码最简洁**。

> [!EXAMPLE|bg-gray] **2D Matmul 代码**
> ```python
> a = torch.full((2,2), 3.)
> b = torch.ones(2,2)
> 
> # 三种方式结果相同
> out1 = torch.mm(a, b)
> out2 = torch.matmul(a, b)
> out3 = a @ b
> 
> # out1, out2, out3 均为:
> # tensor([[6., 6.],
> #         [6., 6.]])
> ```

### b) 高维张量 (Batched) 的矩阵乘法

> [!WARNING|bg-red] **`torch.mm` 的局限性**
> `torch.mm` **无法处理** 包含批次维度（Batch Dimension）的 3D 或更高维度的张量。强行使用会直接报错 `RuntimeError`。

> [!SUCCESS|bg-green] **正确方式: `torch.matmul` 或 `@`**
> 当张量维度 > 2 时，`torch.matmul` 会将最后两个维度视为矩阵进行乘法，而将前面的维度视为批次维度，并自动应用**广播 (Broadcasting) 机制**。

> [!EXAMPLE|bg-gray] **Batched Matmul 代码与错误分析**
> ```python
> # 场景: a的批次维度为(4,3), b的批次维度为(4,1)
> a = torch.rand(4, 3, 28, 64)
> b = torch.rand(4, 1, 64, 32)
> 
> # 正确: b的维度(4,1)可以广播到(4,3)
> c = torch.matmul(a, b) 
> c.shape # Out: torch.Size([4, 3, 28, 32])
> 
> # --- 错误示范 ---
> # a的批次维度(4,3)与b的批次维度(4,)不匹配且无法广播
> a = torch.rand(4, 3, 28, 64)
> b = torch.rand(4, 64, 32) # b缺少一个批次维度
> 
> # torch.matmul(a,b) # 会导致 RuntimeError
> ```
> **核心要点**: 在处理带批次（如图像、文本序列）的数据时，请始终使用 `torch.matmul` 或 `@`。

---

## 📙 3. 幂、指数、对数与平方根

> [!NOTE|bg-purple] **Power & Exponential Operations**
> 这些函数同样是按元素进行运算的。

> [!EXAMPLE|bg-gray] **代码示例**
> ```python
> a = torch.full((2,2), 9.)
> 
> # --- Power & Sqrt ---
> a.pow(2)   # or a**2,   结果是 [[81., 81.], [81., 81.]]
> a.sqrt()   # or a**0.5, 结果是 [[3., 3.], [3., 3.]]
> a.rsqrt()  # 平方根的倒数 1/sqrt(a), 结果是 [[0.3333, ...]]
> 
> # --- Exp & Log ---
> b = torch.ones(2,2)
> c = torch.exp(b) # 计算 e^1, 结果是 [[2.7183, ...]]
> torch.log(c)     # 计算 ln(c), 结果还原为 [[1., 1.], [1., 1.]]
> ```
> **要点**: `exp` 和 `log` 是互逆运算。`rsqrt()` 在某些算法中（如 Adam 优化器）比 `1/a.sqrt()` 计算效率更高。

---

## 📒 4. 近似与取整 (Approximation)

> [!INFO|bg-orange] **常用取整函数**
> - `.floor()`: **向下**取整 (地板)。`3.9 -> 3.0`
> - `.ceil()`: **向上**取整 (天花板)。`3.1 -> 4.0`
> - `.trunc()`: **截断**，直接去掉小数部分。`3.9 -> 3.0`
> - `.frac()`: 只取**小数部分**。`3.9 -> 0.9`
> - `.round()`: **四舍五入**。

> [!QUESTION|bg-blue] **`round(x.5)` 如何工作?**
> **答案**: PyTorch 的 `round()` 采用 "四舍五入到最近的偶数" (*Round half to even*) 的策略，这是 IEEE 754 浮点数标准。
> - `torch.tensor(3.5).round()`  结果是 `4.0` (4是偶数)
> - `torch.tensor(2.5).round()`  结果是 `2.0` (2是偶数)

---

## 📕 5. 数值裁剪 (Clamp)

> [!TIP|bg-pink] **重要应用: 梯度裁剪 (Gradient Clipping)**
> `clamp` 函数是稳定神经网络训练的利器。在训练过程中，如果梯度值过大（称为“梯度爆炸”），会导致模型参数更新过猛，破坏学习过程。`clamp` 可以将梯度强制限制在一个安全的范围内。

> [!EXAMPLE|bg-gray] **Clamp 代码示例**
> ```python
> grad = torch.rand(2, 3) * 15 # 创建一个 0-15 之间的随机梯度
> # grad 可能是: 
> # tensor([[14.8737, 10.1571,  4.4872],
> #         [11.3591,  8.9101, 14.0524]])
> 
> # 1. 只设置下限 min
> # 将所有小于 10 的值提升到 10
> grad.clamp(10)
> # Out:
> # tensor([[14.8737, 10.1571, 10.0000],
> #         [11.3591, 10.0000, 14.0524]])
> 
> # 2. 设置上下限 [min, max]
> # 将所有值裁剪到 [0, 10] 区间内
> grad.clamp(0, 10)
> # Out:
> # tensor([[10.0000, 10.0000,  4.4872],
> #         [10.0000,  8.9101, 10.0000]])
> ```