# 第四章 期望 (EXPECTATION)

本章我们将学习一个关于随机变量最重要的总结性特征：**期望**。

**章节概览：**
* 4.1 随机变量的期望 (The Expectation of a Random Variable)
* 4.2 期望的性质 (Properties of Expectations)
* 4.3 方差 (Variance)
* 4.4 矩 (Moments)
* 4.5 均值和中位数 (The Mean and the Median)
* 4.6 协方差和相关性 (Covariance and Correlation)
* 4.7 条件期望 (Conditional Expectation)
* 4.8 效用 (Utility)
* 4.9 补充练习 (Supplementary Exercises)

---

## 4.1 随机变量的期望 (The Expectation of a Random Variable)

[!note] 为什么需要期望？
一个随机变量 $X$ 的分布包含了关于 $X$ 的所有概率信息。但在实际应用中，完整的分布信息（例如一个复杂的p.d.f.或p.f.）通常过于繁琐，不便于展示和比较。

因此，我们需要一些**概括性总结（Summaries）**来描述分布的关键特征。**期望值（Expected Value）**（也称为**均值**或**平均值**）就是其中最重要的一种。它告诉我们 $X$ 的“平均”取值，或者说我们预期 $X$ 会在哪个值附近。

### 离散分布的期望 (Expectation for a Discrete Distribution)

[!example] 例子 4.1.1 - 股票的公平价格 (Fair Price for a Stock)
一个投资者正在考虑是否以每股 $18 的价格投资某只股票一年。一年后，股票的价值将变为 $18 + X$ 美元，其中 $X$ 是这一年内股价的变化量。

目前 $X$ 是未知的（是一个随机变量）。投资者希望计算 $X$ 的“平均值”，以便将其与把 $18 存入银行（年利率5%）所能获得的收益进行比较。

[!abstract] 期望的直观理解
随机变量的均值（或期望）的直观概念是：**它是随机变量所有可能取值的加权平均值，而权重就是这些值出现的概率。**

[!example] 例子 4.1.2 - 股价变动 (Stock Price Change)
假设例子 4.1.1 中的股价变化 $X$ 只能取四个值：-2, 0, 1, 4。
其概率（p.f.）如下：
* $Pr(X = -2) = 0.1$
* $Pr(X = 0) = 0.4$
* $Pr(X = 1) = 0.3$
* $Pr(X = 4) = 0.2$

根据“加权平均”的思想， $X$ 的期望值为：
$$
(-2) \times (0.1) + (0) \times (0.4) + (1) \times (0.3) + (4) \times (0.2) = -0.2 + 0 + 0.3 + 0.8 = 0.9
$$
现在投资者可以进行比较：
* **投资股票**：预期的价格变化 $E(X)$ 是 $0.9。
* **存入银行**：$18 在 5% 利率下存放一年的利息是 $18 \times 0.05 = 0.9$。

由于预期收益相同，从这个角度看，$18 的价格似乎是“公平”的。

---

[!definition] 定义 4.1.1 - 有界离散随机变量的均值 (Mean of Bounded Discrete Random Variable)
设 $X$ 是一个有界（即其取值范围有限）的离散随机变量，其概率质量函数（p.f.）为 $f$。$X$ 的**期望**（Expectation），记作 $E(X)$，定义如下：
$$
E(X) = \sum_{\text{All } x} xf(x)
$$
$E(X)$ 也被称为 $X$ 的**均值 (Mean)** 或 **期望值 (Expected Value)**。

[!tip] 注意
在例子 4.1.2 中，$E(X) = 0.9$。请注意，0.9 **并不是** $X$ 可能取的值（-2, 0, 1, 4）中的任何一个。对于离散随机变量，期望值不一定是其可能的输出值，这是很典型的情况。

[!example] 例子 4.1.3 - 伯努利随机变量 (Bernoulli Random Variable)
假设 $X$ 服从参数为 $p$ 的伯努利分布。这意味着 $X$ 只能取两个值：
* $Pr(X = 1) = p$
* $Pr(X = 0) = 1 - p$

$X$ 的均值是：
$$
E(X) = (0) \times (1-p) + (1) \times p = p
$$
一个伯努利随机变量的期望就是它取值为 1 的概率。

---

[!note] 扩展到无界离散分布
如果一个随机变量可以取无穷多个值（例如，所有正整数），我们就需要更小心地定义期望。

[!definition] 定义 4.1.2 - 一般离散随机变量的均值 (Mean of General Discrete Random Variable)
设 $X$ 是一个离散随机变量，p.f. 为 $f$。
首先，我们考察下面两个和：
$$
\sum_{\text{Positive } x} xf(x) \quad \text{和} \quad \sum_{\text{Negative } x} xf(x)
$$
**期望存在的条件**：**至少有一个**和是有限的。
* 如果这个条件满足，$X$ 的均值（期望）就被认为**存在**，并定义为：
    $$
    E(X) = \sum_{\text{All } x} xf(x)
    $$
* 如果上述两个和**都是无穷大**，那么 $E(X)$ **不存在**。

[!warning] 为什么期望可能不存在？
如果正值部分的总和为 $+\infty$，负值部分的总和为 $-\infty$，那么 $E(X)$ 的总和 $\sum xf(x)$ 就是一个 "$\infty - \infty$" 形式的未定式。
在微积分中我们知道，这种级数的和**不收敛**，或者其值取决于你对项重新排序的方式。我们不希望期望值依赖于我们加总的顺序，因此在这种情况下我们说期望**不存在**。

* 如果一个和是无限的，另一个是有限的（比如 0），那么期望就是无限的（例如 $E(X) = \infty$）。

[!example] 例子 4.1.4 - 均值不存在 (The Mean of X Does Not Exist)
设 $X$ 的 p.f. 为：
$f(x) = \frac{1}{2|x|(|x|+1)}$ ，对于 $x = \pm 1, \pm 2, \pm 3, \ldots$
（可以验证 $f(x)$ 是一个合法的 p.f.，即 $\sum f(x) = 1$）

我们来检查两个和：
1.  **正值部分**：
    $\sum_{x=1}^{\infty} x f(x) = \sum_{x=1}^{\infty} x \frac{1}{2x(x+1)} = \sum_{x=1}^{\infty} \frac{1}{2(x+1)} = \frac{1}{2} (\frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \cdots)$
    这是一个发散的调和级数，所以和为 $\infty$。
2.  **负值部分**：
    $\sum_{x=-1}^{-\infty} x f(x) = \sum_{x=-1}^{-\infty} x \frac{1}{2|x|(|x|+1)} = \sum_{k=1}^{\infty} (-k) \frac{1}{2k(k+1)} = \sum_{k=1}^{\infty} -\frac{1}{2(k+1)}$
    这个和为 $-\infty$。

因为正负两部分的和都是无限的，所以 $E(X)$ **不存在**。

[!example] 例子 4.1.5 - 无穷大的均值 (An Infinite Mean)
设 $X$ 的 p.f. 为：
$f(x) = \frac{1}{x(x+1)}$ ，对于 $x = 1, 2, 3, \ldots$
（$X$ 只取正值，所以负值部分的和为 0，是有限的）

我们检查正值部分：
$E(X) = \sum_{x=1}^{\infty} x f(x) = \sum_{x=1}^{\infty} x \frac{1}{x(x+1)} = \sum_{x=1}^{\infty} \frac{1}{x+1} = \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \cdots = \infty$
因为负值部分为 0（有限），所以期望**存在**，且 $E(X) = \infty$。

---
### 连续分布的期望 (Expectation for a Continuous Distribution)

[!note]
对于连续随机变量，我们将加权平均的思想从**求和 (Summation)** 推广到**积分 (Integration)**。

[!definition] 定义 4.1.3 - 有界连续随机变量的均值 (Mean of Bounded Continuous Random Variable)
设 $X$ 是一个有界连续随机变量，其概率密度函数（p.d.f.）为 $f$。$X$ 的期望 $E(X)$ 定义如下：
$$
E(X) = \int_{-\infty}^{\infty} xf(x)dx
$$

[!example] 例子 4.1.6 - 预期故障时间 (Expected Failure Time)
某电器的最大寿命为 1 年。其故障时间 $X$ 是一个连续随机变量，p.d.f. 为：
$f(x) = 2x$ ，对于 $0 < x < 1$（其他地方为 0）

$X$ 的期望（预期故障时间）为：
$$
E(X) = \int_{0}^{1} x(2x)dx = \int_{0}^{1} 2x^2 dx = \left[ \frac{2}{3}x^3 \right]_0^1 = \frac{2}{3}
$$
预期故障时间为 2/3 年。

---

[!definition] 定义 4.1.4 - 一般连续随机变量的均值 (Mean of General Continuous Random Variable)
与离散情况类似，我们必须处理期望可能不存在的情况。
设 $X$ 是一个连续随机变量，p.d.f. 为 $f$。我们考察两个积分：
$$
\int_{0}^{\infty} xf(x)dx \quad \text{和} \quad \int_{-\infty}^{0} xf(x)dx
$$
**期望存在的条件**：**至少有一个**积分是有限的。
* 如果这个条件满足，均值**存在**，定义为：
    $$
    E(X) = \int_{-\infty}^{\infty} xf(x)dx
    $$
* 如果两个积分**都是无穷大**，那么 $E(X)$ **不存在**。

[!example] 例子 4.1.7 - 保修期后故障 (Failure after Warranty)
某产品保修期为 1 年。其故障时间 $X$ 的 p.d.f. 为：
$f(x) = \frac{2}{x^3}$ ，对于 $x \ge 1$（$x<1$ 时 $f(x)=0$）

负值部分的积分为 0（有限）。我们计算正值部分：
$$
E(X) = \int_{1}^{\infty} x \left( \frac{2}{x^3} \right) dx = \int_{1}^{\infty} \frac{2}{x^2} dx
$$
$$
= \left[ -\frac{2}{x} \right]_1^\infty = \lim_{b \to \infty} \left( -\frac{2}{b} \right) - \left( -\frac{2}{1} \right) = 0 - (-2) = 2
$$
预期故障时间是 2 年。

[!example] 例子 4.1.8 - 均值不存在（柯西分布）
设 $X$ 的 p.d.f. 如下（这被称为**柯西分布 (Cauchy Distribution)**）：
$$
f(x) = \frac{1}{\pi(1+x^2)} \quad \text{for } -\infty < x < \infty
$$
（可以验证 $\int_{-\infty}^{\infty} f(x)dx = 1$）

我们检查两个积分：
1.  **正值部分**：$\int_{0}^{\infty} xf(x)dx = \int_{0}^{\infty} \frac{x}{\pi(1+x^2)} dx$
    (换元 $u = 1+x^2$, $du = 2xdx$)
    $= \frac{1}{2\pi} \int_1^\infty \frac{1}{u} du = \frac{1}{2\pi} [\ln|u|]_1^\infty = \infty$
2.  **负值部分**：$\int_{-\infty}^{0} xf(x)dx = \int_{-\infty}^{0} \frac{x}{\pi(1+x^2)} dx$
    (换元 $u = 1+x^2$, $du = 2xdx$)
    $= \frac{1}{2\pi} \int_\infty^1 \frac{1}{u} du = \frac{1}{2\pi} [\ln|u|]_\infty^1 = -\infty$

因为正负两部分都是无限的，所以柯西分布的均值 $E(X)$ **不存在**。

---
### 期望的解释 (Interpretation of the Expectation)

[!abstract] 期望的物理解释 - 重心 (Center of Gravity)
期望 $E(X)$ 可以被看作是其概率分布的**重心**。

* **对于离散分布**：
    想象 x 轴是一根没有重量的杆。如果在每个点 $x_j$ 挂上一个质量等于其概率 $f(x_j)$ 的重物（如图 4.1），那么这根杆的**平衡点**（支点）就是 $E(X)$。

* **对于连续分布**：
    想象 x 轴是一根密度不均匀的杆，在点 $x$ 处的密度（单位长度的质量）等于 $f(x)$（如图 4.2）。这根杆的**质心**（平衡点）就是 $E(X)$。

[!warning] 均值对“尾巴”的敏感性
这个重心解释也说明了均值的一个重要特性：它对分布的“尾巴”（即远离中心的大/小值）非常敏感。
即使你只在距离原点非常远的地方分配了**极小**的概率（例如 0.001%），也可能像在杠杆上加了一个很长的力臂，极大地移动（或“拉动”）重心的位置，从而显著改变均值。

---
### 对称性和期望 (Symmetry and Expectation)

如果一个分布的 p.f. 或 p.d.f. $f$ 关于某点 $x_0$ **对称**（即 $f(x_0 + \delta) = f(x_0 - \delta)$ 对所有 $\delta$ 成立），并且**其均值 $E(X)$ 存在**，那么根据重心的物理解释，平衡点必定在对称中心。
**结论**：$E(X) = x_0$。

[!example] 例子 4.1.9 - 柯西分布 (Cauchy Distribution)
我们再来看柯西分布 $f(x) = \frac{1}{\pi(1+x^2)}$。
* 这个函数显然是关于 $x=0$ 对称的（$f(x) = f(-x)$）。
* 如果它的均值存在，那么均值必须是 0。
* 但是，我们在例子 4.1.8 中已经证明了，它的均值**不存在**。

**为什么会这样？**
* 柯西分布的“尾巴” $f(x)$ 下降得足够快（如 $\frac{1}{x^2}$），使得曲线下的总面积为 1。
* 但是，$y=xf(x)$（即 $\frac{x}{\pi(1+x^2)}$）的“尾巴”下降得太慢了（如 $\frac{1}{x}$），使得 $y=xf(x)$ 曲线下的面积（即期望的积分）为无穷大。

---
### 函数的期望 (The Expectation of a Function)

我们经常需要计算的不是 $X$ 本身的期望，而是 $X$ 的某个函数 $r(X)$ 的期望。
设 $Y = r(X)$，我们想求 $E(Y)$。

[!example] 例子 4.1.10 - 故障率与故障时间
假设 $X$ 是某电器的年故障率。我们可能更关心它的预期寿命，即 $Y = 1/X$。我们如何计算 $E(Y) = E[1/X]$？

**方法一：“正统”方法（繁琐）**
1.  已知 $X$ 的 p.d.f. $f(x)$。
2.  利用变量变换法，求出 $Y = r(X)$ 的 p.d.f. $g(y)$。
3.  根据期望的定义，计算 $E(Y) = \int_{-\infty}^{\infty} y g(y) dy$。

[!example] 例子 4.1.11 - (续) 故障率与故障时间
假设 $X$ 的 p.d.f. 为 $f(x) = 3x^2$（当 $0 < x < 1$ 时）。我们要求 $E(Y)$，其中 $Y = 1/X$。
1.  **求 $Y$ 的 p.d.f. $g(y)$**：
    （使用第 3 章的 c.d.f. 变换法）
    $F_Y(y) = P(Y \le y) = P(1/X \le y) = P(X \ge 1/y)$
    $= \int_{1/y}^{1} 3x^2 dx = [x^3]_{1/y}^1 = 1 - (1/y)^3 = 1 - y^{-3}$（对于 $y > 1$）
    $g(y) = F_Y'(y) = \frac{d}{dy}(1 - y^{-3}) = 3y^{-4}$（对于 $y > 1$）
2.  **计算 $E(Y)$**：
    $E(Y) = \int_{1}^{\infty} y g(y) dy = \int_{1}^{\infty} y (3y^{-4}) dy = \int_{1}^{\infty} 3y^{-3} dy$
    $= \left[ -\frac{3}{2}y^{-2} \right]_1^\infty = 0 - (-\frac{3}{2}) = \frac{3}{2}$

这个方法非常麻烦。幸运的是，有更简单的方法。

---

**方法二：“懵懂统计师定律”（简单）**

[!theorem] 定理 4.1.1 - 懵懂统计师定律 (Law of the Unconscious Statistician - LOTUS)
设 $X$ 是一个随机变量，我们想求 $E[r(X)]$。我们**不需要**先求出 $Y=r(X)$ 的分布 $g(y)$。
我们可以直接使用 $X$ 的分布 $f(x)$ 来计算：

* **如果 $X$ 是连续的** (p.d.f. $f(x)$):
    $$
    E[r(X)] = \int_{-\infty}^{\infty} r(x)f(x)dx
    $$
* **如果 $X$ 是离散的** (p.f. $f(x)$):
    $$
    E[r(X)] = \sum_{\text{All } x} r(x)f(x)
    $$
    （假设期望存在）

[!question] 为什么叫“懵懂统计师定律”？
因为它非常直观，以至于很多“懵懂的”统计学学生（甚至专业人士）会无意识地使用它，把它当作 $E[r(X)]$ 的**定义**，而忘记了“正统”定义（方法一）才应该是最根本的。

[!question] 简要证明（离散情况）
“正统”定义是 $E(Y) = \sum_y y \cdot g(y)$。
$g(y) = Pr(Y=y) = Pr(r(X)=y) = \sum_{x:r(x)=y} f(x)$
代入：
$E(Y) = \sum_y y \left( \sum_{x:r(x)=y} f(x) \right)$
既然 $y = r(x)$，我们可以把 $y$ 替换成 $r(x)$:
$E(Y) = \sum_y \sum_{x:r(x)=y} r(x)f(x)$
把所有可能的 $x$ 值加起来，就等于把所有可能的 $y$ 值过一遍：
$E(Y) = \sum_{\text{All } x} r(x)f(x)$
这正是 LOTUS 公式。

[!example] 例子 4.1.12 - (续) 故障率与故障时间 (LOTUS法)
我们用 LOTUS 重新计算例子 4.1.11。
已知 $f(x) = 3x^2$（当 $0 < x < 1$）且 $r(x) = 1/x$。
$$
E[1/X] = \int_{0}^{1} r(x)f(x) dx = \int_{0}^{1} \left( \frac{1}{x} \right) (3x^2) dx
$$
$$
= \int_{0}^{1} 3x dx = \left[ \frac{3}{2}x^2 \right]_0^1 = \frac{3}{2}
$$
这和例子 4.1.11 的结果完全一样，但计算过程简单了非常多！

[!example] 例子 4.1.13 - 计算 $E(X^{1/2})$
设 $X$ 的 p.d.f. 如例子 4.1.6 所示：$f(x) = 2x$（当 $0 < x < 1$）。
我们想求 $Y = X^{1/2}$ 的期望。
使用 LOTUS：
$$
E(Y) = E[X^{1/2}] = \int_{0}^{1} r(x)f(x) dx = \int_{0}^{1} (x^{1/2}) (2x) dx
$$
$$
= \int_{0}^{1} 2x^{3/2} dx = \left[ 2 \cdot \frac{x^{5/2}}{5/2} \right]_0^1 = \left[ \frac{4}{5}x^{5/2} \right]_0^1 = \frac{4}{5}
$$

---

[!warning] 重大警示：$E[g(X)] \neq g(E(X))$
这是一个非常非常常见的错误！**一般来说，函数内的期望不等于期望的函数。**

* 在例子 4.1.13 中，我们算了 $E(X^{1/2}) = \frac{4}{5}$。
* 在例子 4.1.6 中，我们算了 $E(X) = \frac{2}{3}$。

如果我们错误地计算 $g(E(X))$，我们会得到：
$g(E(X)) = (E(X))^{1/2} = (\frac{2}{3})^{1/2} \approx 0.8165$
这显然**不等于** $\frac{4}{5} = 0.8$。

**唯一的例外**是当 $g$ 是一个线性函数时（例如 $g(X) = aX + b$），我们将在 4.2 节看到 $E[aX+b] = aE[X]+b$，此时等号成立。

---

[!example] 例子 4.1.14 - 期权定价 (Option Pricing)
这是一个 LOTUS 和期望思想的综合应用。

* **背景**：A 公司股票现价 $200。你获得一个期权：1 年后可以 $200 的价格购买该股票。
* **假设**：1 年后的股价 $X$ 是一个离散随机变量，只有两种可能：
    * $X = 260$ (概率为 $p$)
    * $X = 180$ (概率为 $1-p$)
* **期权价值 $Y$**：
    $Y$ 是 $X$ 的函数，$Y = h(X)$。
    * 如果 $X=180$：你不会行权（没人会花 $200 买 $180 的东西），期权价值 $Y = 0$。
    * 如果 $X=260$：你会行权，花 $200 买入，立刻 $260 卖出，获利 $60。期权价值 $Y = 60$。
    所以，$h(x) = \begin{cases} 0 & \text{if } x=180 \\ 60 & \text{if } x=260 \end{cases}$
* **期权期望价值**：
    使用 LOTUS ( discrete )：$E(Y) = \sum h(x)f(x)$
    $E(Y) = h(180) \cdot Pr(X=180) + h(260) \cdot Pr(X=260)$
    $E(Y) = 0 \times (1-p) + 60 \times p = 60p$
* **期权公平价格 $c$**：
    假设无风险年利率为 4%。$E(Y)$ 是一年后的预期价值。该期权在**今天**的公平价格 $c$ (即现值) 应该满足： $E(Y) = c \times (1 + 0.04)$
    $c = \frac{E(Y)}{1.04} = \frac{60p}{1.04} \approx 57.69p$
* **如何确定 $p$？（风险中性定价法）**
    金融业的标准方法是假设**股票的预期未来价值的现值等于其当前市价**。
    即：$E(X) = (\text{Current Price}) \times (1 + 0.04)$
    $E(X) = 260p + 180(1-p)$
    所以：$260p + 180(1-p) = 200 \times 1.04$
    $260p + 180 - 180p = 208$
    $80p = 28 \implies p = 0.35$
* **最终价格**：
    把 $p=0.35$ 代入期权价格公式：
    $c = 57.69 \times 0.35 = \$20.19$
    这个 $20.19 就是该期权的**风险中性价格 (risk-neutral price)**。

---
### 多个随机变量的函数 (Functions of Several Random Variables)

LOTUS 定理可以推广到多个变量。

[!theorem] 定理 4.1.2 - 懵懂统计师定律 (多维)
假设 $X_1, \ldots, X_n$ 是 $n$ 个随机变量，联合 p.d.f. (或 p.f.) 为 $f(x_1, \ldots, x_n)$。
设 $Y = r(X_1, \ldots, X_n)$ 是它们的一个函数。

* **如果 $X$ 是连续的**：
    $$
    E(Y) = \int \cdots \int_{R^n} r(x_1, \ldots, x_n) f(x_1, \ldots, x_n) dx_1 \cdots dx_n
    $$
* **如果 $X$ 是离散的**：
    $$
    E(Y) = \sum_{\text{All } x_1, \ldots, x_n} r(x_1, \ldots, x_n) f(x_1, \ldots, x_n)
    $$

[!example] 例子 4.1.16 - 两个变量函数的期望
一个点 $(X, Y)$ 在 $0 \le x \le 1, 0 \le y \le 1$ 的正方形 $S$ 上**随机**（均匀）选取。
求 $E(X^2 + Y^2)$。

1.  **联合 p.d.f.**：
    由于是在 $S$ 上均匀分布，且 $S$ 的面积为 1，所以 p.d.f. $f(x,y) = 1$（在 $S$ 内），否则为 0。
2.  **函数**：$r(x,y) = x^2 + y^2$
3.  **使用 LOTUS (多维)**：
    $E(X^2 + Y^2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x^2 + y^2) f(x,y) dx dy$
    $= \int_{0}^{1} \int_{0}^{1} (x^2 + y^2) \cdot 1 dx dy$
    先对 $x$ 积分：
    $= \int_{0}^{1} \left[ \frac{x^3}{3} + y^2x \right]_{x=0}^{1} dy$
    $= \int_{0}^{1} \left( \frac{1}{3} + y^2 \right) dy$
    再对 $y$ 积分：
    $= \left[ \frac{1}{3}y + \frac{y^3}{3} \right]_{y=0}^{1}$
    $= \left( \frac{1}{3} + \frac{1}{3} \right) - (0) = \frac{2}{3}$

---

[!summary] 4.1 总结
1.  **期望 (Mean)** 是分布的**重心**或**加权平均值**。
2.  **定义**：
    * 离散：$E(X) = \sum xf(x)$
    * 连续：$E(X) = \int xf(x)dx$
3.  **存在性**：期望不一定存在（例如柯西分布），当 $\sum$ 或 $\int$ 的正负两部分都发散时，期望不存在。
4.  **LOTUS**：计算 $E[r(X)]$ 时，不需要求 $r(X)$ 的分布，直接计算：
    * $E[r(X)] = \sum r(x)f(x)$
    * $E[r(X)] = \int r(x)f(x)dx$
5.  **重要警示**：$E[g(X)] \neq g(E(X))$

---
---

## 4.2 期望的性质 (Properties of Expectations)

本节我们介绍一些简化期望计算的基本定理。

[!theorem] 定理 4.2.1 - 线性函数的性质
如果 $Y = aX + b$，其中 $a$ 和 $b$ 是有限常数，那么：
$$
E(Y) = E(aX + b) = aE(X) + b
$$

[!question] 证明 (连续情况)
根据 LOTUS (定理 4.1.1)，$r(x) = ax+b$。
$$
E(aX + b) = \int_{-\infty}^{\infty} (ax+b)f(x)dx
$$
$$
= \int_{-\infty}^{\infty} axf(x)dx + \int_{-\infty}^{\infty} bf(x)dx
$$
把常数 $a$ 和 $b$ 提出来：
$$
= a \int_{-\infty}^{\infty} xf(x)dx + b \int_{-\infty}^{\infty} f(x)dx
$$
第一个积分是 $E(X)$ 的定义，第二个积分是 p.d.f. 的总面积（等于 1）。
$$
= aE(X) + b \cdot 1 = aE(X) + b
$$
(离散情况的证明类似，把 $\int$ 换成 $\sum$)

[!example] 例子 4.2.1 - 计算线性函数的期望
假设 $E(X) = 5$。
1.  $E(3X - 5) = 3E(X) - 5 = 3(5) - 5 = 10$
2.  $E(-3X + 15) = -3E(X) + 15 = -3(5) + 15 = 0$

[!theorem] 推论 4.2.1
如果一个随机变量 $X$ 以 100% 的概率等于一个常数 $c$（即 $Pr(X=c)=1$），那么 $E(X) = c$。
(这可以看作是 $a=0, b=c$ 时的特殊情况)

---

[!theorem] 定理 4.2.2 - 期望的单调性
1.  如果 $X$ 总是不小于 $a$ (即 $Pr(X \ge a) = 1$)，那么 $E(X) \ge a$。
2.  如果 $X$ 总是不大于 $b$ (即 $Pr(X \le b) = 1$)，那么 $E(X) \le b$。
3.  **推论**：如果 $Pr(a \le X \le b) = 1$，那么 $a \le E(X) \le b$。（期望值一定在最大值和最小值之间）

[!question] 证明 (第1部分，连续情况)
如果 $Pr(X \ge a) = 1$，意味着 p.d.f. $f(x)$ 在 $x < a$ 的区域必须为 0。
$$
E(X) = \int_{-\infty}^{\infty} xf(x)dx = \int_{a}^{\infty} xf(x)dx
$$
在这个积分范围内，$x \ge a$ 恒成立。所以 $xf(x) \ge af(x)$（因为 $f(x) \ge 0$）。
$$
E(X) \ge \int_{a}^{\infty} af(x)dx = a \int_{a}^{\infty} f(x)dx
$$
$\int_{a}^{\infty} f(x)dx$ 就是 $Pr(X \ge a)$，而我们已知 $Pr(X \ge a) = 1$。
$$
E(X) \ge a \cdot 1 = a
$$

[!theorem] 定理 4.2.3
如果 $E(X) = a$ 且 $Pr(X \ge a) = 1$，那么 $Pr(X = a) = 1$。
**通俗解释**：如果一个随机变量的期望等于它的最小值，那么这个变量必须 100% 等于这个最小值（它没有任何“波动空间”）。

---

[!theorem] 定理 4.2.4 - 随机变量求和 (期望的线性性)
这是期望最重要的性质之一。
如果 $X_1, \ldots, X_n$ 是 $n$ 个随机变量，且每个 $E(X_i)$ 都存在且有限，那么：
$$
E(X_1 + \cdots + X_n) = E(X_1) + \cdots + E(X_n)
$$
**期望的和等于和的期望。**

[!warning] 极其重要
这个定理**永远成立**。它**不要求** $X_1, \ldots, X_n$ 相互独立！
无论这些变量之间有多么复杂的相关性，这个性质都成立。

[!question] 证明 (n=2, 连续情况)
我们使用多维 LOTUS (定理 4.1.2)。$r(x_1, x_2) = x_1 + x_2$。
$$
E(X_1 + X_2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x_1 + x_2) f(x_1, x_2) dx_1 dx_2
$$
把积分拆开：
$$
= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x_1 f(x_1, x_2) dx_1 dx_2 + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x_2 f(x_1, x_2) dx_1 dx_2
$$
重新组合积分顺序：
$$
= \int_{-\infty}^{\infty} x_1 \left( \int_{-\infty}^{\infty} f(x_1, x_2) dx_2 \right) dx_1 + \int_{-\infty}^{\infty} x_2 \left( \int_{-\infty}^{\infty} f(x_1, x_2) dx_1 \right) dx_2
$$
括号里的积分 $\int f(x_1, x_2) dx_2$ 是 $X_1$ 的**边际 p.d.f. $f_1(x_1)$**。
同理，$\int f(x_1, x_2) dx_1$ 是 $X_2$ 的**边际 p.d.f. $f_2(x_2)$**。
所以上式等于：
$$
= \int_{-\infty}^{\infty} x_1 f_1(x_1) dx_1 + \int_{-\infty}^{\infty} x_2 f_2(x_2) dx_2
$$
根据期望的定义，这就是：
$$
= E(X_1) + E(X_2)
$$
（证明过程只用到了边际 p.d.f.，完全没有使用独立性）

---

[!theorem] 推论 4.2.2 - 线性组合
结合定理 4.2.1 和 4.2.4，我们可以得到最一般的线性性质：
对于任意常数 $a_1, \ldots, a_n$ 和 $b$：
$$
E(a_1X_1 + \cdots + a_nX_n + b) = a_1E(X_1) + \cdots + a_nE(X_n) + b
$$

---
**利用指示器变量 (Indicator Variables) 求解期望**
定理 4.2.4 非常强大，它催生了一种解决复杂期望问题的技巧：**指示器变量法**。
思路：把一个复杂的随机变量 $X$ 拆解成一堆简单的（通常是伯努利）随机变量 $X_i$ 的和。

[!example] 例子 4.2.4 - 无放回抽样 (Sampling without Replacement)
一个箱子里有红球和蓝球，红球的比例是 $p$。从箱中**无放回**地随机抽取 $n$ 个球。
设 $X$ 为选中的红球总数。求 $E(X)$。

* **难点**：$X$ 的分布是超几何分布，直接 $\sum x f(x)$ 会非常复杂。
* **指示器法**：
    定义 $n$ 个新的随机变量 $X_1, \ldots, X_n$：
    $X_i = \begin{cases} 1 & \text{如果第 } i \text{ 次抽到的球是红的} \\ 0 & \text{如果第 } i \text{ 次抽到的球是蓝的} \end{cases}$
    
    显然，$X$（红球总数）等于 $X_1 + \cdots + X_n$。
    根据定理 4.2.4：
    $E(X) = E(X_1 + \cdots + X_n) = E(X_1) + \cdots + E(X_n)$
    
    $E(X_i)$ 是什么？
    $E(X_i) = 1 \cdot Pr(X_i=1) + 0 \cdot Pr(X_i=0) = Pr(X_i=1)$
    $Pr(X_i=1)$ = "第 $i$ 次抽到红球" 的概率。
    
    因为是随机抽取，**任何一次抽取**（第1次、第2次、...、第 $i$ 次）抽到红球的概率都是 $p$。
    （例如 $Pr(X_2=1) = p$ 成立，虽然 $X_1$ 和 $X_2$ 是**相关**的，但这不影响 $X_2$ 的边际概率）
    
    所以，$E(X_i) = p$ 对所有 $i$ 都成立。
    
    $E(X) = p + p + \cdots + p$ (共 $n$ 项)
    $E(X) = np$

[!example] 例子 4.2.5 - 有放回抽样 (Sampling with Replacement)
同上，但改为**有放回**抽样。
$X$ 为选中的红球总数。求 $E(X)$。

* **背景**：$X$ 服从二项分布 $Bin(n, p)$。
* **指示器法**：
    我们依然可以定义 $X = X_1 + \cdots + X_n$，其中 $X_i = 1$ 如果第 $i$ 次是红球。
    $E(X) = E(X_1) + \cdots + E(X_n)$
    在这种情况下，每次抽样都是独立的，所以 $Pr(X_i=1) = p$ 更明显。
    $E(X) = p + \cdots + p = np$

**结论**：二项分布 $Bin(n, p)$ 的期望是 $np$。
而且，无论是有放回还是无放回，抽 $n$ 次，期望的红球数都是 $np$！

[!example] 例子 4.2.6 - 匹配问题 (Expected Number of Matches)
一个人写了 $n$ 封信，和 $n$ 个对应的信封。他随机地将 $n$ 封信放入 $n$ 个信封（每个信封一封）。
设 $X$ 为“放对的信”（信和信封匹配）的数量。求 $E(X)$。

* **难点**：$X$ 的分布非常复杂（这是“错排问题”）。
* **指示器法**：
    定义 $n$ 个指示器变量 $X_1, \ldots, X_n$：
    $X_i = \begin{cases} 1 & \text{如果第 } i \text{ 封信放对了信封} \\ 0 & \text{否则} \end{cases}$
    
    $X$（匹配总数）= $X_1 + \cdots + X_n$。
    $E(X) = E(X_1) + \cdots + E(X_n)$
    
    $E(X_i) = Pr(X_i=1)$
    $Pr(X_i=1)$ = "第 $i$ 封信放对了" 的概率。
    对于第 $i$ 封信，有 $n$ 个信封可选，只有一个是正确的。
    所以 $Pr(X_i=1) = \frac{1}{n}$。
    
    这 $n$ 个 $X_i$ 显然是**不独立**的（如果 $X_1=1$, $X_2=1, \ldots, X_{n-1}=1$，那么 $X_n$ 必须等于 1）。
    但我们**不在乎**！定理 4.2.4 依然适用。
    
    $E(X_i) = \frac{1}{n}$ 对所有 $i$ 都成立。
    
    $E(X) = \frac{1}{n} + \frac{1}{n} + \cdots + \frac{1}{n}$ (共 $n$ 项)
    $E(X) = n \cdot (\frac{1}{n}) = 1$

**惊人的结果**：无论 $n=5$ 还是 $n=1,000,000$，随机装信封，你平均总能期望有一封信是放对的！

---
### 独立随机变量的乘积 (Product of Independent Random Variables)

我们已经知道 $E(X+Y) = E(X) + E(Y)$ 总是成立。
那么 $E(XY) = E(X)E(Y)$ 呢？

[!warning]
这**不总是**成立。它需要一个额外条件：**独立性**。

[!theorem] 定理 4.2.6 - 独立随机变量的乘积
如果 $X_1, \ldots, X_n$ 是 $n$ 个**相互独立**的随机变量，且 $E(X_i)$ 都有限，那么：
$$
E(\prod_{i=1}^{n} X_i) = \prod_{i=1}^{n} E(X_i)
$$
**乘积的期望等于期望的乘积（仅在独立时成立）。**

[!question] 证明 (n=2, 连续情况)
我们需要计算 $E(X_1 X_2)$。根据 LOTUS：
$$
E(X_1 X_2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x_1 x_2) f(x_1, x_2) dx_1 dx_2
$$
**关键一步**：因为 $X_1$ 和 $X_2$ 独立，它们的联合 p.d.f. $f(x_1, x_2)$ 可以分解为边际 p.d.f. 的乘积：
$f(x_1, x_2) = f_1(x_1) f_2(x_2)$
$$
E(X_1 X_2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x_1 x_2) f_1(x_1) f_2(x_2) dx_1 dx_2
$$
我们可以把积分拆开：
$$
= \left( \int_{-\infty}^{\infty} x_1 f_1(x_1) dx_1 \right) \cdot \left( \int_{-\infty}^{\infty} x_2 f_2(x_2) dx_2 \right)
$$
$$
= E(X_1) \cdot E(X_2)
$$

[!example] 例子 4.2.7 - 计算随机变量组合的期望
假设 $X_1, X_2, X_3$ 相互独立，且 $E(X_i) = 0$，$E(X_i^2) = 1$ (对于 $i=1,2,3$)。
求 $E[X_1^2 (X_2 - 4X_3)^2]$。

1.  $X_1$ 与 $(X_2, X_3)$ 独立，因此 $X_1^2$ 与 $(X_2 - 4X_3)^2$ 也**独立**。
2.  根据定理 4.2.6，我们可以把期望拆开：
    $E[\cdot] = E[X_1^2] \cdot E[(X_2 - 4X_3)^2]$
3.  我们已知 $E[X_1^2] = 1$。
4.  计算第二项。先展开平方：
    $E[(X_2 - 4X_3)^2] = E[X_2^2 - 8X_2X_3 + 16X_3^2]$
5.  根据期望的线性性（定理 4.2.4），拆开**和**：
    $= E[X_2^2] - E[8X_2X_3] + E[16X_3^2]$
6.  根据线性性（定理 4.2.1），提出常数：
    $= E[X_2^2] - 8E[X_2X_3] + 16E[X_3^2]$
7.  我们已知 $E[X_2^2] = 1$ 且 $E[X_3^2] = 1$。
8.  计算 $E[X_2X_3]$。因为 $X_2$ 和 $X_3$ 独立，使用定理 4.2.6：
    $E[X_2X_3] = E[X_2] \cdot E[X_3] = 0 \times 0 = 0$
9.  代回第 6 步：
    $E[\cdot] = 1 - 8(0) + 16(1) = 17$
10. **最终答案**：
    $E[X_1^2 (X_2 - 4X_3)^2] = E[X_1^2] \cdot 17 = 1 \times 17 = 17$

---
### 非负分布的期望 (Expectation for Nonnegative Distributions)

有两个特殊的“尾巴求和公式”可以用来计算期望。

[!theorem] 定理 4.2.7 - 整数值随机变量 (Tail Sum Formula)
如果 $X$ 是一个只取非负整数值 (0, 1, 2, ...) 的随机变量，那么：
$$
E(X) = \sum_{n=1}^{\infty} Pr(X \ge n)
$$

[!question] 证明
$E(X) = \sum_{k=0}^{\infty} k \cdot Pr(X=k) = 0 \cdot Pr(X=0) + 1 \cdot Pr(X=1) + 2 \cdot Pr(X=2) + 3 \cdot Pr(X=3) + \cdots$
把 $k \cdot Pr(X=k)$ 写成 $k$ 个 $Pr(X=k)$ 相加：
$E(X) = Pr(X=1) + Pr(X=2) + Pr(X=2) + Pr(X=3) + Pr(X=3) + Pr(X=3) + \cdots$
重新按“行”相加，而不是按“列”：
* 第 1 行：$Pr(X=1) + Pr(X=2) + Pr(X=3) + \cdots = Pr(X \ge 1)$
* 第 2 行：$Pr(X=2) + Pr(X=3) + \cdots = Pr(X \ge 2)$
* 第 3 行：$Pr(X=3) + \cdots = Pr(X \ge 3)$
* ...
把所有行的和加起来：
$E(X) = Pr(X \ge 1) + Pr(X \ge 2) + Pr(X \ge 3) + \cdots = \sum_{n=1}^{\infty} Pr(X \ge n)$

[!example] 例子 4.2.9 - 预期试验次数 (几何分布)
一个人反复尝试某项任务，每次试验成功的概率为 $p$ ( $0 < p < 1$ )，试验间相互独立。
设 $X$ 为“首次成功在第几次试验”。求 $E(X)$。
( $X$ 是几何分布，取值 1, 2, 3, ...)

$Pr(X \ge n)$ 是什么？
"第 $n$ 次或更晚才成功" 意味着 "前 $n-1$ 次试验**都失败了**"。
$Pr(X \ge n) = (1-p)^{n-1}$

使用定理 4.2.7：
$$
E(X) = \sum_{n=1}^{\infty} Pr(X \ge n) = \sum_{n=1}^{\infty} (1-p)^{n-1}
$$
令 $k = n-1$，当 $n$ 从 1 到 $\infty$，$k$ 从 0 到 $\infty$：
$$
E(X) = \sum_{k=0}^{\infty} (1-p)^k = 1 + (1-p) + (1-p)^2 + \cdots
$$
这是一个公比 $r = (1-p)$ 的无穷等比数列。其和为 $\frac{1}{1-r}$。
$$
E(X) = \frac{1}{1 - (1-p)} = \frac{1}{p}
$$
**结论**：几何分布的期望是 $1/p$。
(例如，抛硬币 $p=0.5$，你期望 $\frac{1}{0.5} = 2$ 次能抛出正面。)

---

[!theorem] 定理 4.2.8 - 一般非负随机变量 (Tail Sum Formula)
定理 4.2.7 的连续版本。
如果 $X$ 是一个非负随机变量（$Pr(X \ge 0) = 1$），c.d.f. 为 $F(x)$，那么：
$$
E(X) = \int_{0}^{\infty} [1 - F(x)] dx
$$
(注意：$1 - F(x)$ 就是 $Pr(X > x)$，这和 $\sum Pr(X \ge n)$ 的形式非常相似)

[!example] 例子 4.2.10 - 预期等待时间 (指数分布)
一个顾客的等待时间 $X$ ( $X > 0$ ) 的 c.d.f. 为：
$F(x) = 1 - e^{-2x}$ (对于 $x > 0$)
求 $E(X)$。

$1 - F(x) = 1 - (1 - e^{-2x}) = e^{-2x}$
使用定理 4.2.8：
$$
E(X) = \int_{0}^{\infty} [1 - F(x)] dx = \int_{0}^{\infty} e^{-2x} dx
$$
$$
= \left[ -\frac{1}{2} e^{-2x} \right]_0^\infty = \lim_{b \to \infty} \left( -\frac{1}{2} e^{-2b} \right) - \left( -\frac{1}{2} e^0 \right)
$$
$$
= (0) - (-\frac{1}{2}) = \frac{1}{2}
$$
预期等待时间为 1/2。

---

[!summary] 4.2 总结
1.  **期望的线性性 (最重要)**：
    * $E(aX + b) = aE(X) + b$
    * $E(\sum a_i X_i) = \sum a_i E(X_i)$ (**永远成立**，无需独立)
2.  **指示器变量法**：利用线性性，将复杂 $X$ 拆解为 $X = \sum X_i$，求解 $E(X) = \sum E(X_i)$。
3.  **独立变量的乘积**：
    * $E(XY) = E(X)E(Y)$ (**仅当** X 和 Y 独立时成立)
4.  **非负变量的尾巴公式**：
    * 离散：$E(X) = \sum_{n=1}^\infty Pr(X \ge n)$
    * 连续：$E(X) = \int_0^\infty [1-F(x)]dx$

---
---

## 4.3 方差 (Variance)

[!note] 为什么需要方差？
均值 $E(X)$ 描述了分布的“中心位置”，但它没有提供任何关于分布**“分散程度”**或**“可变性”**的信息。

[!example] 例子 4.3.1 - 股价变动
看两只股票未来的价格：
* **股票 A**：$A$ 服从 [25, 35] 上的均匀分布。
* **股票 B**：$B$ 服从 [15, 45] 上的均匀分布。

计算它们的均值（(a+b)/2）：
* $E(A) = (25+35)/2 = 30$
* $E(B) = (15+45)/2 = 30$

两只股票的期望价格**完全相同**。但它们显然不同：
* 股票 A 很稳定，价格在 25 到 35 之间。
* 股票 B 风险大得多（“波动性”大），价格可能低至 15，也可能高至 45。

我们需要一个数学工具来**量化**这种“分散程度”或“波动性”。这个工具就是**方差 (Variance)**。

---

### 方差和标准差的定义

[!definition] 定义 4.3.1 - 方差 (Variance) 与 标准差 (Standard Deviation)
设 $X$ 是一个随机变量，其有限的均值为 $\mu = E(X)$。
$X$ 的**方差**，记作 $Var(X)$，被定义为：
$$
Var(X) = E[ (X - \mu)^2 ]
$$
**通俗解释**：方差是“随机变量偏离其均值的**平方**”的**期望值**。
* 如果 $X$ 的值经常远离 $\mu$，那么 $(X-\mu)^2$ 就会很大，方差就大。
* 如果 $X$ 的值总是紧密聚集在 $\mu$ 附近，那么 $(X-\mu)^2$ 就会很小，方差就小。

$X$ 的**标准差 (Standard Deviation)**，通常记作 $\sigma$，被定义为方差的**非负平方根**：
$$
\sigma = \sqrt{Var(X)}
$$
(如果 $E(X)$ 不存在， $Var(X)$ 也不存在。)
(标准差的单位与 $X$ 相同，而方差的单位是 $X$ 的平方，因此标准差更常用于实际解释。)

[!example] 例子 4.3.2 - (续) 股价变动
我们来计算例子 4.3.1 中两只股票的方差。两者的 $\mu$ 都是 30。

* **股票 A**：$f(a) = \frac{1}{35-25} = \frac{1}{10}$ (在 [25, 35] 上)
    $Var(A) = E[(A-30)^2]$
    (使用 LOTUS， $r(a) = (a-30)^2$)
    $Var(A) = \int_{25}^{35} (a-30)^2 \cdot \frac{1}{10} da$
    (换元 $x = a-30$, $da=dx$, $a=25 \to x=-5$, $a=35 \to x=5$)
    $= \frac{1}{10} \int_{-5}^{5} x^2 dx = \frac{1}{10} \left[ \frac{x^3}{3} \right]_{-5}^5 = \frac{1}{10} \left( \frac{125}{3} - \frac{-125}{3} \right) = \frac{1}{10} \left( \frac{250}{3} \right) = \frac{25}{3} \approx 8.33$

* **股票 B**：$f(b) = \frac{1}{45-15} = \frac{1}{30}$ (在 [15, 45] 上)
    $Var(B) = E[(B-30)^2] = \int_{15}^{45} (b-30)^2 \cdot \frac{1}{30} db$
    (换元 $y = b-30$, $db=dy$, $b=15 \to y=-15$, $b=45 \to y=15$)
    $= \frac{1}{30} \int_{-15}^{15} y^2 dy = \frac{1}{30} \left[ \frac{y^3}{3} \right]_{-15}^{15} = \frac{1}{30} \left( \frac{15^3}{3} - \frac{-15^3}{3} \right) = \frac{1}{30} \left( \frac{2 \cdot 15^3}{3} \right) = \frac{15^2}{3} = \frac{225}{3} = 75$

* **结论**：$Var(B) = 75$ 远大于 $Var(A) \approx 8.33$。这在数学上量化了“股票B更具波动性”的直观感受。
* **标准差**：$\sigma_A = \sqrt{25/3} \approx 2.87$，$\sigma_B = \sqrt{75} \approx 8.66$。

---

[!example] 例子 4.3.3 - 离散分布的方差和标准差
设 $X$ 取 5 个值：-2, 0, 1, 3, 4，概率相等（$f(x) = 1/5$）。
计算 $Var(X)$ 和 $\sigma$。

1.  **计算均值 $\mu$**：
    $\mu = E(X) = \frac{1}{5}(-2 + 0 + 1 + 3 + 4) = \frac{6}{5} = 1.2$
2.  **计算 $Var(X) = E[(X-\mu)^2]$**：
    $Var(X) = E[(X-1.2)^2]$
    (使用 LOTUS)
    $Var(X) = \sum (x-1.2)^2 f(x)$
    $= \frac{1}{5} \left[ (-2-1.2)^2 + (0-1.2)^2 + (1-1.2)^2 + (3-1.2)^2 + (4-1.2)^2 \right]$
    $= \frac{1}{5} \left[ (-3.2)^2 + (-1.2)^2 + (-0.2)^2 + (1.8)^2 + (2.8)^2 \right]$
    $= \frac{1}{5} [ 10.24 + 1.44 + 0.04 + 3.24 + 7.84 ]$
    $= \frac{1}{5} [ 22.8 ] = 4.56$
3.  **计算标准差 $\sigma$**：
    $\sigma = \sqrt{Var(X)} = \sqrt{4.56} \approx 2.135$

---
### 计算方差的快捷公式

按定义 $E[(X-\mu)^2]$ 计算方差通常很繁琐。有一个更简单的计算公式。

[!theorem] 定理 4.3.1 - 计算方差的快捷公式
对于任意随机变量 $X$：
$$
Var(X) = E(X^2) - [E(X)]^2
$$
**“方差等于 $X$ 平方的期望减去 $X$ 期望的平方。”**

[!question] 证明
设 $\mu = E(X)$。
$Var(X) = E[(X-\mu)^2]$ (根据定义)
展开平方：
$Var(X) = E[X^2 - 2\mu X + \mu^2]$
根据期望的线性性（$E(A+B+C) = E(A)+E(B)+E(C)$）：
$Var(X) = E(X^2) - E(2\mu X) + E(\mu^2)$
$2\mu$ 和 $\mu^2$ 都是常数，根据 $E(aY)=aE(Y)$ 和 $E(b)=b$：
$Var(X) = E(X^2) - 2\mu E(X) + \mu^2$
把 $E(X)$ 替换回 $\mu$：
$Var(X) = E(X^2) - 2\mu(\mu) + \mu^2$
$Var(X) = E(X^2) - 2\mu^2 + \mu^2$
$Var(X) = E(X^2) - \mu^2 = E(X^2) - [E(X)]^2$

[!example] 例子 4.3.4 - (续) 离散分布的方差 (快捷法)
我们用快捷公式重算例子 4.3.3。
$X$ 取值：-2, 0, 1, 3, 4，概率 $1/5$。
我们已经知道 $E(X) = 1.2$。

1.  **计算 $E(X^2)$**：
    (使用 LOTUS，$r(x) = x^2$)
    $E(X^2) = \sum x^2 f(x)$
    $E(X^2) = \frac{1}{5} [ (-2)^2 + (0)^2 + (1)^2 + (3)^2 + (4)^2 ]$
    $E(X^2) = \frac{1}{5} [ 4 + 0 + 1 + 9 + 16 ]$
    $E(X^2) = \frac{1}{5} [ 30 ] = 6$
2.  **使用快捷公式**：
    $Var(X) = E(X^2) - [E(X)]^2$
    $Var(X) = 6 - (1.2)^2$
    $Var(X) = 6 - 1.44 = 4.56$
这与例子 4.3.3 的结果完全相同，但计算过程简单得多。

---

[!example] 例子 4.3.5 - 伯努利分布的微小改动
这个例子展示了均值和方差对“长尾”的敏感性。

* **情况 1：伯努利 $Y$**
    $Y$ 服从 $p=0.5$ 的伯努利分布。$Pr(Y=0)=0.5, Pr(Y=1)=0.5$。
    * $E(Y) = 0.5$ (见 4.1.3)
    * $E(Y^2) = (0^2) \times 0.5 + (1^2) \times 0.5 = 0.5$
    * $Var(Y) = E(Y^2) - [E(Y)]^2 = 0.5 - (0.5)^2 = 0.5 - 0.25 = 0.25$

* **情况 2：轻微改动的 $X$**
    $X$ 的 p.f. 如下：
    $f(x) = \begin{cases} 0.5 & \text{if } x=0 \\ 0.499 & \text{if } x=1 \\ 0.001 & \text{if } x=10,000 \\ 0 & \text{otherwise} \end{cases}$
    (这个分布 $X$ 和 $Y$ 非常接近，只是把 $Y$ 中 1 的 $0.001$ 的概率“移动”到了 10,000。)

    * **$E(X)$**：
        $E(X) = (0) \times 0.5 + (1) \times 0.499 + (10000) \times 0.001$
        $E(X) = 0 + 0.499 + 10 = 10.499$
    * **$E(X^2)$**：
        $E(X^2) = (0^2) \times 0.5 + (1^2) \times 0.499 + (10000^2) \times 0.001$
        $E(X^2) = 0 + 0.499 + (100,000,000) \times 0.001$
        $E(X^2) = 0.499 + 100,000 = 100,000.499$
    * **$Var(X)$**：
        $Var(X) = E(X^2) - [E(X)]^2$
        $Var(X) = 100,000.499 - (10.499)^2$
        $Var(X) = 100,000.499 - 110.229... \approx 99,890.27$

* **结论**：
    * $Y$ (伯努利): $E(Y)=0.5$, $Var(Y)=0.25$
    * $X$ (改动后): $E(X)=10.5$, $Var(X)=99,890$
    仅仅 0.1% (0.001) 的概率被移到了一个很远的值 (10,000)，就导致均值和方差发生了**巨大**的变化。这说明**方差（和均值）对“长尾”或“极端异常值”非常敏感**。

---
### 方差的性质

[!theorem] 定理 4.3.2
1.  **方差非负**：$Var(X) \ge 0$
    (因为 $Var(X) = E[(X-\mu)^2]$，而 $(X-\mu)^2$ 永远是非负的，非负随机变量的期望也永远是非负的。)
2.  **有界则方差有限**：如果 $X$ 是有界随机变量（即 $Pr(a \le X \le b) = 1$），那么 $Var(X)$ 必然存在且有限。

[!theorem] 定理 4.3.3
**方差为 0 的充要条件**：
$Var(X) = 0$ **当且仅当** 存在一个常数 $c$，使得 $Pr(X = c) = 1$。

**通俗解释**：方差为 0 意味着“没有波动性”。唯一没有波动性的“随机”变量就是常数。

[!question] 证明
1.  **“如果”部分 ( $Pr(X=c)=1 \implies Var(X)=0$ )**：
    如果 $Pr(X=c)=1$，那么 $X$ 的均值 $\mu = E(X) = c$ (推论 4.2.1)。
    $Var(X) = E[(X-\mu)^2] = E[(X-c)^2]$。
    因为 $X$ 总是等于 $c$，所以 $(X-c)^2$ 总是等于 0。
    $Var(X) = E[0] = 0$。

2.  **“仅当”部分 ( $Var(X)=0 \implies Pr(X=c)=1$ )**：
    如果 $Var(X) = 0$，即 $E[(X-\mu)^2] = 0$。
    我们有一个非负随机变量 $W = (X-\mu)^2$，它的期望 $E(W) = 0$。
    根据定理 4.2.3（如果非负 RV 的期望为 0，则该 RV 必须以 1 的概率为 0）：
    $Pr(W = 0) = 1$
    $Pr((X-\mu)^2 = 0) = 1$
    $Pr(X - \mu = 0) = 1$
    $Pr(X = \mu) = 1$
    这就证明了 $X$ 必须以 100% 的概率等于其均值 $\mu$（即 $c = \mu$）。