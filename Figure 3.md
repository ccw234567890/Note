![](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509111152201.png)
# 图像解析：DenseNet vs. ResNet 的效率对比图

**标签**: #DeepLearning #Benchmark #DenseNet #ResNet #ComputerVision #Efficiency

这张图（图3）是 [[架构：DenseNet中的“密集块”与“过渡层”|DenseNet]] 论文的另一个核心论证，旨在从**参数效率**和**计算效率**两个维度，直观地展示 DenseNet 相较于其主要竞争对手 [[残差网络 (ResNet)|ResNet]] 的优越性。

---

## 1. 如何解读此图

首先，我们需要理解图的各个组成部分代表什么。

- **Y 轴 (纵坐标)**: `validation error (%)`
    - 代表模型在 ImageNet 验证集上的 **Top-1 错误率**。
    - 这个值**越低越好**，表示模型性能越强。

- **X 轴 (横坐标)**:
    - **左图**: `#parameters` (参数量)，单位是千万（$10^7$）。
        - 衡量的是**模型的大小**。参数越少，模型越轻量。
        - 这个值**越小越好**（越靠左越好）。
    - **右图**: `#flops` (浮点运算数)，单位是百亿（$10^{10}$）。
        - 衡量的是模型进行一次预测所需的**计算量**。FLOPs 越少，模型速度越快，对硬件要求越低。
        - 这个值**越小越好**（越靠左越好）。

- **理想区域 (The "Winning Corner")**:
    - 综合来看，一个理想的模型应该位于图的**左下角**，因为它代表了**以最低的成本（参数少/计算快）取得了最低的错误率（性能好）**。

- **两条曲线**:
    - **红色 ▲ 线**: 代表 **ResNets** 家族。
    - **蓝色 ◀ 线**: 代表 **DenseNets-BC** 家族（BC指使用了[[Bottleneck Layer|瓶颈层]]和[[原文 (Original): We adopt a standard data augmentation scheme (mirroring/shifting) that is widely used for these two datasets.|压缩因子]]的高效版本）。

---

## 2. 图表讲述的故事与核心结论

这张图通过两条曲线的位置关系，讲述了一个非常清晰的故事。

### A. 左图解析：参数效率 (Parameter Efficiency)

这张图比较的是**“模型大小” vs “性能”**。

- **观察**: 整条**蓝色 DenseNet 曲线**都位于**红色 ResNet 曲线**的**左下方**。
- **解读**:
    1.  **在相同的性能水平上**：比如，要达到约 `24%` 的错误率，ResNet 需要 `ResNet-50`（约2500万参数），而 DenseNet 只需要 `DenseNet-169`（不到2000万参数）。DenseNet 用更少的参数达到了同样的性能。
    2.  **在相同的参数规模上**：比如，在参数量约 `2500` 万左右，`ResNet-50` 的错误率约为 `24%`，而 `DenseNet-201` 的错误率已经低至 `22.5%` 左右。DenseNet 用同样的参数达到了更好的性能。

- **结论**: **DenseNet 的参数效率远高于 ResNet**。它可以用更少的参数击败更庞大的 ResNet 模型。

### B. 右图解析：计算效率 (Computational Efficiency)

这张图比较的是**“计算成本” vs “性能”**。

- **观察**: 故事和左图完全一样。整条**蓝色 DenseNet 曲线**依然位于**红色 ResNet 曲线**的**左下方**。
- **解读**:
    - 要达到 `22.5%` 左右的错误率，`ResNet-152` 需要超过 `2.25 x 10^{10}` 的 FLOPs（即超过225亿次浮点运算）。
    - 而 `DenseNet-264` 可以在达到**更低错误率**（约 `22%`）的同时，只使用约 `1.5 x 10^{10}` 的 FLOPs（约150亿次浮点运算）。

- **结论**: **DenseNet 的计算效率也高于 ResNet**。它可以用更少的计算资源，完成更高质量的预测。

---

### **总结与意义**

这张对比图是 DenseNet 论文最有力的证据，它雄辩地证明了：

1.  **DenseNet 不仅仅是“另一个”深度网络，而是一个“更高效”的深度网络**。
2.  其核心的**密集连接**和**特征重用**机制，确实转化为了实实在在的**参数和计算优势**。
3.  对于追求在有限资源下（无论是模型大小还是算力）达到最佳性能的场景，DenseNet 提供了一个比 ResNet 更优的**性能/成本权衡（Trade-off）**。

简而言之，这张图宣告了 DenseNet 在“如何更聪明地构建深度网络”这个问题上，迈出了成功的一步。