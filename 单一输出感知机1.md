# 学习笔记：反向传播 - 从手动推导到PyTorch实战

> [!NOTE] 核心纲要
> 我们已经知道需要用**梯度**来更新模型参数。但是，这个梯度究竟是如何从最终的“误差”一路计算回网络起点的呢？这个过程就是大名鼎鼎的**反向传播 (Backpropagation)**。
> 
> 本篇笔记将以最基础的**单个神经元**为例，完整地走一遍这个流程：
> 1.  **定义模型**：理解数据如何**前向传播**。
> 2.  **理论推导**：**手动计算**梯度，理解反向传播的原理。
> 3.  **代码实战**：看 PyTorch 如何用一行代码**自动完成**这个复杂过程。

---
## 第一部分：模型定义 - 单个神经元的前向传播 ➡️

> [!info] 一个神经元的工作流程
> ![[Pasted image 20250725000535.png]]
> **前向传播 (Forward Propagation)** 是指数据从输入层开始，一步步流向输出层的过程。
> 
> 1.  **输入 (Input)**
>     - 一组输入特征，表示为向量 $x^0 = (x^0_0, x^0_1, \dots, x^0_n)$。
> 
> 2.  **线性加权和 (Linear Combination)**
>     - 将输入与对应的权重 `w` 进行点积运算（暂时忽略偏置`b`）。
>      ![[Pasted image 20250725000615.png]]
>       $$ x^1_0 = \sum_{i=0}^{n} w^1_{i0} x^0_i $$
> 
> 3.  **激活函数 (Activation)**
>     - 将加权和的结果传入 Sigmoid 函数 `σ`，得到神经元的最终输出 `O`。
>       $$ O^1_0 = \sigma(x^1_0) $$
> 
> 4.  **计算损失 (Loss)**
>     - 将输出 `O` 与真实目标 `t` 比较，计算均方误差 `E`。
>       $$ E = \frac{1}{2}(O^1_0 - t)^2 $$

---
## 第二部分：理论核心 - 手动反向传播详解 🧠

> [!abstract] 目标：计算梯度 $\frac{\partial E}{\partial w^1_{j0}}$
> ![[Pasted image 20250725000705.png]]
> 我们的目标是找出最终误差 `E` 相对于网络中任意一个权重 $w^1_{j0}$ 的梯度。为此，我们必须使用**链式法则**，将 `E` 对 `w` 的影响链条拆解开来。
> 
> $$ \frac{\partial E}{\partial w^1_{j0}} = \underbrace{\frac{\partial E}{\partial O^1_0}}_{\text{第一环}} \cdot \underbrace{\frac{\partial O^1_0}{\partial x^1_0}}_{\text{第二环}} \cdot \underbrace{\frac{\partial x^1_0}{\partial w^1_{j0}}}_{\text{第三环}} $$
> 
> #### 分步拆解
> 
> 1.  **第一环: 损失对输出的梯度**
>     - $E = \frac{1}{2}(O^1_0 - t)^2 \implies \frac{\partial E}{\partial O^1_0} = (O^1_0 - t)$
>     - **含义**: 当前的输出值与真实值之间的误差。
> 
> 2.  **第二环: 输出对激活前的值的梯度**
>     - $O^1_0 = \sigma(x^1_0) \implies \frac{\partial O^1_0}{\partial x^1_0} = O^1_0(1 - O^1_0)$
>     - **含义**: Sigmoid 激活函数本身的梯度。
> 
> 3.  **第三环: 激活前的值对权重的梯度**
>     - $x^1_0 = \sum_i w^1_{i0} x^0_i \implies \frac{\partial x^1_0}{\partial w^1_{j0}} = x^0_j$
>     - **含义**: 与该权重 $w^1_{j0}$ 相连接的那个输入 $x^0_j$。
> 
> ---
> > [!success] 最终梯度公式
> > 将三环相乘，我们得到了最终的梯度更新公式：
> >
> > $$ \frac{\partial E}{\partial w^1_{j0}} = (O^1_0 - t) \cdot O^1_0(1 - O^1_0) \cdot x^0_j $$
> > **这个公式优美地告诉我们，对一个权重的调整量，取决于 ==整体误差==、==激活函数的梯度== 和 ==对应的输入信号== 这三者的乘积。**

---
## 第三部分：PyTorch 实战 - 自动化的高效实现 🚀

> [!example] 代码实现：理论的完美映射
> 手动推导帮助我们理解原理，而 PyTorch 的 `autograd` 则将我们从繁琐的计算中解放。下面的代码实现了与手动推导完全相同的过程。
> ![[Pasted image 20250725000851.png]]
> 
> #### 代码逐行解析
> - **`In [41]`, `[48]`**: 创建输入 `x` 和权重 `w`。最关键的一步是为 `w` 设置 `requires_grad=True`，开启梯度追踪。
> - **`In [49]`**: `o = torch.sigmoid(x @ w.t())`
>   - 这一行代码就完成了**前向传播**中的“线性加权和”与“激活”两步，得到了最终输出 `o`。
> - **`In [51]`**: `loss = F.mse_loss(...)`
>   - 计算模型输出 `o` 和目标值 `1` 之间的**损失**。
> - **`In [53]`**: `loss.backward()`
>   - **==魔法发生的地方！==** PyTorch 自动执行了我们第二部分中所有的链式法则推导，计算出 `loss` 相对于所有 `requires_grad=True` 的张量的梯度。
> - **`In [54]`**: `w.grad`
>   - 我们直接访问 `w` 的 `.grad` 属性，就能看到 PyTorch 为我们自动算好的梯度向量。这个向量的每一个值，都对应着我们辛辛苦苦手动推导出来的 $\frac{\partial E}{\partial w^1_{j0}}$。

> [!summary] 总结
> - **反向传播** 是一个利用**链式法则**，从最终损失开始，逐层向后计算梯度，从而“传播”误差信号的过程。
> - **手动推导**能帮助我们建立深刻的直觉，理解模型内部的工作机制。
> - **PyTorch Autograd** 将这个过程完全自动化，让我们能专注于设计更强大的网络架构，而不必陷入繁琐的数学计算中。==理解前者，能让我们更好地使用后者。==