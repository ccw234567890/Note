‘## 摘要

想象一下，你是一位非常厉害的画家，要教一个机器人画一只猫。

### 以前的训练方法（遇到的问题）

传统的方法是，你给机器人一张白纸，然后一步一步地教它：“先画一个椭圆形的头，再画两个三角形的耳朵，然后画身体、四条腿、一条尾巴……”

这个机器人有很多“绘画层”，每一层都在前一层的基础上进行加工。

*   **浅层网络（层数少）**：如果只有几层，机器人可能学得还不错，能画出一个大概的猫的样子。
*   **深层网络（层数多）**：你觉得层数越多，机器人学得就越精细，画得就越像。但问题来了，当层数变得非常多（比如几十上百层）时，机器人反而“糊涂”了。信息从第一层传到最后一层时，已经变得面目全非，就像传话游戏，传到最后意思全变了。 这导致机器人不仅没画得更好，反而画出来的东西越来越奇怪，甚至还不如层数少的时候画得好。

这就是原文开头说的 **“更深的神经网络更难训练”**。

### 作者提出的新方法：“残差学习”

作者们想出了一个绝妙的主意。他们不再让机器人从白纸开始画，而是这样做：

1.  **先给机器人一张“底稿”**：这张底稿就是最开始的输入信息（比如一个模糊的猫的轮廓）。
2.  **让机器人学习“修改”**：不再让每一层学习“如何画猫”，而是学习“如何在这张底稿上修改，能让它更像猫”。比如，A层学习“把耳朵画得更尖一点”，B层学习“把眼睛画得更大一点”，C层学习“把胡须加上去”。

机器人学习的不再是完整的猫，而是**与底稿之间的“差异”或“需要修改的部分”**（这在数学上称为“残差”）。

这就是原文所说的 **“参照层输入学习残差函数”**。

#### 这个新方法如何实现？

为了做到这一点，作者设计了一个“**跳跃连接**”（Shortcut Connection），也叫“捷径连接”。

这就像在绘画流程中开了一条“**绿色通道**”。原始的“底稿”会通过这条绿色通道，直接跳到后面，和经过很多层精细修改后的画稿“合并”。 这样一来，机器人随时都能看到最初的底稿是什么样子，不容易跑偏。即使中间某几层没学好（比如把尾巴画歪了），因为有原始底稿托底，最终结果也不会太离谱。

![Image of a residual block diagram](https://www.gstatic.com/images/branding/googlelogo/googlelogo_clr_74x24px.svg)

这个结构就是“残差网络”（ResNet）。

### 新方法带来的巨大成功

这个简单的改进带来了革命性的效果：

1.  **网络可以变得非常深**：因为不怕信息丢失了，作者成功训练了高达152层的网络，比之前流行的VGG网络（只有十几层）深了8倍，而且效果更好。
2.  **训练更容易，准确率更高**：这种网络更容易优化，准确率也得到了巨大提升。
3.  **赢得比赛冠军**：凭借这个方法，他们在2015年全球最权威的计算机视觉竞赛（ILSVRC和COCO）中横扫了图像分类、物体检测等多个项目的第一名。

---

### 总结一下这段话的大白话版本：

“我们发现，让神经网络学得太深，它会‘学糊涂’。所以我们换了个方法，不让它从零开始学一个复杂的东西，而是让它在一个‘基础版本’上学习‘如何修改得更好’。我们还设计了一条‘绿色通道’，确保它在修改时不会忘记‘基础版本’长啥样。结果证明这个方法非常成功，我们因此可以把网络建得非常非常深，让它变得极其聪明，轻松赢得了2015年好几个世界级的AI比赛冠军。”