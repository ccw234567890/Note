# 🧠 神经网络核心知识点笔记

> [!abstract] 学习路径总览
> 这份笔记将按照一个完整的项目流程来组织，带你走过从定义问题到最终应用的每一步：
> **问题定义 -> 模型构建 -> 模型训练 -> 模型应用**

---

## 1. 🎯 问题定义：识别手写数字 (MNIST)
(**Modified National Institute of Standards and Technology database**)
> [!note] 我们的起点和目标
> (源自: `DL is NOT a Toy.jpg`)

- **🎯 目标**: 我们的任务是训练一个模型，让它能够识别图片中的手写数字 (0 到 9)。
- **📚 数据集 (Dataset)**: 使用著名的 **MNIST** 数据集。
    - **内容**: 每个数字 (0-9) 都有约7000张手写体图片。
    - **图片规格**: 每张图片都是一个 `28x28` 像素的灰度图。
    - **数据划分**: 数据集被分为两部分：
        - **训练集 (Training set)**: **60,000** 张图片，用于训练模型。
        - **测试集 (Test set)**: **10,000** 张图片，用于在模型训练完成后评估其性能。

![[Pasted image 20250723221551.png]]

---

## 2. ⚙️ 模型构建：从线性函数到三层神经网络
> [!info] 搭建我们的“大脑”
> (源自: `NO deep learning, just function mapping.jpg`, `Non-linear Factor.jpg`)
>
> 我们的模型本质上是一个“函数映射”，它接收一个输入（图片），经过一系列计算，输出一个结果（分类）。

### a. 输入数据展平 (Input Flattening)

电脑无法直接理解 `28x28` 的二维图片，所以我们首先要将其**“展平”(Flatten)**成一个一维长向量。
- **计算**: $28 \times 28 = 784$
- **输入向量 $X$**: `X` 是一个包含784个元素的一维向量，代表了一张图片的所有像素值。
    - 维度: `[1, 784]`

### b. 核心构建块：线性层 + 激活函数

神经网络由多个“层”堆叠而成，每一层的核心是两个部分：

1.  **线性变换 (Linear Transformation)**:
    - **公式**: $y = Wx + b$
    - `W` (**权重 Weights**): 一个矩阵，可以被看作是特征的“重要性”或“连接强度”。
    - `b` (**偏置 Biases**): 一个向量，用于微调输出，增加模型的灵活性。

2.  **非线性激活 (Non-linear Activation)**:
> [!tip] 为什么需要非线性？
> 这是深度学习的**关键**！如果没有非线性激活函数，无论你堆叠多少层线性网络，其效果都等同于一个单层的线性网络，因此无法学习复杂的模式。

    - 常用函数:
        - ReLU (Rectified Linear Unit): 公式为 $ReLU(z) = max(0, z)$。它非常简单高效：小于0的值变为0，大于0的值保持不变。
        - Sigmoid: 将数值压缩到 `(0, 1)` 区间，常用于表示概率（图中也画出了它的S形曲线）。

![[Pasted image 20250723221645.png]]

### c. 构建一个三层神经网络

我们将三个这样的“层”连接起来，形成一个完整的前向传播路径：

- **第一层 (Hidden Layer 1)**:
    - `H1 = relu(X @ W1 + b1)`
    - `X` (输入): `[1, 784]`
    - `W1` (权重): `[784, d1]` (d1是自定义的神经元数量)
    - `b1` (偏置): `[d1]`
    - `H1` (输出): `[1, d1]`

- **第二层 (Hidden Layer 2)**:
    - `H2 = relu(H1 @ W2 + b2)`
    - `H1` (输入): `[1, d1]`
    - `W2` (权重): `[d1, d2]` (d2是自定义的神经元数量)
    - `b2` (偏置): `[d2]`
    - `H2` (输出): `[1, d2]`

- **第三层 (Output Layer)**:
    - `H3 = H2 @ W3 + b3`  *(注意：输出层通常在计算损失前不加ReLU)*
    - `H2` (输入): `[1, d2]`
    - `W3` (权重): `[d2, 10]` (输出维度必须是10，因为有10个类别0-9)
    - `b3` (偏置): `[10]`
    - `H3` or `pred` (最终预测): `[1, 10]`

![[Pasted image 20250723221806.png]]

---

## 3. 📉 训练模型：如何让模型学习
> [!warning] 让模型从错误中进步
> (源自: `Loss?.jpg`, `Gradient Descent.jpg`)
>
> 模型建好了，但里面的 `W` 和 `b` 都是随机的。我们需要一个方法来衡量模型的好坏，并指导它去优化这些参数。

### a. 损失函数 (Loss Function): 衡量“错得有多离谱”

- **目标 (Y)**: 我们需要一个标准答案来和模型的预测作比较。这个标准答案 `Y` 使用 **One-Hot 编码**。
> [!example] 什么是 One-Hot 编码?
> 一种将类别标签转换为向量的方法。向量的长度等于类别总数，只有对应类别的位置为1，其余为0。
> ```
> 标签 1 => [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
> 标签 3 => [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
> ```

- **计算损失**: 比较模型的预测 `pred` (一个`[1, 10]`的向量，如 `[0.1, 0.8, ...]`) 和真实标签 `Y` (一个One-Hot向量)。
    - **方法**: **欧氏距离** 或 **均方误差 (MSE)**
    - **公式**: $$Loss = \sum (pred - Y)^2$$
![[Pasted image 20250723221938.png]]

### b. 优化器 (Optimizer): 如何调整参数以减少损失

- **🎯 目标**: **最小化损失函数 (minimize objective)**。
- **⚙️ 方法**: **梯度下降 (Gradient Descent)**。
    - **核心思想**: 计算损失函数对于每一个参数 (`W1, b1, W2, b2, ...`) 的**梯度**（即导数）。梯度指明了能使损失函数增长最快的方向。
    - **更新规则**: 我们沿着梯度的**相反方向**去微调参数，这样就能让损失函数逐步降低。这个过程就像“盲人下山”，一步步走到山谷（损失最低点）。
- **🛠️ 需要优化的参数**: `[W1, W2, W3]` 和 `[b1, b2, b3]`。

![[Pasted image 20250723222005.png]]

---

## 4. 🚀 应用模型：进行预测 (Inference)
> [!success] 用训练好的模型大显身手
> (源自: `Inference.jpg`, `In a nutshell.png`)
>
> 当模型训练完成后（即 `W` 和 `b` 的值已经优化好），我们就可以用它来预测新的、从未见过的图片了。

1.  **前向传播 (Forward Pass)**:
    - 将新图片 `X` 输入到训练好的网络中。
    - 执行完整的计算:
    $$pred = relu(relu(XW_1+b_1)W_2 + b_2)W_3 + b_3$$
    - 得到一个 `[1, 10]` 的预测向量 `pred`，如 `[0.1, 0.8, 0.01, 0, 0.02, ...]`。向量中的每个值可以被看作是模型认为图片属于对应类别的**“置信度分数”**。

2.  **获取最终结果 (Argmax)**:
    - 我们只关心置信度最高的那个类别。
    - **`argmax(pred)`**: 这个函数会返回预测向量 `pred` 中最大值的**索引 (index)**。
    - **示例**:
        - `pred = [0.1, 0.8, 0.01, ...]`
        - 最大值是 `0.8`，它的索引位置是 `1`。
        - 因此，最终预测的**标签就是 `1`**。

![[Pasted image 20250723222042.png]]![[Pasted image 20250723222057.png]]