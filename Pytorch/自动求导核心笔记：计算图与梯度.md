---
title: "PyTorch 自动求导核心笔记：计算图与梯度"
date: 2025-07-26
tags:
  - PyTorch
  - DeepLearning
  - Autograd
  - ComputationGraph
---

# PyTorch 自动求导核心笔记：计算图与梯度

> [!NOTE] 核心摘要
> 本笔记旨在阐明 PyTorch 自动求导 (Autograd) 的两大基石：**计算图 (Computation Graph)** 和 **梯度 (Gradients)**。理解它们是掌握 PyTorch 训练神经网络的关键。

---

> [!ERROR] 核心痛点：为什么我的梯度计算失败了？
> 刚接触 PyTorch 时最常见的 `RuntimeError`，源于对计算图构建时机的误解。

##### 错误场景分析
1.  创建张量 `w` 时，其默认属性 `w.requires_grad` 为 `False`。
2.  基于此 `w` 计算损失 `mse`。由于 `w` 不被追踪，PyTorch **没有构建**关于 `w` 的计算图路径。
3.  **事后**将 `w.requires_grad` 设为 `True`，但这为时已晚。
4.  `mse` 张量本身不含有任何梯度函数 (`grad_fn`)，因此调用 `.backward()` 时，无法回溯到 `w`。

> [!FAIL] ❌ 错误的做法
> ```python
> # w 的 requires_grad 默认为 False
> w = torch.tensor([2.0])
> 
> # PyTorch 认为 w 无需追踪，因此 mse 没有 grad_fn
> mse = compute_loss(w) 
> 
> # 事后弥补，但 mse 已经定型，无法改变
> w.requires_grad_()    
> 
> # 报错! RuntimeError: element 0 of tensors does not require grad...
> mse.backward()        
> ```

> [!SUCCESS] ✅ 正确的做法
> 必须在张量**参与运算之前**就声明需要梯度追踪。
> ```python
> # 在创建时就指定 requires_grad=True
> w = torch.tensor([2.0], requires_grad=True) 
> 
> # PyTorch 会构建一个包含 w 的、完整的计算图
> mse = compute_loss(w) 
> 
> # 成功!
> mse.backward()        
> 
> # 查看梯度，结果被正确计算并存储
> print(w.grad)         
> ```

---
> [!INFO] 第一部分：什么是计算图 (Computation Graph)？
> 计算图是 PyTorch 自动求导机制的“大脑”，它是一个记录了所有数据和运算步骤的“流程图”。

- **定义**: 一个**有向无环图 (DAG)**，它详细记录了从输入张量到最终输出张量所经历的每一个操作。
- **比喻**: 就像一份烘焙食谱，记录了从“面粉、鸡蛋”（输入）经过“混合、烘烤”（操作）最终得到“蛋糕”（输出）的全过程。

> [!LIST] 计算图的组成
> - **🔵 节点 (Nodes)**: 代表两种东西：
>   - **数据节点**: 张量 (Tensor)，例如 `x` 和 `w`。
>   - **操作节点**: 数学运算 (Operation)，例如 `*` (乘法) 和 `F.mse_loss`。
> - **→ 边 (Edges)**: 代表数据的流动方向，显示了一个操作的输出如何成为下一个操作的输入。


> [!EXAMPLE] 示例：`mse = F.mse_loss(torch.ones(1), x * w)` 的计算图
> ```mermaid
> graph TD
>     subgraph "前向传播：构建计算图"
>     	direction LR
>     	x[("张量 x")] --> mul("操作 *")
>     	w[("张量 w <br> requires_grad=True")] --> mul
>     	ones[("张量 ones(1)")] --> mse_op("操作 F.mse_loss")
>     	mul --> mse_op
>     	mse_op --> mse(("最终结果 mse <br> 带有 grad_fn"))
>     end
>     
>     style w fill:#ffb8b8,stroke:#333,stroke-width:2px
>     style mse fill:#c1e1c1,stroke:#333,stroke-width:2px
> ```
> 上图清晰地展示了 `w` 是如何一步步影响到最终结果 `mse` 的。因为 `w` 需要梯度，所以 `mse` 也包含了如何反向传播到 `w` 的信息 (`grad_fn`)。

---

> [!QUESTION] 第二部分：计算图如何工作？
> 计算图的工作分为两个核心阶段：**前向传播**和**反向传播**。

##### ➡️ 前向传播 (Forward Pass) → 构建计算图
- **过程**: 从输入开始执行代码，一步步计算得到最终的输出（例如 `loss`）。
- **目的**: PyTorch 会像一个“书记员”，在你执行代码时**动态地、实时地构建**出计算图。只有当某个张量的 `requires_grad=True` 时，“书记员”才会记录与它相关的操作。

##### ⬅️ 反向传播 (Backward Pass) → 使用图计算梯度
- **触发**: 当你对一个需要梯度的输出张量（通常是 `loss`）调用 `.backward()` 方法时开始。
- **过程**: PyTorch 拿出之前构建好的计算图，从**终点 (`loss`) 开始，沿着图的边反向回溯**。
- **目的**: 在回溯的每一步，它都会利用**链式法则**来计算损失相对于每个参数的梯度（偏导数）。
- **结果**: 计算出的梯度值会被自动累加到相应参数的 `.grad` 属性中（例如 `w.grad`）。

---

> [!TIP] 💡 总结与关键要点 (Key Takeaways)
> 1.  ✅ **先声明，后运算**: 必须在张量参与运算**之前**就将其 `requires_grad` 属性设置为 `True`。这是所有后续步骤的前提。
> 2.  **动态性**: PyTorch 的计算图是**动态**的，在你运行代码时即时生成。这使得调试和编写复杂模型更加直观。
> 3.  ⚙️ **`.backward()` 是开关**: 这个函数是触发整个反向传播和梯度计算过程的“启动按钮”。
> 4.  💾 **`.grad` 是容器**: 调用 `.backward()` 后，所有计算出的梯度都会被累加到相应张量的 `.grad` 属性上。在下一次迭代前，记得用 `optimizer.zero_grad()` 将其清零。