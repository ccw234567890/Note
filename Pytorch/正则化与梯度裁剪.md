> [!NOTE] Title: 训练技巧：正则化与梯度裁剪
> ---

> [!TIP] ## 1. 正则化 (Regularization)
> **目标：防止模型过拟合 (Overfitting)，提升模型的泛化能力。**
> 
> > [!abstract] 核心思想：限制模型复杂度
> > 在损失函数中加入一个“惩罚项”，模型越复杂（权重越大），惩罚就越重。这迫使模型学习更简单、更普适的规律。
> > ```
> > Loss = 误差 + λ * 模型复杂度惩罚
> > ```
> > - `λ` 是一个超参数，用于控制惩罚的强度。

> [!INFO] ### 常见正则化方法
> 
> > [!todo]- **L2 正则化 (权重衰减 / Ridge)**
> > > **惩罚项**: 所有权重参数的**平方和** ($||w||_2^2$)。
> > > **效果**: 使权重参数趋向于变小，但通常不为0。让权重分布更平滑，是**最常用**的正则化方法。
> 
> > [!todo]- **L1 正则化 (Lasso)**
> > > **惩罚项**: 所有权重参数的**绝对值之和** ($||w||_1$)。
> > > **效果**: 能使一些不重要的特征权重变为**精确的0**，从而实现**特征选择**，让模型更稀疏。
> 
> > [!todo]- **Dropout (随机失活)**
> > > **工作方式**: 在训练的每次迭代中，随机“丢弃”一部分神经元及其连接。
> > > **效果**: 强迫网络学习更鲁棒的特征，不依赖于任何单一的神经元。可以看作是集成多个不同小网络的方法。

> [!TIP] ## 2. 梯度裁剪 (Gradient Clipping)
> **目标：防止梯度爆炸 (Exploding Gradients)，保证训练过程的稳定性。**
> 
> > [!abstract] 核心思想：给梯度设置一个上限
> > 在模型权重更新之前，检查梯度的范数（大小）。如果超过了预设的阈值，就按比例将其缩小，防止因梯度过大导致训练发散。
> >
> > ![Gradient Clipping](https://i.imgur.com/gB3I8Z4.png)
> 
> > [!INFO] ### 工作流程伪代码
> > ```python
> > # 1. 计算所有参数的梯度
> > grads = model.calculate_gradients()
> > 
> > # 2. 计算梯度的L2范数
> > total_norm = norm(grads)
> > 
> > # 3. 设定一个阈值
> > threshold = 1.0
> > 
> > # 4. 如果范数超过阈值，就进行裁剪
> > if total_norm > threshold:
> >     # 按比例缩小，保持方向不变
> >     clipping_ratio = threshold / total_norm
> >     for g in grads:
> >         g.multiply_by(clipping_ratio)
> >
> > # 5. 使用裁剪后的梯度更新模型权重
> > model.update_weights(grads)
> > ```

> [!SUMMARY] ## 核心区别对比
> | | **正则化 (Regularization)** | **梯度裁剪 (Gradient Clipping)** |
> | :--- | :--- | :--- |
> | **🎯 目标** | **防止过拟合**，提升泛化能力 | **防止梯度爆炸**，保证训练稳定 |
> | **作用对象** | **损失函数** (通过增加惩罚项) | **梯度本身** (直接修改梯度值) |
> | **作用阶段** | 模型**优化目标**的一部分 | 训练过程中的一个**干预步骤** |
> | **解决问题**| 模型的“学得太好”问题 | 训练过程的“步子迈太大”问题 |