# PyTorch深度学习核心路径总结

> [!abstract]
> 本笔记系统地梳理了从神经网络基础到前沿生成模型的完整学习路径。它不仅涵盖了各类模型的实现细节，更着重阐述了技术演进背后的核心思想，展现了现代深度学习的思考脉络与工程实践。

---

### Ⅰ. 基础构建与核心工作流

> [!info]
> 万丈高楼平地起，我们首先掌握了使用PyTorch进行开发的基本功。
> - **模型搭建**: 学习了通过继承 `nn.Module` 和利用 `nn.Sequential` 来创建自定义网络。
> - **自动管理**: 理解了PyTorch如何自动追踪参数 (`.parameters()`) 并与优化器 (`optimizer`) 无缝集成。
>
> > [!check] **核心架构：全连接网络 (FCN/MLP)**
> > 全连接网络是神经网络最基础的形态，其核心在于层与层之间神经元的“全部连接”。
> > - **主要应用1 (主力)**: 处理**结构化/表格数据**（如金融、医疗、商业数据），学习不同特征间的复杂关系。
> > - **主要应用2 (决策大脑)**: 担任CNN或RNN等复杂网络的**最终分类/回归头**，在高级特征被提取后进行汇总与决策。
>
> > [!tip] **核心实践技能**
> > - 设备迁移 (`.to(device)`)
> > - 状态保存与加载 (`state_dict`)
> > - 模式切换 (`.train()` / `.eval()`)
> > - 数据增强 (`transforms`)

### Ⅱ. 卷积神经网络 (CNN)：攻克空间数据

> [!example]
> 针对图像等空间数据，我们深入探讨了CNN的革命性设计。
> - **核心思想**: 通过 **局部感受野** 和 **权重共享** 两大创新，极大地减少了参数量并保留了空间信息。
> - **架构控制**: 掌握了通过卷积核 (Kernel)、步长 (Stride) 和填充 (Padding) 来精确控制网络结构与输出尺寸。
>
> > [!tip] **特定应用：全卷积网络 (FCN for Segmentation)**
> > 这是CNN的一种重要变体，它用卷积层**替换掉所有全连接层**，使其能接受任意尺寸的图像输入，并产生相应尺寸的**像素级输出**。
> > - **核心任务**: **语义分割**（为图像中的每个像素进行分类）、医学影像分析等。

### Ⅲ. 循环神经网络 (RNN/LSTM)：解读序列密码

> [!help]
> 对于文本、语音等序列数据，我们转向了RNN及其强大的变体LSTM。
> - **RNN**: 学习了其通过隐藏状态实现记忆，并通过权重共享处理变长序列的基本原理。
> - **核心挑战**: 深入分析了RNN在处理长序列时，因梯度累乘导致的梯度消失/爆炸问题。
> - **LSTM**: 作为解决方案，我们详细解析了LSTM如何通过“三门”结构（遗忘、输入、输出） 和独立的“细胞状态”，创建了一条梯度的“高速公路”，从而有效建模长期依赖。

### Ⅳ. 生成对抗网络 (GAN)：从博弈到创造

> [!tip]
> 在无监督学习领域，我们重点剖析了GAN及其演进过程，见证了AI如何学会“创造”。
>
> > [!check] **标准GAN**
> > - **核心机制**: 理解了生成器 (Generator) 与 判别器 (Discriminator) 之间的“猫鼠游戏”般的对抗博弈。
> > - **理论瓶颈**: 剖析了其损失函数与JS散度的关联，并指出了当两个分布不重叠时，JS散度失效并导致训练不稳定的根本原因。
>
> > [!success] **WGAN & WGAN-GP**
> > - **革命性改进**: 学习了更先进的Wasserstein GAN (WGAN)，它用更优越的Wasserstein距离（推土机距离）替代了脆弱的JS散度。
> > - **稳定训练**: 掌握了梯度惩罚 (Gradient Penalty) 这一关键技术，它通过直接约束判别器的梯度，从根本上解决了训练不稳定的问题，极大地提升了生成模型的性能和可靠性。

### Ⅴ. 总结：从“如何实现”到“为何如此”

> [!summary]
> 整个学习路径贯穿了一条清晰的主线：不断发现问题，并用更巧妙的结构和数学原理去解决问题。从CNN对FCN的超越，到LSTM对RNN的改进，再到WGAN对GAN的革新，我们不仅学习了模型的使用方法，更深刻理解了驱动深度学习技术发展的内在逻辑。