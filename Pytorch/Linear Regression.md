# 🧠 线性回归核心概念笔记

> [!note]
> 这份笔记整合了线性回归从 **基本定义** 到 **优化求解** 的完整流程，旨在帮助您快速建立起核心知识框架。

---

## 🎯 1. 核心目标：用直线拟合数据

> [!info] 核心模型：`y = wx + b`
> 线性回归的本质，就是寻找一个==线性函数==来描述输入特征 `x` 和输出 `y` 之间的关系。
>
> $$
> y = w \cdot x + b
> $$
>
> - **`w` (Weight)**: 权重或==斜率==，决定了直线的倾斜程度。
> - **`b` (Bias)**: 偏置或==y轴截距==，决定了直线在y轴上的位置。
>
> **我们的目标**：找到最优的 `w` 和 `b`。

![[Pasted image 20250723202727.png]]

---

## 🌪️ 2. 现实情况：数据带有噪声

> [!warning] 警告：现实世界并非完美
> 在真实世界的数据中，几乎总是存在随机==噪声==或测量==误差==。数据点不会完美地落在一条直线上。
>
> 因此，我们的模型需要加入一个误差项 `ε` (epsilon):
>
> $$
> y = w \cdot x + b + \epsilon
> $$
>
> 这也解释了为什么数据点通常分布在直线的**周围**。

![[Pasted image 20250723202828.png]]

---

## ⚖️ 3. 如何衡量“最佳”：损失函数 (Loss Function)

> [!question] 如何量化一条直线的“好坏”？
> 我们需要一个标准来衡量预测值与真实值之间的差距。这个标准就是 **损失函数 (Loss Function)**。
>
> 对于线性回归，最常用的损失函数是 ==均方误差 (Mean Squared Error, MSE)==，它计算了所有数据点的 **预测误差的平方和**。
>
> $$
> \text{Loss} = \sum_{i=1}^{n} (y_{\text{predicted}} - y_{\text{actual}})^2 = \sum_{i=1}^{n} (w \cdot x_i + b - y_i)^2
> $$
>
> **核心任务**：找到能让 `Loss` 值==最小==的 `w` 和 `b`。

---

## 💡 4. 如何找到最优解：启发式搜索与凸优化

### 💎 关键特性：凸优化 (Convex Optimization)

> [!tip] 好消息：我们一定能找到最优解！
> 将 `Loss` 与参数 `w` 和 `b` 的关系可视化，会得到一个三维的**损失曲面 (Loss Surface)**。
>
> - **形状**: 这个曲面是一个光滑的 ==“碗”状==。在数学上，这被称为**凸函数**。
> - **优点**: 凸函数最棒的特性是它**只有一个全局最低点**，没有局部陷阱。这意味着我们只要找到碗底，就一定是全局最优解！

![[Pasted image 20250723202925.png]]

### 🧭 求解方法：梯度下降 (Gradient Descent)

> [!example] 策略：像“下山”一样寻找最低点
> **梯度下降** 是一种迭代优化算法，直观上就像一个人在山坡上，为了最快到达谷底，每一步都选择==最陡峭的下坡方向==行走。
> 1.  **随机出发**: 随机初始化一组 `w` 和 `b`。
> 2.  **寻找方向**: 计算当前位置的**梯度**（坡度最陡的方向）。
> 3.  **迈出一步**: 沿着梯度的**反方向**（下山方向）更新 `w` 和 `b`。
> 4.  **循环往复**: 重复步骤 2 和 3，直到 `Loss` 值不再显著下降，即到达碗底。

![[Pasted image 20250723203002.png]]

---

## 📊 5. 学习过程与结果可视化

> [!success] 学习成果
> 经过足够的迭代（例如100次），梯度下降算法会收敛到一个最优解。
> - **左图 (参数空间)**: 展示了参数 `w` 和 `b` 从初始点一步步走向最优点的路径。
> - **右图 (数据空间)**: 展示了用最终学到的 `w` 和 `b` 绘制的直线，它完美地拟合了数据的整体趋势。

![[Pasted image 20250723203029.png]]

---

## 🗺️ 6. 知识归类：线性回归的应用场景

> [!summary] 回归 vs 分类
> - **线性回归 (Linear Regression)**:
>   - **任务**: ==回归 (Regression)==
>   - **目标**: 预测一个**连续的数值** (e.g., 房价, 气温)。
>   - **输出范围**: `(-∞, +∞)`
>
> - **逻辑回归 (Logistic Regression)**:
>   - **任务**: ==分类 (Classification)==
>   - **目标**: 预测一个**离散的类别** (e.g., 是/否, 猫/狗)，通常以概率形式输出。
>   - **输出范围**: `[0, 1]`

![[Pasted image 20250723203046.png]]

---

## ✨ 总结与快速记忆要点

### 流程图

```mermaid
graph TD
    A[Start: 带有噪声的数据点] --> B{1. 定义模型<br>y = wx + b};
    B --> C{2. 定义损失函数<br>Loss = Σ(wx+b-y)²};
    C --> D{3. 损失函数是凸函数吗?};
    D -- Yes --> E[碗状结构, 有唯一全局最优解];
    E --> F{4. 使用梯度下降法<br>迭代寻找最低点};
    F --> G[得到最优 w* 和 b*];
    G --> H[End: 获得最佳拟合直线];
    style A fill:#ffadad,stroke:#333
    style H fill:#9bf6ff,stroke:#333
    style F fill:#fdffb6
    style E fill:#caffbf
```

### 核心知识表格

> [!tldr] 一句话总结
> 线性回归就是**为了最小化均方误差（一种凸函数），通过梯度下降法，找到最佳的 `w` 和 `b` 来拟合数据**。

| 概念       | 核心内容                                      | 关联 Emoji |
| :------- | :---------------------------------------- | :------- |
| **问题定义** | 用直线 `y = wx + b` 去拟合带噪声的数据。               | 📈       |
| **衡量标准** | 定义**损失函数** `Loss = Σ(wx + b - y)²`，值越小越好。 | ⚖️       |
| **求解特性** | 损失函数是**凸函数**（碗状），保证有唯一的全局最优解。             | 🥣       |
| **求解方法** | 使用**梯度下降**（启发式搜索）像“下山”一样，逐步找到碗底。          | ⛰️🚶‍♂️  |
| **最终结果** | 找到一组最优的 `(w, b)`，得到最佳拟合直线。                | ✅        |
| **应用场景** | **回归任务**：预测连续值（如房价、温度）。                   | 🏠🌡️    |