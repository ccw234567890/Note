# 学习笔记：如何计算梯度？从基础到链式法则

> [!NOTE] 核心目标
> 我们已经知道需要用**梯度**来指导模型优化。本篇笔记将深入学习**如何手动计算梯度**，这个过程是理解**反向传播 (Backpropagation)** 算法的基石。

---
## 第一部分：基础工具 - 常用函数求导表

> [!info] 核心求导规则表 🧰
> 任何复杂的求导，最终都会分解为对基础函数的求导。这张表就是我们的“乘法口诀表”。
>![[Pasted image 20250724225850.png]]

---
## 第二部分：小试牛刀 - 计算简单函数的梯度

> [!example] 直接应用求导规则
> **核心技巧**: 在对一个变量（比如 `w`）求偏导时，要把所有其他变量（比如 `b` 和 `x`）都**==视为常数==**。
> 
> ### 示例 A：线性函数
> ![[Pasted image 20250724225912.png]]
> 对于函数 $y = xw + b$，梯度 $\nabla y = (\frac{\partial y}{\partial w}, \frac{\partial y}{\partial b})$ 的计算如下：
> - $\frac{\partial y}{\partial w} = x$  *(将 `xw` 对 `w` 求导，`b` 视为常数得0)*
> - $\frac{\partial y}{\partial b} = 1$  *(将 `b` 对 `b` 求导，`xw` 视为常数得0)*
> - **梯度**: $\nabla y = (x, 1)$
> 
> ### 示例 B & C：其他函数
> 使用同样的原则，我们可以轻松计算更复杂的函数：
> 
> | 函数 (Function) | 梯度 (Gradient) | 对应图片 |
> | :--- | :--- | :--- |
> | $y = xw^2 + b^2$ | $\nabla y = (2xw, 2b)$ | ![[Pasted image 20250724225941.png]] |
> | $y = xe^w + e^b$ | $\nabla y = (xe^w, e^b)$ |![[Pasted image 20250724230020.png]] |

---
## 第三部分：核心武器 - 链式法则 (Chain Rule)

> [!warning] 核心思想：处理层层嵌套的函数 🧠
> 在深度学习中，函数通常是**复合函数**（函数套函数），如 `Loss(Output(Parameters))`。这时，我们必须使用**链式法则**。
> 
> **链式法则公式**: 对于复合函数 $f(g(x))$，其导数为“==先求外层导，再乘内层导==”。
> $$ \frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx} $$

> [!example] 链式法则应用
> ### 示例 D：均方误差损失函数 (MSE Loss)
> ![[Pasted image 20250724230050.png]]
> 我们来计算损失函数 $f = (y - (xw+b))^2$ 对 `w` 和 `b` 的梯度。
> 
> - **分解函数**:
>   - 外层函数: $f(g) = (y-g)^2$
>   - 内层函数: $g(w, b) = xw + b$
> - **分别求导**:
>   - 外层导数: $\frac{df}{dg} = 2(y-g) \cdot (-1) = -2(y-g)$
>   - 内层导数: $\frac{\partial g}{\partial w} = x$ 和 $\frac{\partial g}{\partial b} = 1$
> 
> > [!danger] 注意：原图计算存在笔误！
> > 原图在计算 $\frac{df}{dg}$ 时，似乎漏掉了 `(y-g)` 对 `g` 求导所产生的 `-1` 因子。以下是正确的推导：
> 
> - **应用链式法则 (正确推导)**:
>   - $\frac{\partial f}{\partial w} = \frac{df}{dg} \cdot \frac{\partial g}{\partial w} = [-2(y-g)] \cdot x = -2(y - (xw+b))x$
>   - $\frac{\partial f}{\partial b} = \frac{df}{dg} \cdot \frac{\partial g}{\partial b} = [-2(y-g)] \cdot 1 = -2(y - (xw+b))$
> 
> ### 示例 E：对数损失函数 (Log Loss)
>![[Pasted image 20250724230113.png]]
> 计算函数 $f = y \cdot \ln(xw+b)$ 的梯度。
> - **分解函数**:
>   - 外层函数: $f(g) = y \cdot \ln(g)$
>   - 内层函数: $g(w, b) = xw+b$
> - **分别求导**:
>   - 外层导数: $\frac{df}{dg} = y \cdot \frac{1}{g}$
>   - 内层导数: $\frac{\partial g}{\partial w} = x$ 和 $\frac{\partial g}{\partial b} = 1$
> - **应用链式法则**:
>   - $\frac{\partial f}{\partial w} = \frac{df}{dg} \cdot \frac{\partial g}{\partial w} = \frac{y}{g} \cdot x = \frac{yx}{xw+b}$
>   - $\frac{\partial f}{\partial b} = \frac{df}{dg} \cdot \frac{\partial g}{\partial b} = \frac{y}{g} \cdot 1 = \frac{y}{xw+b}$
> > [!check] 这次，原图中的计算是完全正确的。

---
> [!summary] 总结
> 计算梯度是一个“机械”但至关重要的过程，它完全依赖于：
> 1.  ✅ **掌握基础函数的求导规则**。
> 2.  ✅ **理解偏导数的概念**（将其他变量视为常数）。
> 3.  ✅ **熟练运用链式法则**来处理层层嵌套的复合函数。
> 
> ==这个系统性地、从后往前应用链式法则计算梯度的过程，就是大名鼎鼎的**反向传播算法 (Backpropagation)**。==