# 学习笔记：神经网络核心组件 （CCW版)

> [!NOTE] 核心纲要
> 这份笔记将带您完整地了解构成神经网络并使其能够学习的核心组件。我们将依次深入探讨：
> 1.  **激活函数**: 赋予网络非线性学习能力的“开关”。
> 2.  **损失函数**: 衡量模型预测好坏的“标尺”。
> 3.  **自动求导**: PyTorch中实现自动优化的“引擎”。

---
## 第一部分：神经网络的“开关” - 激活函数 🧠

> [!info] 什么是激活函数？
> ![[Pasted image 20250724232329.png]]
> 在神经元接收所有输入并进行**加权求和**后，其结果会经过一个**激活函数** `f(·)` 的处理，得到最终的输出信号。
> 
> **核心作用**: ==为网络引入非线性==。没有激活函数，无论网络多深，都只是一个简单的线性模型，无法学习复杂的现实世界规律。

---
### 激活函数的演进之路

#### 1. “老祖宗”：阶跃函数 (Step Function)

> [!danger] 致命缺陷：无法求导
> ![[Pasted image 20250724232405.png]]
> - **逻辑**: 简单的“开/关”逻辑，大于阈值输出1，否则输出0。
> - **缺陷**: 函数不连续，其导数几乎处处为零。这意味着梯度无法传播，模型无法通过梯度下降进行学习。

#### 2. “经典之选”：Sigmoid 函数

> [!example] Sigmoid: 平滑的概率化开关
> ![[Pasted image 20250724232432.png]]
> - **公式**: $\sigma(x) = \frac{1}{1 + e^{-x}}$
> - **特性**:
>   - **平滑可导**：解决了阶跃函数的最大问题。
>   - **输出范围 (0, 1)**：能将任意输入“压缩”到 (0, 1) 之间，常用于表示概率或二分类任务的输出。
> - **导数**: $\sigma' = \sigma(1-\sigma)$，计算高效。
> 
> > [!tip] 代码示例
> > ![[Pasted image 20250724232517.png]]

#### 3. “改进之选”：Tanh (双曲正切) 函数

> [!example] Tanh: 零中心的Sigmoid
> ![[Pasted image 20250724232643.png]]
> - **公式**: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
> - **特性**:
>   - **输出范围 (-1, 1)**。
>   - **==零中心 (Zero-centered)==**: 输出均值为0，这一特性在实践中通常能让模型**收敛得更快**，因此在隐藏层中表现常优于Sigmoid。
> - **导数**: $\tanh'(x) = 1 - \tanh^2(x)$。
> 
> > [!tip] 代码示例
> >![[Pasted image 20250724232713.png]]

#### 4. “现代默认之选”：ReLU (修正线性单元)

> [!success] ReLU: 简单、高效、缓解梯度消失
> ![[Pasted image 20250724232754.png]]
> - **公式**: $f(x) = \max(0, x)$
> - **核心优势**:
>   1.  **计算极其高效**：仅需一个比较操作。
>   2.  **==有效缓解梯度消失==**：当输入为正数时，其梯度恒为 **1**。这使得梯度能够顺畅地在深度网络中传播，极大地加速了训练。
> - **导数**:
> ![[Pasted image 20250724232826.png]]
> 
> > [!tip] 代码示例
> > ![[Pasted image 20250724232909.png]]

---
## 第二部分：衡量误差的“标尺” - 损失函数 📏

> [!info] 什么是损失函数？
> **损失函数 (Loss Function)** 是一个用来计算**模型预测值**与**真实标签**之间差距的函数。
> 
> **训练目标**: ==通过调整模型参数，让损失函数的值变得尽可能小。==

### 深入理解：均方误差 (Mean Squared Error, MSE)

> [!example] MSE: 回归任务的常用标尺
>![[Pasted image 20250724233017.png]]
> - **核心公式**: $\text{loss} = \sum (\text{真实值} - \text{预测值})^2 = \sum [y - f_\theta(x)]^2$
> - **作用**: 主要用于**回归任务**。通过计算误差的平方和，来衡量模型预测的准确性。

### 损失函数的梯度：连接误差与优化

> [!abstract] 梯度的桥梁作用
> ![[Pasted image 20250724233118.png]]
> - **公式**: $\frac{\nabla \text{loss}}{\nabla \theta} = -2 \sum \underbrace{[y - f_\theta(x)]}_{\text{误差}} \cdot \underbrace{\frac{\nabla f_\theta(x)}{\nabla \theta}}_{\text{模型输出对参数的梯度}}$
> - **深刻含义**: 这个公式是整个学习过程的动力源泉。它告诉我们，最终的**误差**信号，是如何通过**链式法则**，一步步地反向传播，并指导网络中的**每一个参数**应该如何进行微调，从而让总误差减小。

---
## 第三部分：自动化的“引擎” - PyTorch Autograd ⚙️

> [!info] 什么是 Autograd？
> 我们无需手动计算上面复杂的梯度。PyTorch 的 **`autograd`** 引擎会通过构建**动态计算图**来自动完成这个过程。

### 1. 黄金法则：开启梯度追踪 (`requires_grad`)

> [!warning] 必须先开启追踪！
> 要想让 PyTorch 自动求导，必须在计算发生**之前**，将参数张量的 `.requires_grad` 属性设置为 `True`。
> `w.requires_grad_(True)`

### 2. 计算梯度的两大API

> [!tip] API对比
> ![[Pasted image 20250724233253.png]]
> - **`loss.backward()` (推荐)**: 最常用的方法。计算梯度并将其**存储**在参数的 `.grad` 属性中。
> - **`torch.autograd.grad()`**: 更底层的接口，直接将梯度作为结果**返回**。

> [!example] 代码中的常见陷阱与正确流程
> ![[Pasted image 20250724233314.png]]
> - **错误原因**: 在计算损失 `mse` 时，参数 `w` 还没有开启梯度追踪。
> - **正确流程**: ==必须先设置 `w.requires_grad_(True)`，然后再用这个 `w` 去计算损失 `mse`==，最后才能成功调用 `mse.backward()` 或 `autograd.grad`。

---
## 第四部分：多分类“标准答案” - Softmax 函数 🎯

> [!info] 什么是 Softmax？
> ![[Pasted image 20250724233333.png]]
> **Softmax** 能将一个包含任意分数的向量（logits），转换成一个**概率分布**。
> 
> **关键特性**:
> 1.  所有输出值都在 `[0, 1]` 之间。
> 2.  所有输出值的总和为 `1`。

### Softmax 的数学与代码实践

> [!abstract] Softmax 的导数
>![[Pasted image 20250724233413.png]]
> Softmax 的导数比较复杂，分为 `i = j` 和 `i ≠ j` 两种情况。在实践中，我们通常将 **Softmax** 和 **交叉熵损失** 结合使用，它们的组合梯度形式非常简洁。

> [!warning] PyTorch实践中的 `retain_graph`
> ![[Pasted image 20250724233503.png]]
> - **默认行为**: 调用 `.backward()` 或 `autograd.grad()` 后，计算图会被**立即释放**以节省内存。
> - **`retain_graph=True`**: 如果你需要对同一个计算结果**多次**执行梯度计算，就必须在调用时传入此参数，以==强制保留计算图==。