# å­¦ä¹ ç¬”è®°ï¼šç¥žç»ç½‘ç»œæ ¸å¿ƒç»„ä»¶ ï¼ˆCCWç‰ˆ)

> [!NOTE] æ ¸å¿ƒçº²è¦
> è¿™ä»½ç¬”è®°å°†å¸¦æ‚¨å®Œæ•´åœ°äº†è§£æž„æˆç¥žç»ç½‘ç»œå¹¶ä½¿å…¶èƒ½å¤Ÿå­¦ä¹ çš„æ ¸å¿ƒç»„ä»¶ã€‚æˆ‘ä»¬å°†ä¾æ¬¡æ·±å…¥æŽ¢è®¨ï¼š
> 1.  **æ¿€æ´»å‡½æ•°**: èµ‹äºˆç½‘ç»œéžçº¿æ€§å­¦ä¹ èƒ½åŠ›çš„â€œå¼€å…³â€ã€‚
> 2.  **æŸå¤±å‡½æ•°**: è¡¡é‡æ¨¡åž‹é¢„æµ‹å¥½åçš„â€œæ ‡å°ºâ€ã€‚
> 3.  **è‡ªåŠ¨æ±‚å¯¼**: PyTorchä¸­å®žçŽ°è‡ªåŠ¨ä¼˜åŒ–çš„â€œå¼•æ“Žâ€ã€‚

---
## ç¬¬ä¸€éƒ¨åˆ†ï¼šç¥žç»ç½‘ç»œçš„â€œå¼€å…³â€ - æ¿€æ´»å‡½æ•° ðŸ§ 

> [!info] ä»€ä¹ˆæ˜¯æ¿€æ´»å‡½æ•°ï¼Ÿ
> ![[Pasted image 20250724232329.png]]
> åœ¨ç¥žç»å…ƒæŽ¥æ”¶æ‰€æœ‰è¾“å…¥å¹¶è¿›è¡Œ**åŠ æƒæ±‚å’Œ**åŽï¼Œå…¶ç»“æžœä¼šç»è¿‡ä¸€ä¸ª**æ¿€æ´»å‡½æ•°** `f(Â·)` çš„å¤„ç†ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºä¿¡å·ã€‚
> 
> **æ ¸å¿ƒä½œç”¨**: ==ä¸ºç½‘ç»œå¼•å…¥éžçº¿æ€§==ã€‚æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæ— è®ºç½‘ç»œå¤šæ·±ï¼Œéƒ½åªæ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡åž‹ï¼Œæ— æ³•å­¦ä¹ å¤æ‚çš„çŽ°å®žä¸–ç•Œè§„å¾‹ã€‚

---
### æ¿€æ´»å‡½æ•°çš„æ¼”è¿›ä¹‹è·¯

#### 1. â€œè€ç¥–å®—â€ï¼šé˜¶è·ƒå‡½æ•° (Step Function)

> [!danger] è‡´å‘½ç¼ºé™·ï¼šæ— æ³•æ±‚å¯¼
> ![[Pasted image 20250724232405.png]]
> - **é€»è¾‘**: ç®€å•çš„â€œå¼€/å…³â€é€»è¾‘ï¼Œå¤§äºŽé˜ˆå€¼è¾“å‡º1ï¼Œå¦åˆ™è¾“å‡º0ã€‚
> - **ç¼ºé™·**: å‡½æ•°ä¸è¿žç»­ï¼Œå…¶å¯¼æ•°å‡ ä¹Žå¤„å¤„ä¸ºé›¶ã€‚è¿™æ„å‘³ç€æ¢¯åº¦æ— æ³•ä¼ æ’­ï¼Œæ¨¡åž‹æ— æ³•é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œå­¦ä¹ ã€‚

#### 2. â€œç»å…¸ä¹‹é€‰â€ï¼šSigmoid å‡½æ•°

> [!example] Sigmoid: å¹³æ»‘çš„æ¦‚çŽ‡åŒ–å¼€å…³
> ![[Pasted image 20250724232432.png]]
> - **å…¬å¼**: $\sigma(x) = \frac{1}{1 + e^{-x}}$
> - **ç‰¹æ€§**:
>   - **å¹³æ»‘å¯å¯¼**ï¼šè§£å†³äº†é˜¶è·ƒå‡½æ•°çš„æœ€å¤§é—®é¢˜ã€‚
>   - **è¾“å‡ºèŒƒå›´ (0, 1)**ï¼šèƒ½å°†ä»»æ„è¾“å…¥â€œåŽ‹ç¼©â€åˆ° (0, 1) ä¹‹é—´ï¼Œå¸¸ç”¨äºŽè¡¨ç¤ºæ¦‚çŽ‡æˆ–äºŒåˆ†ç±»ä»»åŠ¡çš„è¾“å‡ºã€‚
> - **å¯¼æ•°**: $\sigma' = \sigma(1-\sigma)$ï¼Œè®¡ç®—é«˜æ•ˆã€‚
> 
> > [!tip] ä»£ç ç¤ºä¾‹
> > ![[Pasted image 20250724232517.png]]

#### 3. â€œæ”¹è¿›ä¹‹é€‰â€ï¼šTanh (åŒæ›²æ­£åˆ‡) å‡½æ•°

> [!example] Tanh: é›¶ä¸­å¿ƒçš„Sigmoid
> ![[Pasted image 20250724232643.png]]
> - **å…¬å¼**: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
> - **ç‰¹æ€§**:
>   - **è¾“å‡ºèŒƒå›´ (-1, 1)**ã€‚
>   - **==é›¶ä¸­å¿ƒ (Zero-centered)==**: è¾“å‡ºå‡å€¼ä¸º0ï¼Œè¿™ä¸€ç‰¹æ€§åœ¨å®žè·µä¸­é€šå¸¸èƒ½è®©æ¨¡åž‹**æ”¶æ•›å¾—æ›´å¿«**ï¼Œå› æ­¤åœ¨éšè—å±‚ä¸­è¡¨çŽ°å¸¸ä¼˜äºŽSigmoidã€‚
> - **å¯¼æ•°**: $\tanh'(x) = 1 - \tanh^2(x)$ã€‚
> 
> > [!tip] ä»£ç ç¤ºä¾‹
> >![[Pasted image 20250724232713.png]]

#### 4. â€œçŽ°ä»£é»˜è®¤ä¹‹é€‰â€ï¼šReLU (ä¿®æ­£çº¿æ€§å•å…ƒ)

> [!success] ReLU: ç®€å•ã€é«˜æ•ˆã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
> ![[Pasted image 20250724232754.png]]
> - **å…¬å¼**: $f(x) = \max(0, x)$
> - **æ ¸å¿ƒä¼˜åŠ¿**:
>   1.  **è®¡ç®—æžå…¶é«˜æ•ˆ**ï¼šä»…éœ€ä¸€ä¸ªæ¯”è¾ƒæ“ä½œã€‚
>   2.  **==æœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±==**ï¼šå½“è¾“å…¥ä¸ºæ­£æ•°æ—¶ï¼Œå…¶æ¢¯åº¦æ’ä¸º **1**ã€‚è¿™ä½¿å¾—æ¢¯åº¦èƒ½å¤Ÿé¡ºç•…åœ°åœ¨æ·±åº¦ç½‘ç»œä¸­ä¼ æ’­ï¼Œæžå¤§åœ°åŠ é€Ÿäº†è®­ç»ƒã€‚
> - **å¯¼æ•°**:
> ![[Pasted image 20250724232826.png]]
> 
> > [!tip] ä»£ç ç¤ºä¾‹
> > ![[Pasted image 20250724232909.png]]

---
## ç¬¬äºŒéƒ¨åˆ†ï¼šè¡¡é‡è¯¯å·®çš„â€œæ ‡å°ºâ€ - æŸå¤±å‡½æ•° ðŸ“

> [!info] ä»€ä¹ˆæ˜¯æŸå¤±å‡½æ•°ï¼Ÿ
> **æŸå¤±å‡½æ•° (Loss Function)** æ˜¯ä¸€ä¸ªç”¨æ¥è®¡ç®—**æ¨¡åž‹é¢„æµ‹å€¼**ä¸Ž**çœŸå®žæ ‡ç­¾**ä¹‹é—´å·®è·çš„å‡½æ•°ã€‚
> 
> **è®­ç»ƒç›®æ ‡**: ==é€šè¿‡è°ƒæ•´æ¨¡åž‹å‚æ•°ï¼Œè®©æŸå¤±å‡½æ•°çš„å€¼å˜å¾—å°½å¯èƒ½å°ã€‚==

### æ·±å…¥ç†è§£ï¼šå‡æ–¹è¯¯å·® (Mean Squared Error, MSE)

> [!example] MSE: å›žå½’ä»»åŠ¡çš„å¸¸ç”¨æ ‡å°º
>![[Pasted image 20250724233017.png]]
> - **æ ¸å¿ƒå…¬å¼**: $\text{loss} = \sum (\text{çœŸå®žå€¼} - \text{é¢„æµ‹å€¼})^2 = \sum [y - f_\theta(x)]^2$
> - **ä½œç”¨**: ä¸»è¦ç”¨äºŽ**å›žå½’ä»»åŠ¡**ã€‚é€šè¿‡è®¡ç®—è¯¯å·®çš„å¹³æ–¹å’Œï¼Œæ¥è¡¡é‡æ¨¡åž‹é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

### æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼šè¿žæŽ¥è¯¯å·®ä¸Žä¼˜åŒ–

> [!abstract] æ¢¯åº¦çš„æ¡¥æ¢ä½œç”¨
> ![[Pasted image 20250724233118.png]]
> - **å…¬å¼**: $\frac{\nabla \text{loss}}{\nabla \theta} = -2 \sum \underbrace{[y - f_\theta(x)]}_{\text{è¯¯å·®}} \cdot \underbrace{\frac{\nabla f_\theta(x)}{\nabla \theta}}_{\text{æ¨¡åž‹è¾“å‡ºå¯¹å‚æ•°çš„æ¢¯åº¦}}$
> - **æ·±åˆ»å«ä¹‰**: è¿™ä¸ªå…¬å¼æ˜¯æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹çš„åŠ¨åŠ›æºæ³‰ã€‚å®ƒå‘Šè¯‰æˆ‘ä»¬ï¼Œæœ€ç»ˆçš„**è¯¯å·®**ä¿¡å·ï¼Œæ˜¯å¦‚ä½•é€šè¿‡**é“¾å¼æ³•åˆ™**ï¼Œä¸€æ­¥æ­¥åœ°åå‘ä¼ æ’­ï¼Œå¹¶æŒ‡å¯¼ç½‘ç»œä¸­çš„**æ¯ä¸€ä¸ªå‚æ•°**åº”è¯¥å¦‚ä½•è¿›è¡Œå¾®è°ƒï¼Œä»Žè€Œè®©æ€»è¯¯å·®å‡å°ã€‚

---
## ç¬¬ä¸‰éƒ¨åˆ†ï¼šè‡ªåŠ¨åŒ–çš„â€œå¼•æ“Žâ€ - PyTorch Autograd âš™ï¸

> [!info] ä»€ä¹ˆæ˜¯ Autogradï¼Ÿ
> æˆ‘ä»¬æ— éœ€æ‰‹åŠ¨è®¡ç®—ä¸Šé¢å¤æ‚çš„æ¢¯åº¦ã€‚PyTorch çš„ **`autograd`** å¼•æ“Žä¼šé€šè¿‡æž„å»º**åŠ¨æ€è®¡ç®—å›¾**æ¥è‡ªåŠ¨å®Œæˆè¿™ä¸ªè¿‡ç¨‹ã€‚

### 1. é»„é‡‘æ³•åˆ™ï¼šå¼€å¯æ¢¯åº¦è¿½è¸ª (`requires_grad`)
> [!warning] å¿…é¡»å…ˆå¼€å¯è¿½è¸ªï¼ > è¦æƒ³è®© PyTorch è‡ªåŠ¨æ±‚å¯¼ï¼Œå¿…é¡»åœ¨è®¡ç®—å‘ç”Ÿ**ä¹‹å‰**ï¼Œå°†å‚æ•°å¼ é‡çš„ `.requires_grad` å±žæ€§è®¾ç½®ä¸º `True`ã€‚ > `w.requires_grad_(True)`



### 2. è®¡ç®—æ¢¯åº¦çš„ä¸¤å¤§API

> [!tip] APIå¯¹æ¯”
> ![[Pasted image 20250724233253.png]]
> - **`loss.backward()` (æŽ¨è)**: æœ€å¸¸ç”¨çš„æ–¹æ³•ã€‚è®¡ç®—æ¢¯åº¦å¹¶å°†å…¶**å­˜å‚¨**åœ¨å‚æ•°çš„ `.grad` å±žæ€§ä¸­ã€‚
> - **`torch.autograd.grad()`**: æ›´åº•å±‚çš„æŽ¥å£ï¼Œç›´æŽ¥å°†æ¢¯åº¦ä½œä¸ºç»“æžœ**è¿”å›ž**ã€‚

> [!example] ä»£ç ä¸­çš„å¸¸è§é™·é˜±ä¸Žæ­£ç¡®æµç¨‹
> ![[Pasted image 20250724233314.png]]
> - **é”™è¯¯åŽŸå› **: åœ¨è®¡ç®—æŸå¤± `mse` æ—¶ï¼Œå‚æ•° `w` è¿˜æ²¡æœ‰å¼€å¯æ¢¯åº¦è¿½è¸ªã€‚
> - **æ­£ç¡®æµç¨‹**: ==å¿…é¡»å…ˆè®¾ç½® `w.requires_grad_(True)`ï¼Œç„¶åŽå†ç”¨è¿™ä¸ª `w` åŽ»è®¡ç®—æŸå¤± `mse`==ï¼Œæœ€åŽæ‰èƒ½æˆåŠŸè°ƒç”¨ `mse.backward()` æˆ– `autograd.grad`ã€‚

---
## ç¬¬å››éƒ¨åˆ†ï¼šå¤šåˆ†ç±»â€œæ ‡å‡†ç­”æ¡ˆâ€ - Softmax å‡½æ•° ðŸŽ¯

> [!info] ä»€ä¹ˆæ˜¯ Softmaxï¼Ÿ
> ![[Pasted image 20250724233333.png]]
> **Softmax** èƒ½å°†ä¸€ä¸ªåŒ…å«ä»»æ„åˆ†æ•°çš„å‘é‡ï¼ˆlogitsï¼‰ï¼Œè½¬æ¢æˆä¸€ä¸ª**æ¦‚çŽ‡åˆ†å¸ƒ**ã€‚
> 
> **å…³é”®ç‰¹æ€§**:
> 1.  æ‰€æœ‰è¾“å‡ºå€¼éƒ½åœ¨ `[0, 1]` ä¹‹é—´ã€‚
> 2.  æ‰€æœ‰è¾“å‡ºå€¼çš„æ€»å’Œä¸º `1`ã€‚

### Softmax çš„æ•°å­¦ä¸Žä»£ç å®žè·µ

> [!abstract] Softmax çš„å¯¼æ•°
>![[Pasted image 20250724233413.png]]
> Softmax çš„å¯¼æ•°æ¯”è¾ƒå¤æ‚ï¼Œåˆ†ä¸º `i = j` å’Œ `i â‰  j` ä¸¤ç§æƒ…å†µã€‚åœ¨å®žè·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å°† **Softmax** å’Œ **äº¤å‰ç†µæŸå¤±** ç»“åˆä½¿ç”¨ï¼Œå®ƒä»¬çš„ç»„åˆæ¢¯åº¦å½¢å¼éžå¸¸ç®€æ´ã€‚

> [!warning] PyTorchå®žè·µä¸­çš„ `retain_graph`
> ![[Pasted image 20250724233503.png]]
> - **é»˜è®¤è¡Œä¸º**: è°ƒç”¨ `.backward()` æˆ– `autograd.grad()` åŽï¼Œè®¡ç®—å›¾ä¼šè¢«**ç«‹å³é‡Šæ”¾**ä»¥èŠ‚çœå†…å­˜ã€‚
> - **`retain_graph=True`**: å¦‚æžœä½ éœ€è¦å¯¹åŒä¸€ä¸ªè®¡ç®—ç»“æžœ**å¤šæ¬¡**æ‰§è¡Œæ¢¯åº¦è®¡ç®—ï¼Œå°±å¿…é¡»åœ¨è°ƒç”¨æ—¶ä¼ å…¥æ­¤å‚æ•°ï¼Œä»¥==å¼ºåˆ¶ä¿ç•™è®¡ç®—å›¾==ã€‚

---

# PyTorch æ ¸å¿ƒä»£ç è¯¦è§£ï¼šä»Žæ¿€æ´»å‡½æ•°åˆ°è‡ªåŠ¨æ±‚å¯¼

> [!NOTE] æ ¸å¿ƒçº²è¦
> è¿™ä»½ç¬”è®°å°†æ·±å…¥è§£æžåœ¨ä¹‹å‰å­¦ä¹ ä¸­é‡åˆ°çš„å…³é”® PyTorch ä»£ç ç‰‡æ®µï¼Œå¸®åŠ©æ‚¨ä»Žâ€œçŸ¥é“æ˜¯ä»€ä¹ˆâ€åˆ°â€œç²¾é€šæ€Žä¹ˆç”¨â€ã€‚æˆ‘ä»¬å°†é€ä¸€å‰–æžï¼š
> 1.  **å¸¸ç”¨æ¿€æ´»å‡½æ•°** (`sigmoid`, `tanh`, `relu`) çš„ä»£ç å®žçŽ°ä¸Žç‰¹æ€§ã€‚
> 2.  **æ ¸å¿ƒè®­ç»ƒæŒ‡ä»¤** `loss.backward()` çš„æ­£ç¡®å·¥ä½œæµç¨‹ä¸Žå¸¸è§é™·é˜±ã€‚
> 3.  **è®¡ç®—å›¾çš„ç”Ÿå‘½å‘¨æœŸ**ä»¥åŠ `retain_graph=True` çš„ç‰¹æ®Šåº”ç”¨ã€‚

---
## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¿€æ´»å‡½æ•°ä»£ç è¯¦è§£ ðŸ’¡

### 1. `torch.sigmoid` è¯¦è§£

> [!example] `torch.sigmoid`: é¥±å’Œçš„æ¦‚çŽ‡åŒ–å¼€å…³
> **æ ¸å¿ƒåŠŸèƒ½**: å°†ä»»æ„å®žæ•°â€œåŽ‹ç¼©â€åˆ° `(0, 1)` åŒºé—´ï¼Œå¸¸ç”¨äºŽè¡¨ç¤ºæ¦‚çŽ‡ã€‚
> 
> ```python
> # In [5]:
> a = torch.linspace(-100, 100, 10)
> # tensor([-100.00..., -77.77..., ..., 77.77..., 100.00...])
> 
> # In [7]:
> torch.sigmoid(a)
> # tensor([0.0000e+00, 1.6655e-34, ..., 9.9999e-01, 1.0000e+00])
> ```
> #### ä»£ç è§£æž
> - **è¾“å…¥**: `torch.linspace(-100, 100, 10)` åˆ›å»ºäº†ä¸€ä¸ªä»Ž-100åˆ°100çš„ç­‰å·®æ•°åˆ—ï¼Œè¿™ä¸ªå¤§èŒƒå›´æ˜¯ä¸ºäº†å±•ç¤ºå…¶==é¥±å’Œæ•ˆåº”==ã€‚
> - **æ“ä½œ**: `torch.sigmoid(a)` å°† Sigmoid å‡½æ•° $\sigma(x) = \frac{1}{1 + e^{-x}}$ åº”ç”¨äºŽ `a` ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ã€‚
> - **è¾“å‡ºåˆ†æž**:
>   - æžå¤§çš„è´Ÿæ•°ï¼ˆå¦‚ `-100`ï¼‰è¢«æ˜ å°„åˆ°**æŽ¥è¿‘ 0**ã€‚
>   - æžå¤§çš„æ­£æ•°ï¼ˆå¦‚ `100`ï¼‰è¢«æ˜ å°„åˆ°**æŽ¥è¿‘ 1**ã€‚
>   - æŽ¥è¿‘ 0 çš„è¾“å…¥ä¼šè¢«æ˜ å°„åˆ° 0.5 é™„è¿‘ã€‚

### 2. `torch.tanh` è¯¦è§£

> [!example] `torch.tanh`: é›¶ä¸­å¿ƒçš„Så½¢å¼€å…³
> **æ ¸å¿ƒåŠŸèƒ½**: å°†ä»»æ„å®žæ•°â€œåŽ‹ç¼©â€åˆ° `(-1, 1)` åŒºé—´ï¼Œå› å…¶è¾“å‡º**é›¶ä¸­å¿ƒ**ï¼Œåœ¨éšè—å±‚ä¸­é€šå¸¸è¡¨çŽ°æ›´ä¼˜ã€‚
> ```python
> # In [9]:
> a = torch.linspace(-1, 1, 10)
> 
> # In [10]:
> torch.tanh(a)
> # tensor([-0.7616, -0.6514, ..., 0.6514,  0.7616])
> ```
> #### ä»£ç è§£æž
> - **è¾“å…¥**: `torch.linspace(-1, 1, 10)` è¿™ä¸ªèŒƒå›´ä¸»è¦å±•ç¤º Tanh åœ¨ä¸­å¿ƒåŒºåŸŸçš„è¡Œä¸ºã€‚
> - **æ“ä½œ**: å°† Tanh å‡½æ•°æŒ‰å…ƒç´ åº”ç”¨äºŽå¼ é‡ `a`ã€‚
> - **è¾“å‡ºåˆ†æž**: è¾“å‡ºç»“æžœå…³äºŽ 0 **å®Œç¾Žå¯¹ç§°**ï¼Œè¾“å…¥ä¸º 0 æ—¶è¾“å‡ºä¹Ÿä¸º 0ï¼Œè¿™æ­£æ˜¯å…¶â€œé›¶ä¸­å¿ƒâ€çš„ä½“çŽ°ã€‚

### 3. `torch.relu` / `F.relu` è¯¦è§£

> [!example] `torch.relu`: çŽ°ä»£ç½‘ç»œçš„é»˜è®¤ä¹‹é€‰
> **æ ¸å¿ƒåŠŸèƒ½**: ä¿®æ­£çº¿æ€§å•å…ƒã€‚==è´Ÿæ•°å½’é›¶ï¼Œæ­£æ•°ä¸å˜==ã€‚æžå…¶é«˜æ•ˆï¼Œå¹¶èƒ½æœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±ã€‚
> ```python
> from torch.nn import functional as F
> a = torch.linspace(-1, 1, 10)
> 
> # torch.relu(a) å’Œ F.relu(a) ç»“æžœå®Œå…¨ç›¸åŒ
> # tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 
> #         0.1111, 0.3333, 0.5556, 0.7778, 1.0000])
> ```
> #### ä»£ç è§£æž
> - **`torch.relu` vs `F.relu`**:
>   - ä¸¤è€…åŠŸèƒ½å®Œå…¨ç­‰ä»·ã€‚`F.relu` æ˜¯ `torch.nn.functional` æ¨¡å—ä¸­çš„ä¸€ä¸ªçº¯å‡½æ•°ã€‚
>   - è¿˜æœ‰ä¸€ä¸ªé¢å‘å¯¹è±¡çš„ç‰ˆæœ¬ `torch.nn.ReLU()` (æ³¨æ„å¤§å†™)ï¼Œå®ƒæ˜¯ä¸€ä¸ªç±»ã€‚åœ¨ç”¨ `nn.Sequential` æ­å»ºç½‘ç»œæ—¶ä½¿ç”¨ç±»ï¼Œåœ¨è‡ªå®šä¹‰ `forward` å‡½æ•°æ—¶ï¼Œä½¿ç”¨ `F.relu` æ›´å¸¸è§ã€‚
> - **è¾“å‡ºåˆ†æž**: è¾“å…¥ `a` ä¸­çš„æ‰€æœ‰è´Ÿæ•°éƒ½è¢«â€œä¿®æ­£â€ä¸º0ï¼Œæ‰€æœ‰æ­£æ•°åˆ™ä¿æŒåŽŸæ ·ï¼Œå®Œç¾Žä½“çŽ°äº† `max(0, x)` çš„é€»è¾‘ã€‚

---
## ç¬¬äºŒéƒ¨åˆ†ï¼šè‡ªåŠ¨æ±‚å¯¼ä»£ç è¯¦è§£ âš™ï¸

### 1. `loss.backward()` çš„æ­£ç¡®å·¥ä½œæµç¨‹

> [!tip] ä¸€ä¸ªå¸¸è§é”™è¯¯çš„æ•…äº‹ï¼šä¸‰å¹•å‰§ ðŸ“œ
> è¿™æ®µä»£ç å®Œç¾Žåœ°å±•ç¤ºäº†æ–°æ‰‹åœ¨ä½¿ç”¨ `autograd` æ—¶æœ€å®¹æ˜“çŠ¯çš„é”™è¯¯ï¼Œä»¥åŠæ­£ç¡®çš„å¤„ç†æµç¨‹ã€‚
> ![loss.backward() ç¤ºä¾‹](http://googleusercontent.com/file_content/1)
> 
> > [!danger] ç¬¬ä¸€å¹•: ç¬¬ä¸€æ¬¡å¤±è´¥ (In [19] & [21])
> > **å‰§æƒ…**: è®¡ç®— `mse` åŽï¼Œå°è¯•æ±‚æ¢¯åº¦ã€‚
> > **å¤±è´¥åŽŸå› **: è®¡ç®— `mse` æ—¶ï¼Œ`w.requires_grad` ä¸º `False`ã€‚PyTorch æ²¡æœ‰â€œå½•åˆ¶â€è®¡ç®—è¿‡ç¨‹ã€‚
> > **æ¯”å–»**: ==æ²¡æŠ¥åè¯¾ç¨‹ï¼Œå´æƒ³åœ¨æœŸæœ«è¦æˆç»©å•ã€‚==
> 
> > [!danger] ç¬¬äºŒå¹•: ç¬¬äºŒæ¬¡å¤±è´¥ (In [22] & [23])
> > **å‰§æƒ…**: å°† `w` è®¾ç½®ä¸º `requires_grad=True` åŽï¼Œå¯¹**ä¹‹å‰è®¡ç®—çš„æ—§ `mse`** æ±‚æ¢¯åº¦ã€‚
> > **å¤±è´¥åŽŸå› **: `mse` è¯žç”ŸäºŽä¸€ä¸ªæœªè¢«è¿½è¸ªçš„æ—§è®¡ç®—è¿‡ç¨‹ï¼Œä¸ŽçŽ°åœ¨çš„ `w` æ˜¯â€œå¤±è”â€çš„ã€‚
> > **æ¯”å–»**: ==çŽ°åœ¨æŠ¥äº†åï¼Œå´æ‹¿ç€çŽ°åœ¨çš„å­¦ç”Ÿè¯åŽ»è¦â€œä¸Šå­¦æœŸâ€çš„æˆç»©å•ã€‚==
> 
> > [!success] ç¬¬ä¸‰å¹•: æ­£ç¡®çš„æµç¨‹ (In [24] -> [28])
> > **å‰§æƒ…**: é‡æ–°è®¡ç®— `mse`ï¼Œè°ƒç”¨ `.backward()`ï¼ŒæˆåŠŸèŽ·å–æ¢¯åº¦ã€‚
> > **æ­£ç¡®æ­¥éª¤**:
> > 1.  **å‡†å¤‡å‚æ•°**: ç¡®ä¿ `w.requires_grad` ä¸º `True`ã€‚
> > 2.  **æž„å»ºå›¾**: **ç„¶åŽ**ç”¨è¿™ä¸ª `w` åŽ»è®¡ç®— `mse`ï¼Œè®© PyTorch å®Œæ•´è®°å½•ã€‚
> > 3.  **åå‘ä¼ æ’­**: åœ¨ `mse` ä¸Šè°ƒç”¨ `.backward()`ã€‚
> > 4.  **èŽ·å–ç»“æžœ**: é€šè¿‡ `w.grad` æŸ¥çœ‹å­˜å‚¨çš„æ¢¯åº¦ã€‚

### 2. `F.softmax` ä¸Ž `retain_graph=True`
> [!warning] ç†è§£è®¡ç®—å›¾çš„â€œé˜…åŽå³ç„šâ€ç‰¹æ€§ ðŸ”¥ 
> > ![[Pasted image 20250726213011.png]]
> > > **æ ¸å¿ƒæ¦‚å¿µ**: åœ¨ PyTorch ä¸­ï¼Œä¸ºäº†èŠ‚çœå†…å­˜ï¼Œè®¡ç®—å›¾åœ¨é»˜è®¤æƒ…å†µä¸‹æ˜¯**â€œä¸€æ¬¡æ€§â€**çš„ã€‚å½“ä½ è°ƒç”¨ `.backward()` æˆ– `autograd.grad()` åŽï¼Œç”¨äºŽè®¡ç®—æ¢¯åº¦çš„å›¾ç»“æž„**ä¼šè¢«ç«‹å³é”€æ¯**ã€‚ 
> > > > - **`In [35]` çš„æ½œåœ¨é”™è¯¯**: å¦‚æžœä½ è¿žç»­ä¸¤æ¬¡å¯¹ `p` è°ƒç”¨ `.backward()`ï¼Œç¬¬äºŒæ¬¡å°±ä¼šæŠ¥é”™ï¼Œå› ä¸ºè®¡ç®—å›¾åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨åŽå·²è¢«é‡Šæ”¾ã€‚ 
> > > > - **æ¯”å–»**: ==è®¡ç®—å›¾å°±åƒä¸€å¼ â€œé˜…åŽå³ç„šâ€çš„å¯†æŠ¥ï¼Œçœ‹è¿‡ä¸€æ¬¡å°±æ²¡äº†ã€‚==
> > > > - [!tip] è§£å†³æ–¹æ¡ˆ: `retain_graph=True` 
> > > > - **ä½•æ—¶ä½¿ç”¨**: ä»…åœ¨ä½ éœ€è¦å¯¹**åŒä¸€ä¸ªè®¡ç®—å›¾**è¿›è¡Œ**å¤šæ¬¡**åå‘ä¼ æ’­æˆ–æ¢¯åº¦è®¡ç®—æ—¶ã€‚
> > > > - **`In [39]` çš„æ“ä½œ**: `torch.autograd.grad(..., retain_graph=True)` 
> > > > - è¿™è¡Œä»£ç è®¡ç®—äº†æ¢¯åº¦ï¼ŒåŒæ—¶æ˜Žç¡®å‘Šè¯‰ PyTorchï¼šâ€œ==è¯·åœ¨è®¡ç®—åŽä¿ç•™è¿™å¼ è®¡ç®—å›¾ï¼Œæˆ‘ç¨åŽè¿˜æœ‰ç”¨ã€‚==â€ 
> > > > - **`In [40]` çš„æˆåŠŸ**: å› ä¸ºä¸Šä¸€æ­¥ä¿ç•™äº†å›¾ï¼Œæ‰€ä»¥å¯¹å›¾çš„ç¬¬äºŒæ¬¡æ¢¯åº¦è®¡ç®—ä¹Ÿèƒ½æˆåŠŸã€‚ 
> > > > - **åº”ç”¨åœºæ™¯**: è®¡ç®—é«˜é˜¶å¯¼æ•°ã€æŸäº›å¤æ‚çš„GANè®­ç»ƒç­–ç•¥ç­‰ã€‚

---

# PyTorch .requires_grad å±žæ€§æ¢³ç† (å±‚çº§ç‰ˆ)

> [!NOTE]
> `.requires_grad` æ˜¯ PyTorch ä¸­ `torch.Tensor` çš„ä¸€ä¸ªæ ¸å¿ƒå±žæ€§ï¼Œç”¨äºŽæŽ§åˆ¶è‡ªåŠ¨æ±‚å¯¼ã€‚

### 1. æ ¸å¿ƒå®šä¹‰
- **å±žæ€§ç±»åž‹**: å¸ƒå°”å€¼ (`True` or `False`)ã€‚
- **æ ¸å¿ƒåŠŸèƒ½**: ä½œä¸º `autograd` å¼•æ“Žçš„å¼€å…³ï¼Œå†³å®šæ˜¯å¦è¿½è¸ªå¯¹è¯¥å¼ é‡çš„æ“ä½œåŽ†å²ã€‚
- **é»˜è®¤å€¼**: `False`ã€‚

### 2. ä¸»è¦ä½œç”¨
- **`å¼€å¯` (`True`)**:
    - **ç›®çš„**: ä¸ºäº†è®¡ç®—æ¢¯åº¦ã€‚
    - **åº”ç”¨**: ç¥žç»ç½‘ç»œä¸­æ‰€æœ‰**å¯å­¦ä¹ çš„å‚æ•°**ï¼ˆå¦‚ `nn.Linear` å±‚çš„ `weight` å’Œ `bias`ï¼‰ã€‚è¿™äº›å‚æ•°åœ¨è®­ç»ƒä¸­éœ€è¦æ ¹æ®æ¢¯åº¦è¿›è¡Œæ›´æ–°ã€‚
- **`å…³é—­` (`False`)**:
    - **ç›®çš„**: èŠ‚çœè®¡ç®—èµ„æºï¼ˆå†…å­˜å’ŒCPUï¼‰ã€‚
    - **åº”ç”¨**:
        - è¾“å…¥æ•°æ®å’Œç›®æ ‡æ ‡ç­¾ã€‚
        - æ¨¡åž‹è¯„ä¼°ï¼ˆvalidation/testingï¼‰é˜¶æ®µã€‚
        - å†»ç»“éƒ¨åˆ†ç½‘ç»œå±‚è¿›è¡Œå¾®è°ƒï¼ˆfine-tuningï¼‰ã€‚

### 3. ä½¿ç”¨æ–¹æ³•ä¸Žè§„åˆ™
- **è®¾ç½®æ–¹æ³•**:
    1. `x = torch.tensor([1.], requires_grad=True)` (åˆ›å»ºæ—¶)
    2. `x.requires_grad_(True)` (In-place ä¿®æ”¹)
- **ä¼ æ’­è§„åˆ™**:
    - ä¸€ä¸ªæ“ä½œçš„è¾“å‡ºå¼ é‡ï¼Œå…¶ `requires_grad` ä¼šæ˜¯ `True`ï¼Œ**å½“ä¸”ä»…å½“**å…¶è¾“å…¥å¼ é‡ä¸­**è‡³å°‘æœ‰ä¸€ä¸ª**çš„ `requires_grad` ä¸º `True`ã€‚
    - `c = a + b`
        - è‹¥ `a.requires_grad=True`, `b.requires_grad=False` -> `c.requires_grad` ä¼šæ˜¯ `True`ã€‚
- **ä¸´æ—¶ç¦ç”¨æ¢¯åº¦**:
    - ä½¿ç”¨ `with torch.no_grad():` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚
    - åœ¨è¯¥ä»£ç å—å†…ï¼Œæ‰€æœ‰è®¡ç®—ç»“æžœçš„ `requires_grad` éƒ½ä¼šè¢«å¼ºåˆ¶è®¾ä¸º `False`ã€‚

### 4. æ¢¯åº¦å­˜å‚¨
- å½“åœ¨ä¸€ä¸ªè®¡ç®—å›¾çš„æœ€ç»ˆè¾“å‡ºï¼ˆé€šå¸¸æ˜¯ `loss`ï¼‰ä¸Šè°ƒç”¨ `.backward()` åŽï¼Œæ‰€æœ‰ `requires_grad=True` çš„å¶å­å¼ é‡çš„æ¢¯åº¦ä¼š**ç´¯ç§¯**åˆ°å®ƒä»¬çš„ `.grad` å±žæ€§ä¸­ã€‚

> [!EXAMPLE]
> ```python
> # å®Œæ•´çš„è®­ç»ƒæµç¨‹ç¤ºæ„
> x = torch.ones(2, requires_grad=True)
> y = x + 2
> z = y * y * 3
> out = z.mean()
> 
> # out æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå¯¹å…¶åå‘ä¼ æ’­
> out.backward()
> 
> # è®¡ç®—å‡ºçš„æ¢¯åº¦å­˜å‚¨åœ¨ x.grad ä¸­
> print(x.grad) # tensor([9., 9.])
> ```