# 学习笔记：神经网络核心组件 （CCW版)

> [!NOTE] 核心纲要
> 这份笔记将带您完整地了解构成神经网络并使其能够学习的核心组件。我们将依次深入探讨：
> 1.  **激活函数**: 赋予网络非线性学习能力的“开关”。
> 2.  **损失函数**: 衡量模型预测好坏的“标尺”。
> 3.  **自动求导**: PyTorch中实现自动优化的“引擎”。

---
## 第一部分：神经网络的“开关” - 激活函数 🧠

> [!info] 什么是激活函数？
> ![[Pasted image 20250724232329.png]]
> 在神经元接收所有输入并进行**加权求和**后，其结果会经过一个**激活函数** `f(·)` 的处理，得到最终的输出信号。
> 
> **核心作用**: ==为网络引入非线性==。没有激活函数，无论网络多深，都只是一个简单的线性模型，无法学习复杂的现实世界规律。

---
### 激活函数的演进之路

#### 1. “老祖宗”：阶跃函数 (Step Function)

> [!danger] 致命缺陷：无法求导
> ![[Pasted image 20250724232405.png]]
> - **逻辑**: 简单的“开/关”逻辑，大于阈值输出1，否则输出0。
> - **缺陷**: 函数不连续，其导数几乎处处为零。这意味着梯度无法传播，模型无法通过梯度下降进行学习。

#### 2. “经典之选”：Sigmoid 函数

> [!example] Sigmoid: 平滑的概率化开关
> ![[Pasted image 20250724232432.png]]
> - **公式**: $\sigma(x) = \frac{1}{1 + e^{-x}}$
> - **特性**:
>   - **平滑可导**：解决了阶跃函数的最大问题。
>   - **输出范围 (0, 1)**：能将任意输入“压缩”到 (0, 1) 之间，常用于表示概率或二分类任务的输出。
> - **导数**: $\sigma' = \sigma(1-\sigma)$，计算高效。
> 
> > [!tip] 代码示例
> > ![[Pasted image 20250724232517.png]]

#### 3. “改进之选”：Tanh (双曲正切) 函数

> [!example] Tanh: 零中心的Sigmoid
> ![[Pasted image 20250724232643.png]]
> - **公式**: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
> - **特性**:
>   - **输出范围 (-1, 1)**。
>   - **==零中心 (Zero-centered)==**: 输出均值为0，这一特性在实践中通常能让模型**收敛得更快**，因此在隐藏层中表现常优于Sigmoid。
> - **导数**: $\tanh'(x) = 1 - \tanh^2(x)$。
> 
> > [!tip] 代码示例
> >![[Pasted image 20250724232713.png]]

#### 4. “现代默认之选”：ReLU (修正线性单元)

> [!success] ReLU: 简单、高效、缓解梯度消失
> ![[Pasted image 20250724232754.png]]
> - **公式**: $f(x) = \max(0, x)$
> - **核心优势**:
>   1.  **计算极其高效**：仅需一个比较操作。
>   2.  **==有效缓解梯度消失==**：当输入为正数时，其梯度恒为 **1**。这使得梯度能够顺畅地在深度网络中传播，极大地加速了训练。
> - **导数**:
> ![[Pasted image 20250724232826.png]]
> 
> > [!tip] 代码示例
> > ![[Pasted image 20250724232909.png]]

---
## 第二部分：衡量误差的“标尺” - 损失函数 📏

> [!info] 什么是损失函数？
> **损失函数 (Loss Function)** 是一个用来计算**模型预测值**与**真实标签**之间差距的函数。
> 
> **训练目标**: ==通过调整模型参数，让损失函数的值变得尽可能小。==

### 深入理解：均方误差 (Mean Squared Error, MSE)

> [!example] MSE: 回归任务的常用标尺
>![[Pasted image 20250724233017.png]]
> - **核心公式**: $\text{loss} = \sum (\text{真实值} - \text{预测值})^2 = \sum [y - f_\theta(x)]^2$
> - **作用**: 主要用于**回归任务**。通过计算误差的平方和，来衡量模型预测的准确性。

### 损失函数的梯度：连接误差与优化

> [!abstract] 梯度的桥梁作用
> ![[Pasted image 20250724233118.png]]
> - **公式**: $\frac{\nabla \text{loss}}{\nabla \theta} = -2 \sum \underbrace{[y - f_\theta(x)]}_{\text{误差}} \cdot \underbrace{\frac{\nabla f_\theta(x)}{\nabla \theta}}_{\text{模型输出对参数的梯度}}$
> - **深刻含义**: 这个公式是整个学习过程的动力源泉。它告诉我们，最终的**误差**信号，是如何通过**链式法则**，一步步地反向传播，并指导网络中的**每一个参数**应该如何进行微调，从而让总误差减小。

---
## 第三部分：自动化的“引擎” - PyTorch Autograd ⚙️

> [!info] 什么是 Autograd？
> 我们无需手动计算上面复杂的梯度。PyTorch 的 **`autograd`** 引擎会通过构建**动态计算图**来自动完成这个过程。

### 1. 黄金法则：开启梯度追踪 (`requires_grad`)

> [!warning] 必须先开启追踪！
> 要想让 PyTorch 自动求导，必须在计算发生**之前**，将参数张量的 `.requires_grad` 属性设置为 `True`。
> `w.requires_grad_(True)`

### 2. 计算梯度的两大API

> [!tip] API对比
> ![[Pasted image 20250724233253.png]]
> - **`loss.backward()` (推荐)**: 最常用的方法。计算梯度并将其**存储**在参数的 `.grad` 属性中。
> - **`torch.autograd.grad()`**: 更底层的接口，直接将梯度作为结果**返回**。

> [!example] 代码中的常见陷阱与正确流程
> ![[Pasted image 20250724233314.png]]
> - **错误原因**: 在计算损失 `mse` 时，参数 `w` 还没有开启梯度追踪。
> - **正确流程**: ==必须先设置 `w.requires_grad_(True)`，然后再用这个 `w` 去计算损失 `mse`==，最后才能成功调用 `mse.backward()` 或 `autograd.grad`。

---
## 第四部分：多分类“标准答案” - Softmax 函数 🎯

> [!info] 什么是 Softmax？
> ![[Pasted image 20250724233333.png]]
> **Softmax** 能将一个包含任意分数的向量（logits），转换成一个**概率分布**。
> 
> **关键特性**:
> 1.  所有输出值都在 `[0, 1]` 之间。
> 2.  所有输出值的总和为 `1`。

### Softmax 的数学与代码实践

> [!abstract] Softmax 的导数
>![[Pasted image 20250724233413.png]]
> Softmax 的导数比较复杂，分为 `i = j` 和 `i ≠ j` 两种情况。在实践中，我们通常将 **Softmax** 和 **交叉熵损失** 结合使用，它们的组合梯度形式非常简洁。

> [!warning] PyTorch实践中的 `retain_graph`
> ![[Pasted image 20250724233503.png]]
> - **默认行为**: 调用 `.backward()` 或 `autograd.grad()` 后，计算图会被**立即释放**以节省内存。
> - **`retain_graph=True`**: 如果你需要对同一个计算结果**多次**执行梯度计算，就必须在调用时传入此参数，以==强制保留计算图==。

---

# PyTorch 核心代码详解：从激活函数到自动求导

> [!NOTE] 核心纲要
> 这份笔记将深入解析在之前学习中遇到的关键 PyTorch 代码片段，帮助您从“知道是什么”到“精通怎么用”。我们将逐一剖析：
> 1.  **常用激活函数** (`sigmoid`, `tanh`, `relu`) 的代码实现与特性。
> 2.  **核心训练指令** `loss.backward()` 的正确工作流程与常见陷阱。
> 3.  **计算图的生命周期**以及 `retain_graph=True` 的特殊应用。

---
## 第一部分：激活函数代码详解 💡

### 1. `torch.sigmoid` 详解

> [!example] `torch.sigmoid`: 饱和的概率化开关
> **核心功能**: 将任意实数“压缩”到 `(0, 1)` 区间，常用于表示概率。
> 
> ```python
> # In [5]:
> a = torch.linspace(-100, 100, 10)
> # tensor([-100.00..., -77.77..., ..., 77.77..., 100.00...])
> 
> # In [7]:
> torch.sigmoid(a)
> # tensor([0.0000e+00, 1.6655e-34, ..., 9.9999e-01, 1.0000e+00])
> ```
> #### 代码解析
> - **输入**: `torch.linspace(-100, 100, 10)` 创建了一个从-100到100的等差数列，这个大范围是为了展示其==饱和效应==。
> - **操作**: `torch.sigmoid(a)` 将 Sigmoid 函数 $\sigma(x) = \frac{1}{1 + e^{-x}}$ 应用于 `a` 中的每一个元素。
> - **输出分析**:
>   - 极大的负数（如 `-100`）被映射到**接近 0**。
>   - 极大的正数（如 `100`）被映射到**接近 1**。
>   - 接近 0 的输入会被映射到 0.5 附近。

### 2. `torch.tanh` 详解

> [!example] `torch.tanh`: 零中心的S形开关
> **核心功能**: 将任意实数“压缩”到 `(-1, 1)` 区间，因其输出**零中心**，在隐藏层中通常表现更优。
> ```python
> # In [9]:
> a = torch.linspace(-1, 1, 10)
> 
> # In [10]:
> torch.tanh(a)
> # tensor([-0.7616, -0.6514, ..., 0.6514,  0.7616])
> ```
> #### 代码解析
> - **输入**: `torch.linspace(-1, 1, 10)` 这个范围主要展示 Tanh 在中心区域的行为。
> - **操作**: 将 Tanh 函数按元素应用于张量 `a`。
> - **输出分析**: 输出结果关于 0 **完美对称**，输入为 0 时输出也为 0，这正是其“零中心”的体现。

### 3. `torch.relu` / `F.relu` 详解

> [!example] `torch.relu`: 现代网络的默认之选
> **核心功能**: 修正线性单元。==负数归零，正数不变==。极其高效，并能有效缓解梯度消失。
> ```python
> from torch.nn import functional as F
> a = torch.linspace(-1, 1, 10)
> 
> # torch.relu(a) 和 F.relu(a) 结果完全相同
> # tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 
> #         0.1111, 0.3333, 0.5556, 0.7778, 1.0000])
> ```
> #### 代码解析
> - **`torch.relu` vs `F.relu`**:
>   - 两者功能完全等价。`F.relu` 是 `torch.nn.functional` 模块中的一个纯函数。
>   - 还有一个面向对象的版本 `torch.nn.ReLU()` (注意大写)，它是一个类。在用 `nn.Sequential` 搭建网络时使用类，在自定义 `forward` 函数时，使用 `F.relu` 更常见。
> - **输出分析**: 输入 `a` 中的所有负数都被“修正”为0，所有正数则保持原样，完美体现了 `max(0, x)` 的逻辑。

---
## 第二部分：自动求导代码详解 ⚙️

### 1. `loss.backward()` 的正确工作流程

> [!tip] 一个常见错误的故事：三幕剧 📜
> 这段代码完美地展示了新手在使用 `autograd` 时最容易犯的错误，以及正确的处理流程。
> ![loss.backward() 示例](http://googleusercontent.com/file_content/1)
> 
> > [!danger] 第一幕: 第一次失败 (In [19] & [21])
> > **剧情**: 计算 `mse` 后，尝试求梯度。
> > **失败原因**: 计算 `mse` 时，`w.requires_grad` 为 `False`。PyTorch 没有“录制”计算过程。
> > **比喻**: ==没报名课程，却想在期末要成绩单。==
> 
> > [!danger] 第二幕: 第二次失败 (In [22] & [23])
> > **剧情**: 将 `w` 设置为 `requires_grad=True` 后，对**之前计算的旧 `mse`** 求梯度。
> > **失败原因**: `mse` 诞生于一个未被追踪的旧计算过程，与现在的 `w` 是“失联”的。
> > **比喻**: ==现在报了名，却拿着现在的学生证去要“上学期”的成绩单。==
> 
> > [!success] 第三幕: 正确的流程 (In [24] -> [28])
> > **剧情**: 重新计算 `mse`，调用 `.backward()`，成功获取梯度。
> > **正确步骤**:
> > 1.  **准备参数**: 确保 `w.requires_grad` 为 `True`。
> > 2.  **构建图**: **然后**用这个 `w` 去计算 `mse`，让 PyTorch 完整记录。
> > 3.  **反向传播**: 在 `mse` 上调用 `.backward()`。
> > 4.  **获取结果**: 通过 `w.grad` 查看存储的梯度。

### 2. `F.softmax` 与 `retain_graph=True`

> [!warning] 理解计算图的“阅后即焚”特性 🔥
> ![F.softmax 代码示例](http://googleusercontent.com/file_content/3)
> **核心概念**: 在 PyTorch 中，为了节省内存，计算图在默认情况下是**“一次性”**的。当你调用 `.backward()` 或 `autograd.grad()` 后，用于计算梯度的图结构**会被立即销毁**。
> 
> - **`In [35]` 的潜在错误**: 如果你连续两次对 `p` 调用 `.backward()`，第二次就会报错，因为计算图在第一次调用后已被释放。
> - **比喻**: ==计算图就像一张“阅后即焚”的密报，看过一次就没了。==
> 
> > [!tip] 解决方案: `retain_graph=True`
> > - **何时使用**: 仅在你需要对**同一个计算图**进行**多次**反向传播或梯度计算时。
> > - **`In [39]` 的操作**: `torch.autograd.grad(..., retain_graph=True)`
> >   - 这行代码计算了梯度，同时明确告诉 PyTorch：“==请在计算后保留这张计算图，我稍后还有用。==”
> > - **`In [40]` 的成功**: 因为上一步保留了图，所以对图的第二次梯度计算也能成功。
> > - **应用场景**: 计算高阶导数、某些复杂的GAN训练策略等。