以下是一份完整复盘，帮助您在下一次重新初始化 GPU 服务器时，能​一次性、快速、稳定地跑通 LISA-13B Demo。内容包括：操作顺序、曾经踩过的坑、对应修复措施，以及仍可优化之处。

────────────────────────────────────
一、推荐的“一条龙”操作流程
────────────────────────────────────
0. 约定路径  
   数据盘：`/root/autodl-fs` 项目：`/root/autodl-fs/LISA`  
   HF 缓存：`/root/autodl-fs/huggingface_cache`

1. 克隆代码 & 建虚拟环境  
```bash
cd /root/autodl-fs
git clone https://github.com/xinlai/LISA.git
cd LISA
python3 -m venv lisa_env
source lisa_env/bin/activate
pip install -r requirements.txt        # 若遇 CUDA ⟂ bitsandbytes 冲突，再手动指定版本
pip install scipy
```

2. 设置 Hugging Face 缓存位置（永久写进 `.bashrc`）  
```bash
echo 'export HF_HOME=/root/autodl-fs/huggingface_cache' >> ~/.bashrc
source ~/.bashrc
```

3. **一次性离线下载**权重  
   （在网路良好时执行一次，后续服务器可离线启动）  
```bash
# 开启学术加速
source /etc/network_turbo
python chat.py --load_in_4bit                 # 仅为触发下载
# 下载完 Ctrl+C 退出
unset http_proxy https_proxy                  # 关闭加速
```
下载完成后会在  
```
$HF_HOME/hub/models--xinlai--LISA-13B-llama2-v1/snapshots/<hash>/
$HF_HOME/hub/models--openai--clip-vit-large-patch14/snapshots/<hash>/
```  
各生成 10+ GB 与 400 MB 的本地快照。

4. **创建软链** 让离线模式可直接命中视觉塔路径  
```bash
mkdir -p /root/autodl-fs/LISA/openai
ln -sfn $HF_HOME/hub/models--openai--clip-vit-large-patch14/snapshots/* \
        /root/autodl-fs/LISA/openai/clip-vit-large-patch14
```

5. 离线运行 Demo（显存友好、加速版）  
```bash
source lisa_env/bin/activate
export HF_HUB_OFFLINE=1
SNAP=$(ls -d $HF_HOME/hub/models--xinlai--LISA-13B-llama2-v1/snapshots/* | head -n1)

python chat.py \
       --load_in_4bit \
       --version "$SNAP" \
       --image-size 512          # 图像缩到 512，速度提升 2~3 倍
```
– 首轮推理约 1–3 分钟，之后数十秒一轮。  
– 结果保存在 `vis_output/`。  

────────────────────────────────────
二、曾经踩过的主要坑 ＆ 对应修复
────────────────────────────────────
1. **HF 缓存默认写系统盘 → 空间不足**  
   • 通过 `export HF_HOME=/root/autodl-fs/huggingface_cache` 改到数据盘。

2. **bitsandbytes 与 CUDA 版本不匹配**  
   • 3090 + CUDA 11.7 执行  
     `pip install bitsandbytes==0.39.0`（兼容 torch 2.0.1）。

3. **torch 2.0.1 ↔ torchvision 0.15.2 版本矛盾**  
   • 同时固定：`pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117`.

4. **离线模式仍请求网络 → 401 / RepositoryNotFound**  
   • 设置 `export HF_HUB_OFFLINE=1`。  
   • 对于视觉塔，建立本地软链或在 `chat.py` 里注入  
     `args.vision_tower = <本地路径>`。

5. **24 GB 显存 OOM**  
   • 只加载 4-bit LLM 到 GPU；视觉塔留在 CPU。  
   • 减少 `max_new_tokens` 与 `image_size`。  
   • 如仍 OOM，可改 `--load_in_8bit` 或进一步裁剪 CLIP 层数。

6. **cpu/cuda 混用报错**  
   • 保证图像张量 `.to(model.lm.device)` 与视觉塔同设备。

────────────────────────────────────
三、仍可改进的细节
────────────────────────────────────
1. **将路径、软链创建、环境变量写成一键 Shell 脚本**，重开实例后直接执行。  
2. 把 `max_new_tokens`、`image_size`、`vision_tower_depth` 做成命令行参数。  
3. 使用 `accelerate` 或 `bitsandbytes` 的 offload 配置，把 KV 缓存也放到 GPU，进一步提速。  
4. 若经常需要高速交互，可考虑切换到 40 GB+ 显存卡，使 LLM+CLIP 全部驻留 GPU。

这样整理后，下次只需 **5 步** 就能在全新服务器上离线跑通 LISA Demo，且避免显存溢出与版本冲突。祝实验顺利!

补充遗漏的关键点：  
【V0 / V1 模型版本混淆】

问题经过  
1. `chat.py` 默认写死加载仓库 `xinlai/LISA-13B-llama2-v0`。  
2. 您实际通过加速下载的是 **v1**（`xinlai/LISA-13B-llama2-v1`）。  
3. 运行时脚本在离线缓存找不到 v0，于是不断尝试联网 → 报 401 / RepositoryNotFound。  
4. 解决办法有两种（二选一即可）：

   A. **把脚本改成 v1**  
   ```python
   # chat.py → parse_args
   parser.add_argument("--version", default="xinlai/LISA-13B-llama2-v1")
   ```
   B. **直接把 --version 指向已下载的本地快照目录**  
   ```bash
   SNAP=$(ls -d $HF_HOME/hub/models--xinlai--LISA-13B-llama2-v1/snapshots/* | head -n1)
   python chat.py --load_in_4bit --version "$SNAP"
   ```

提醒  
• 若以后官方又更新 `v2`，请保持“代码版本” 与 “实际下载版本/快照路径” 一致，否则离线加载仍会失败。  
• 统一做法是在 `run.sh` 或启动命令里 **总是显式传入 `--version <本地路径>`**，避免脚本硬编码引发的版本错配。

至此，所有关键坑位（缓存目录、版本号、视觉塔路径、显存分配）均已覆盖完整。

补充所有先前踩过、上一次总结尚未写到的“坑 + 解决手册”，方便您彻底复原整个排障过程。

──────────────────────────────
A. 版本、路径、缓存类
──────────────────────────────
1. v0 / v1 模型不符  
   • 脚本硬编码 v0，实际下载 v1 → 离线找不到。  
   • 修复：`--version` 指向 v1 或直接传本地快照路径。

2. 视觉塔路径找不到  
   • `openai/clip-vit-large-patch14` 在线不可达。  
   • 修复：把本地快照软链到 `openai/clip-vit-large-patch14`。

3. HF 缓存默认在系统盘导致「磁盘写满」  
   • 修复：`export HF_HOME=/root/autodl-fs/huggingface_cache`。

4. 离线模式却仍联网  
   • 忘记 `export HF_HUB_OFFLINE=1`；或者配置里还有线上 repo 字符串。  
   • 修复：开离线 + 软链/本地路径全部就绪。

──────────────────────────────
B. 依赖与编译类
──────────────────────────────
5. bitsandbytes 编译失败 / CUDA 11.7 不匹配  
   • 修复：安装 `bitsandbytes==0.39.0`(torch2.0) 或 0.47(CUDA12)+对应 torch2.8。  
   • 若网络编译失败，加 `source /etc/network_turbo` 重试。

6. `torch` / `torchvision` 版本矛盾  
   • 错误：`torch 2.8` + `torchvision 0.15`。  
   • 修复：二者锁成同一 CUDA 版本（如 2.0.1+cu117 / 0.15.2+cu117）。

7. 缺少 `scipy`  
   • bitsandbytes 0.47 里引用 → `pip install scipy`.

──────────────────────────────
C. 显存 & 设备冲突类
──────────────────────────────
8. GPU OOM（24 GB 3090）  
   • 整体 `.cuda()` 把 LLM+CLIP 全搬 GPU，占 22 + 5 GB。  
   • 修复：4-bit LLM 放 GPU、CLIP 留 CPU；或裁减 CLIP 层；或 8-bit、更小塔。

9. cpu / cuda device mismatch  
   • 图像张量 `.cuda()`，而视觉塔在 CPU。  
   • 修复：`images_clip`、`images` 用 `.to(vision_tower.device)`。

10. 只搬视觉塔不搬 LLM → GPU 利用 0%  
    • 原因：取消 `.cuda()` 后显存仅 900 MB，实际推理全在 CPU。  
    • 修复：接受慢速或采用上一步分层方案。

11. 内存碎片导致 OOM 时加 `.cuda()`  
    • 解决：`torch.cuda.empty_cache()` → 边界情况下可腾出几百 MiB，但根本解是减载 GPU。

──────────────────────────────
D. 推理耗时 & 交互类
──────────────────────────────
12. 首轮推理看似“卡死”  
    • CPU 跑 CLIP，需 3-6 分钟；生成 512 token 更久。  
    • 修复：缩 `--image-size 512`、`max_new_tokens 128`，首轮降至 1-2 分钟；或在第二终端 `watch -n1 nvidia-smi` 观察。

13. 用 run_terminal_cmd 启动脚本 → `input()` 立刻 EOF  
    • 修复：在真正的交互式终端前台运行，不用一次性命令。

14. 软链创建失败 “No such file or directory”  
    • 忘记 `mkdir -p openai`，父目录缺失。  
    • 修复：先建目录再 `ln -sfn`.

──────────────────────────────
E. 网络 & 加速类
──────────────────────────────
15. 下载权重超时 / reset  
    • 解决：`source /etc/network_turbo` 走 AutoDL 学术代理；或改源到 `hf-mirror.com`。  
16. 编译 bitsandbytes 从 GitHub clone 失败  
    • 同上，开加速或多次重试。

──────────────────────────────
F. 建议的最终“一键脚本”框架
──────────────────────────────
```bash
#!/usr/bin/env bash
set -e
# --- 基本变量 ---
ROOT=/root/autodl-fs
PROJ=$ROOT/LISA
CACHE=$ROOT/huggingface_cache
export HF_HOME=$CACHE
export HF_HUB_OFFLINE=1

# --- 环境 ---
python3 -m venv $PROJ/lisa_env
source $PROJ/lisa_env/bin/activate
pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 -f https://download.pytorch.org/whl/torch_stable.html
pip install bitsandbytes==0.39.0 scipy -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install -r $PROJ/requirements.txt

# --- 软链视觉塔 ---
mkdir -p $PROJ/openai
CLIP_SNAP=$(ls -d $CACHE/hub/models--openai--clip-vit-large-patch14/snapshots/* | head -n1)
ln -sfn "$CLIP_SNAP" $PROJ/openai/clip-vit-large-patch14

# --- 本地快照路径 ---
LISA_SNAP=$(ls -d $CACHE/hub/models--xinlai--LISA-13B-llama2-v1/snapshots/* | head -n1)

# --- 运行 ---
python $PROJ/chat.py --load_in_4bit --version "$LISA_SNAP" --image-size 512
```
(确保您在 `chat.py` 中已：  
• 默认 `max_new_tokens=128`;  
• 把视觉塔层数裁到 6；  
• 图像张量 `.to(model.lm.device)`；  
• 删除全模型 `.cuda()`。)

复制到新服务器执行即可免踩全部旧坑。

至此，之前所有问题——包括 v0/v1 混淆、权重路径、显存管理、软链、离线模式、依赖冲突、交互方式等——都已补全。祝您下次部署一路顺畅 ⚡!