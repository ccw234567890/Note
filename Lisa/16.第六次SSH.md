![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/HFTOKEN.png)


【完整复盘 | LISA-13B 离线推理一条龙 — 踩坑&修复清单】

─────────────────────────────────  
0. 基本环境 & 目标  
─────────────────────────────────  
GPU：RTX 3090 24 GB CUDA 12.1 PyTorch 2.1.0 Python 3.10  
系统盘 30 GB 数据盘 50 GB（挂载 /​root/autodl-fs）  
目标：**只做推理验证**，不训练、不下载数据集。

─────────────────────────────────  
1. 正确的「黄金路径」  
─────────────────────────────────  
① 克隆代码 & 虚拟环境  
```bash
ROOT=/root/autodl-fs ; PROJ=$ROOT/LISA
git clone https://github.com/xinlai/LISA.git $PROJ
python3 -m venv $PROJ/lisa_env && source $PROJ/lisa_env/bin/activate
pip install torch==2.1.0+cu121 torchvision==0.16.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html
pip install bitsandbytes==0.42.0 scipy -q
pip install -r $PROJ/requirements.txt -q
```

② 配置 Hugging Face 缓存  
```bash
CACHE=$ROOT/huggingface_cache
mkdir -p $CACHE
echo "export HF_HOME=$CACHE"       >> ~/.bashrc
echo "export HF_HUB_OFFLINE=1"     >> ~/.bashrc
source ~/.bashrc
```

③ **获取模型权重（任选其一）**  
A. 官方站点 + 学术代理  
```bash
# 登录 HF（Read-Token 要能读 gated repo）
huggingface-cli login --token <hf_xxx>
source /etc/network_turbo                 # 开 proxy
export HF_HUB_ENABLE_HF_TRANSFER=1
python - <<'PY'
from huggingface_hub import snapshot_download
snapshot_download("xinlai/LISA-13B-llama2-v1", resume_download=True, local_dir_use_symlinks=True)
snapshot_download("openai/clip-vit-large-patch14", resume_download=True, local_dir_use_symlinks=True)
PY
```
B. **hf-mirror 直链 + wget -c**（免 Token，免代理）  
在  
`$CACHE/hub/models--xinlai--LISA-13B-llama2-v1/snapshots/manual/`  
`$CACHE/hub/models--openai--clip-vit-large-patch14/snapshots/manual/`  
分别执行批量下载（共 11 + 8 个文件，见下文「文件清单」）。

④ 建立视觉塔软链  
```bash
mkdir -p $PROJ/openai
CLIP_SNAP=$(ls -d $CACHE/hub/models--openai--clip-vit-large-patch14/snapshots/* | head -n1)
ln -sfn "$CLIP_SNAP" $PROJ/openai/clip-vit-large-patch14
```

⑤ **离线推理**  
```bash
source $PROJ/lisa_env/bin/activate
export HF_HOME=$CACHE ; export HF_HUB_OFFLINE=1
SNAP=$(ls -d $CACHE/hub/models--xinlai--LISA-13B-llama2-v1/snapshots/* | head -n1)
python $PROJ/chat.py --load_in_4bit --version "$SNAP" --image-size 512
```
出现 `Please input your prompt:` 即完成部署。

─────────────────────────────────  
2. 全程踩过的坑 & 对应解决  
─────────────────────────────────  
3) **HF 401 Unauthorized**  
   • 原因：Fine-grained Token 未勾选「Read gated repos」。  
   • 解决：直接用 “Read” 模板生成 Token，重新 `huggingface-cli login`。

4) **Token 正确但 whoami 无输出**  
   • 原因：复制 Token 时首尾有空格 / 回车。  
   • 解决：删掉 `~/.huggingface/token`，重新粘贴 40 位字符。

5) **下载依赖 OpenCV 卡在 `cv2` 导入**  
   • 用 `chat.py` 触发下载会先 import cv2 → 慢/卡。  
   • 解决：改用 `snapshot_download` 或 `wget` 直接下权重。

6) **系统盘被写满**  
   • 初始 HF 缓存在 `~/.cache/huggingface`（系统盘）。  
   • 解决：`export HF_HOME=/root/autodl-fs/huggingface_cache`。

7) **bitsandbytes / torch 版本冲突**  
   • 3090 + CUDA 12.1 → `torch==2.1.0+cu121` & `bitsandbytes==0.42.0`.

8) **视觉塔路径离线找不到**  
   • 通过软链把快照指向 `LISA/openai/clip-vit-large-patch14`。

9) **显存不足**  
   • 用 `--load_in_4bit`；CLIP 保持 CPU；`--image-size 512`。

10) **wget 速度慢 / 中断**  
   • 加 `-c` 断点续传；多线程可并行开多个 wget。  
   • 查看大小：`du -sh <dir>` 判断进度。

─────────────────────────────────  
3. 手动下载文件清单  
─────────────────────────────────  
LISA-13B（11 件，≈ 30 GB）：  
```
.gitattributes
added_tokens.json
config.json
generation_config.json
pytorch_model-00001-of-00003.bin  (~9.95G)
pytorch_model-00002-of-00003.bin  (~9.9G)
pytorch_model-00003-of-00003.bin  (~8.8G)
pytorch_model.bin.index.json
special_tokens_map.json
tokenizer.model
tokenizer_config.json
```
CLIP vit-large-patch14（8 件，≈ 400 MB）：  
```
.gitattributes
config.json
merges.txt
preprocessor_config.json
pytorch_model.bin               (~406M)
special_tokens_map.json
tokenizer_config.json
vocab.json
```
镜像直链示例：  
`https://hf-mirror.com/xinlai/LISA-13B-llama2-v1/resolve/main/pytorch_model-00001-of-00003.bin`

─────────────────────────────────  
4. 服务器一键脚本（含加速）  
─────────────────────────────────
```bash
#!/usr/bin/env bash
set -e
ROOT=/root/autodl-fs ; PROJ=$ROOT/LISA ; CACHE=$ROOT/huggingface_cache
export HF_HOME=$CACHE

# 环境
python3 -m venv $PROJ/lisa_env && source $PROJ/lisa_env/bin/activate
pip install -q torch==2.1.0+cu121 torchvision==0.16.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html
pip install -q bitsandbytes==0.42.0 scipy -r $PROJ/requirements.txt

# 登录 HF（先在本地复制 token）
huggingface-cli login --token <hf_xxx>

# 学术加速
source /etc/network_turbo
export HF_HUB_ENABLE_HF_TRANSFER=1

# 下载
python - <<'PY'
from huggingface_hub import snapshot_download
snapshot_download("xinlai/LISA-13B-llama2-v1", resume_download=True, local_dir_use_symlinks=True)
snapshot_download("openai/clip-vit-large-patch14", resume_download=True, local_dir_use_symlinks=True)
PY

# 软链视觉塔
mkdir -p $PROJ/openai
CLIP_SNAP=$(ls -d $CACHE/hub/models--openai--clip-vit-large-patch14/snapshots/* | head -n1)
ln -sfn "$CLIP_SNAP" $PROJ/openai/clip-vit-large-patch14

# 离线推理
SNAP=$(ls -d $CACHE/hub/models--xinlai--LISA-13B-llama2-v1/snapshots/* | head -n1)
export HF_HUB_OFFLINE=1
python $PROJ/chat.py --load_in_4bit --version "$SNAP" --image-size 512
```

─────────────────────────────────  
有了以上复盘，下次重新开 GPU 实例，只需照「黄金路径」或「一键脚本」操作，所有曾经犯过的错误都会被自动规避。祝部署再无坑，一次成功！