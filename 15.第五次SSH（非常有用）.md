以下内容是最新、最简、最稳的一键离线体验方案 (2024-09-01 更新)。  
把整段复制到新 GPU 服务器终端运行即可「0 交互」拉起 LISA-13B Demo，避免此前全部坑点。

──────────────────────────────────────
一、一条龙脚本（直接复制粘贴）
──────────────────────────────────────
```bash
############################################################################
# ① 建议在原生 SSH 终端执行；整程 100% 离线，无需再手动输入。
# ② 默认 CPU fp32 推理（≈7-8 min）——显存 24 GB 不够安全，统一放 CPU。
# ③ 如有 ≥40 GB GPU，并想提速，可删掉最后的 --cpu 标记让模型进 GPU。
############################################################################

# --- 基本目录 ---
ROOT=/root/autodl-fs
PROJ=$ROOT/LISA
CACHE=$ROOT/huggingface_cache
IMG=$PROJ/imgs/OIP.webp                  # ← 换成自己的图片
PROMPT="segment the person"             # ← 换成自己的提示

# --- 0. 克隆代码 & 建虚拟环境（首次） ---
if [ ! -d "$PROJ" ]; then
  cd $ROOT
  git clone https://github.com/xinlai/LISA.git
  cd $PROJ
  python3 -m venv lisa_env
  source lisa_env/bin/activate
  pip install --upgrade pip
  pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 \
      -f https://download.pytorch.org/whl/torch_stable.html
  pip install -r requirements.txt scipy
else
  source $PROJ/lisa_env/bin/activate
fi

# --- 1. 环境变量 ---
export HF_HOME=$CACHE
export HF_HUB_OFFLINE=1                 # 全局离线标志

# --- 2. 离线下载权重（首次联网执行；下载完即可断网） ---
if [ ! -d "$CACHE/hub/models--xinlai--LISA-13B-llama2-v1" ]; then
  echo "[首次] 连接网络开始自动下载 LISA-13B & CLIP 权重…"
  python $PROJ/chat.py --version xinlai/LISA-13B-llama2-v1 --load_in_4bit <<< $'\n' || true
  echo "[下载完成] 现已离线模式。"
fi

# --- 3. 软链视觉塔（只需一次） ---
mkdir -p $PROJ/openai
CLIP_SNAP=$(ls -d $CACHE/hub/models--openai--clip-vit-large-patch14/snapshots/* | head -n1)
ln -sfn "$CLIP_SNAP" $PROJ/openai/clip-vit-large-patch14

# --- 4. 本地快照路径 ---
SNAP=$(ls -d $CACHE/hub/models--xinlai--LISA-13B-llama2-v1/snapshots/* | head -n1)

# --- 5. 运行一次性推理（CPU fp32 最稳） ---
python $PROJ/run_once.py \
       --prompt  "$PROMPT" \
       --image   "$IMG" \
       --model-snap "$SNAP" \
       --cpu                       # 强制全模型 CPU，彻底绕过 bitsandbytes

echo -e "\n✅ 推理结束，结果文件已保存至: $PROJ/vis_output/"
ls -lt $PROJ/vis_output | head
############################################################################
```

──────────────────────────────────────
二、脚本参数说明
──────────────────────────────────────
1. `$IMG`、`$PROMPT` 自行修改即可测试任意图片 / 提示词。  
2. 若显存 ≥40 GB 想提速，可删掉末尾的 `--cpu`，让 LLM+CLIP 全进 GPU（默认 fp32）。  
3. 若仍需节省显存，可加 `--precision bf16` 或 `--precision fp16`（需支持）。  
4. 如需再次下载其他模型版本，只要把 `--version` 和 `$SNAP` 换成对应 v0/v2 路径即可。

──────────────────────────────────────
三、常见问题一次性排除
──────────────────────────────────────
✓ bitsandbytes ↔ CUDA 版本冲突 → 全 CPU 推理或删除 `--load_in_4bit`  
✓ v0/v1 仓库不符 → 始终用本地快照路径 `$SNAP`  
✓ HF 缓存写系统盘 → 统一 `export HF_HOME=$CACHE`  
✓ 离线仍联网 → `export HF_HUB_OFFLINE=1` + 本地软链视觉塔  
✓ dtype / device mismatch → 统一 CPU、fp32  
✓ IDE 会话断线 → 推荐 `tmux new -s lisa` 后再运行脚本  
✓ 软链不存在 → 自动创建 `openai/clip-vit-large-patch14`

──────────────────────────────────────
四、目录结构（脚本跑完后）
──────────────────────────────────────
```
autodl-fs/
├─ LISA/                  # 代码
│  ├─ imgs/               # 放测试图片
│  ├─ vis_output/         # 推理结果 (mask / blend)
│  └─ run_once.py         # 一次性非交互推理脚本
└─ huggingface_cache/     # 全部权重快照
```

至此，本「一条龙脚本」即可在任何新服务器上：  
1) 自动拉代码、建虚拟环境  
2) 首次联网下载并缓存权重  
3) 之后完全离线、一次命令推理  
4) 避免 CUDA / bitsandbytes / 路径 / 显存 / 交互等全部坑点

复制粘贴即可用，祝部署顺利 ⚡

确实，上面给出的「一条龙脚本」已能跑通，但文档里没有再枚举所有曾经踩坑。下面把 **全部出现过的问题 + 解决方法** 统一补充到末尾，便于后续排障或扩展。

──────────────────────────────────────
附录：完整坑位清单 & 对应修复
──────────────────────────────────────
A. 路径 / 版本 / 缓存类  
1. v0 / v1 仓库硬编码不一致  
   → 统一用本地快照路径 `$SNAP`，脚本不写仓库名。  
2. 视觉塔路径找不到  
   → 软链 `$CACHE/...clip-vit-large-patch14/snapshots/*` → `openai/clip-vit-large-patch14`.  
3. HF 默认缓存到系统盘写满  
   → `export HF_HOME=$CACHE` 指向数据盘。  
4. 离线模式仍联网  
   → `export HF_HUB_OFFLINE=1` + 本地路径全部就绪。

B. 依赖 / 编译类  
5. bitsandbytes 与 CUDA 不匹配  
   → A) 全 CPU 推理（脚本默认）; B) 如需 GPU 量化，锁定 torch==2.0.1+cu117 + bnb==0.39.0。  
6. torch / torchvision 版本矛盾  
   → 同时安装 `torch==2.0.1+cu117 torchvision==0.15.2+cu117`.  
7. 缺少 scipy 导致 bnb fail  
   → `pip install scipy`.  
8. intel_extension_for_pytorch 警告  
   → 纯 CPU 场景无需理会；如在 Intel GPU/XPU 再安装 IPEX。

C. 显存 / 设备冲突类  
9. 24 GB GPU OOM  
   → a) `--precision bf16 --image-size 512`; b) 4-bit / 8-bit 量化；c) CPU 推理。  
10. cpu / cuda mismatch (“FloatTensor vs CUDABFloat16”)  
    → 保证输入张量 `.to(model.device)`；或把 CLIP 也移动到 CPU；或统一 fp32。  
11. GPU 利用 0%（模型全在 CPU）  
    → 检查是否误删 `--cpu` / `--load_in_4bit` 标志、或显存不足自动回退。  
12. 内存碎片导致 RuntimeError: CUDA out of memory  
    → `torch.cuda.empty_cache()` 仅应急；根本解决是减载。

D. 推理耗时 / 交互类  
13. 首轮推理看似“卡死”  
    → 首次加载权重 2–3 min；CPU CLIP 1–2 min；耐心等待或用 `watch nvidia-smi`.  
14. chat.py 默认交互式导致 EOFError  
    → 使用改造后的 `run_once.py` 非交互模式。  
15. IDE / 浏览器终端断线 → 进程被杀  
    → 用原生 SSH + tmux/screen，或 `nohup … &`.

E. bitsandbytes 相关  
16. bnb 报 `libbitsandbytes_cuda***.so not found`  
    → CUDA 版本不符；CPU 推理或重新编译 bnb。  
17. torch.library impl_abstract 缺失  
    → bnb 0.41+ 依赖 torch>=2.1；锁定 torch 2.0 + bnb 0.39 或切换 CPU。  

F. 其他  
18. `pip install` 慢 / reset  
    → `source /etc/network_turbo` 或换清华源。  
19. 软链创建失败 “No such file or directory”  
    → `mkdir -p $PROJ/openai` 再 ln -sfn。  
20. 文件权限 / locale 警告  
    → 无影响，可忽略或 `export LANG=C.UTF-8`.

──────────────────────────────────────
如何使用这些信息
──────────────────────────────────────
• 若脚本跑失败，先看终端最后报错关键字，在上表中一键定位解决方案。  
• 如需 GPU 4-bit/8-bit 推理，务必解决 B5、B6、B7 三项后，再取消脚本里的 `--cpu`。  
• 建议将本附录复制到团队 Wiki，后续升级版本时可对照。

至此，所有已知问题/坑点已覆盖完毕；未来如再遇新异常，欢迎把完整日志发给我继续补充。