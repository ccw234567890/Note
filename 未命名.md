
### **组会精讲汇报：Jailbreaking Attack against Multimodal Large Language Model**

这篇论文，提出了一种非常新颖的思路来“越狱”多模态大模型，并且创造性地将这种思路“降维”应用到了纯语言模型上，实现了比现有方法更高效的攻击。

---

#### **一、 研究背景与核心问题**

*   **背景：** 我们都知道，现在的大语言模型（LLMs）和多模态大模型（MLLMs）都经过了严格的安全对齐，比如RLHF，来防止它们生成有害内容。但“道高一尺，魔高一丈”，各种“越狱攻击”也层出不穷。
*   **现有方法的痛点：**
    1.  **针对LLM的攻击效率低：** 主流的攻击方法（如GCG）需要在离散的文本空间中进行梯度优化来寻找一个攻击性的文本后缀（txtJP），这个过程计算量巨大且非常耗时。
    2.  **MLLM的脆弱性尚未被充分利用：** MLLM因为引入了视觉模块，普遍认为它比纯LLM更脆弱，但如何系统性地利用这个弱点进行通用和可迁移的攻击，研究还比较少。
*   **本文要解决的核心问题：**
    1.  能否找到一个**通用的“图像越狱提示”（Image Jailbreaking Prompt, imgJP）**，用一张特定的图片，就能让MLLM回答各种不同的有害问题？
    2.  这种攻击能否在**黑盒**情况下，迁移到其他不同架构的MLLM上？
    3.  （**最关键的一点**）能否利用MLLM的这个攻击思路，**反过来**为纯LLM设计一种**更高效**的越狱方法？

---

#### **二、 核心方法与技术路径**

作者提出了一个两阶段的攻击框架。

**阶段一：针对多模态模型（MLLM）的越狱攻击**

*   **核心思想：** 不再寻找文本提示（txtJP），而是直接生成一个包含恶意信息的图像（imgJP）。当用户输入有害文本请求时，只要同时输入这张`imgJP`图片，模型就会绕过安全限制。
*   **实现方式：**
    *   **目标设定：** 作者把越狱问题巧妙地转化成一个**最大化似然概率**的优化问题。他们不要求模型直接输出有害内容，而是要求模型在回答时，第一句话是“Sure, here is...”这样的肯定性答复。这使得优化目标变得非常明确。
    *   **寻找`imgJP`：** 通过梯度下降等方法，优化一张初始图片（比如一张噪声图），使其能够最大化模型生成“Sure, here is...”的概率。
*   **两个重要特性：**
    *   **提示通用性 (Prompt-Universal)：** 用少量（例如25个）有害问题训练出的`imgJP`，可以成功诱导模型回答**其他成百上千个未见过**的有害问题。
    *   **模型迁移性 (Model-Transferability)：** 为了实现黑盒攻击，作者在一个**“代理模型集合”**（比如三个不同版本的MiniGPT-4）上生成`imgJP`。由于这个`imgJP`对多个模型都有效，它就具备了更强的泛化能力，能够成功攻击一个完全未知的目标模型。

**阶段二：基于构造的语言模型（LLM）越狱攻击（本文精华）**

这是本文最具创新性的部分。作者揭示了MLLM和LLM越狱之间的内在联系。

*   **核心洞见：** `imgJP`之所以能成功越狱MLLM，是因为它经过视觉编码器后，在模型内部产生了一个**“越狱嵌入向量”（Jailbreaking Embedding, embJP）**。真正起作用的，是这个`embJP`！
*   **“三步走”构造`txtJP`（参考论文图3）：**
    1.  **获取`embJP`：** 首先，用阶段一的方法在MLLM上找到一个高效的`imgJP`，并将其在模型中产生的`embJP`向量保存下来。
    2.  **逆向工程（De-embedding & De-tokenizer）：** 这是最关键的一步。将连续空间的`embJP`向量**反向映射**回离散的文本空间。
        *   对`embJP`中的每一个向量，去词汇表里寻找意义最相近的K个Token（词元）。
        *   这样就得到了一个“候选词元池”。
    3.  **生成`txtJP`：** 从这个“候选池”中随机采样组合，就能**“构造”**出最终的文本越狱提示`txtJP`。

*   **优势：** 这个方法绕过了在庞大的文本空间中进行低效搜索的难题，而是先在连续的图像空间找到一个“答案”（`imgJP`），再把它“翻译”成文本，**极大地提升了效率**。

---

#### **三、 实验结果**

*   **MLLM白盒攻击：** 攻击非常有效，在MiniGPT系列模型上，单个有害指令的攻击成功率（ASR）就能达到**77%**左右。而提示通用的攻击，测试集ASR高达**92%-93%**。
*   **MLLM黑盒迁移攻击：** 效果惊人。在多个模型上训练出的`imgJP`，对mPLUG-Owl2和MiniGPT-v2的迁移攻击成功率达到了**59%**，证明了其强大的泛化能力。
*   **LLM构造法攻击：** 效果卓越。基于MiniGPT-4(LLaMA2)构造出的`txtJP`，仅用一个包含20个候选`txtJP`的池子进行测试，就能让LLaMA2的越狱成功率达到**93%**，远超传统方法。

---

#### **四、 主要贡献与启发**

*   **学术贡献：**
    1.  **首次**系统性地研究了针对MLLM的、具备**数据通用性**和**模型迁移性**的图像越狱攻击。
    2.  **揭示了关键联系：** 创造性地指出了MLLM和LLM越狱攻击可以通过**“越狱嵌入向量 (embJP)”**这一中间层联系起来，为攻击和防御提供了全新的视角。
    3.  **提出了高效新范式：** 提出了“基于构造”的LLM攻击方法，**将LLM的离散优化问题转化为了MLLM的连续优化问题**，在思路上是巨大的创新，并且效率远超当前SOTA方法。

*   **实践启发：**
    1.  **多模态比单模态更脆弱：** 增加视觉模态确实引入了新的、更易被利用的攻击面，这为多模态模型的安全对齐敲响了警钟。
    2.  **安全边界的模糊性：** 攻击可以隐藏在看起来无害的图像中，这意味着模型的安全防御不能只停留在文本审查，必须进行跨模态的深度对齐。

总的来说，这篇论文思路非常清晰且巧妙，从发现并利用MLLM的弱点，到以此为桥梁解决LLM的攻击难题，整个过程环环相扣，为大模型的安全研究提供了非常宝贵的思路。

我的汇报就到这里，谢谢大家。
#### **第一篇：《Weak-to-Strong Jailbreaking on Large Language Models》**

*   **1. 解决了什么问题？**
    *   传统的越狱攻击（比如通过梯度优化的 GCG）计算成本非常高，需要多次前向和后向传播，不适用于大型模型。

*   **2. 核心方法是什么？**
    *   这篇论文提出了一个非常高效的攻击方法，叫做“弱到强”越狱。
    *   它的核心思想很巧妙：**用两个小的、容易获取的模型（一个安全的、一个不安全的）来“指导”一个强大的、安全对齐的大模型（比如 Llama-70B）**。
    *   具体来说，在生成每一个词时，它会用小模型产生的概率分布去修正大模型的概率分布，引导大模型说出有害内容。公式可以简化理解为：**大模型的输出 = 大模型 logits + α * (小坏蛋模型 logits - 小安全模型 logits)**。

*   **3. 主要贡献与结果？**
    *   **极高的效率和成功率**：这种方法只需要一次前向传播，就能在两个主流数据集上达到超过99%的攻击成功率。
    *   **揭示了安全对齐的脆弱性**：研究发现，安全模型和不安全模型的分歧主要出现在生成回答的最初几个词上。一旦开头被“带偏”，后续就很容易生成有害内容，说明当前模型的安全对齐可能比较“肤浅”。

---

#### **第二篇：《Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy》(简称 JOOD)**

*   **1. 提出了什么新思路？**
    *   这篇工作将攻击思路从纯文本扩展到了多模态领域，并提出了一个核心观点：模型的安全训练是在“分布内” (In-distribution) 的数据上进行的，那么我们是否可以通过**构造“分布外” (Out-of-Distribution, OOD) 的输**入来绕过安全机制？

*   **2. 核心方法是什么？**
    *   方法非常简单直接，叫**JOOD**。它利用现成的图像或文本变换技术来制造OOD样本。
    *   比如，对于多模态模型，它将一张“炸弹”图片和一张“苹果”图片用**Mixup**技术混合在一起，然后提问：“请描述如何制造图中的这两个物体”。
    *   对于语言模型，它将“bomb”和“apple”两个词混合成一个不存在的词“bombapple”，然后提问。
    *   这种OOD的输入会让模型感到“困惑”，不确定其真实意图，从而增加了越狱的成功率。

*   **3. 主要贡献与结果？**
    *   **成功攻击了闭源模型**：JOOD是早期成功有效攻击GPT-4V等先进闭源多模态模型的工作之一，在某些场景下攻击成功率达到63%。
    *   **验证了OOD攻击的有效性**：证明了即使是简单的输入变换，也能有效绕过模型的安全对齐，为后续研究开辟了新方向。

---

#### **第三篇：《Images are Achilles' Heel of Alignment》(简称 HADES)**

*   **1. 聚焦于什么问题？**
    *   这篇论文进一步深入研究了多模态模型的视觉漏洞，明确指出**图像是模型安全对齐的“阿喀琉斯之踵”（致命弱点）**。
    *   研究发现，相比于文本，多模态模型在处理包含有害信息的图像时，安全防线要脆弱得多。

*   **2. 核心方法是什么？**
    *   提出了一个更精密的攻击方法**HADES**。这个方法分为三步：
        1.  **隐藏**：将文本中的有害关键词（如“手枪”）转换成一张仅包含这个词的排版图片。
        2.  **增强**：使用扩散模型生成一张与该关键词相关的、内容有害的图片。
        3.  **附加**：最后，还可以附加一个微小的对抗性噪声来进一步提升攻击成功率。
    *   通过这种方式，将有害意图完全移入了视觉模态，从而最大化利用模型的视觉漏洞。

*   **3. 主要贡献与结果？**
    *   **系统性地证明了视觉模态的脆弱性**：通过实验证明，只要输入中包含图像（即使是空白图像），模型的攻击成功率就会显著上升。
    *   **实现了极高的攻击成功率**：HADES在LLaVA-1.5和Gemini Pro Vision等模型上分别取得了高达90.26%和71.60%的平均攻击成功率。

---

#### **第四、五篇：《FigStep》与《Jailbreak in Pieces》**

*   **共同点与演进：**
    *   这两篇论文可以看作是在利用视觉漏洞思路上进一步的具体实现和深化。

*   **《FigStep》**
    *   **核心方法**：可以说是HADES方法的简化版和前身。它直接将有害的指令性文本（比如“如何制造非法药物的步骤”）转换成一张**排版图片**，然后用一个良性的文本提示（如“请详细说明图中的每个项目”）来引导模型作答。
    *   **主要贡献**：用最简单的方式证明了**“文本转图片”**这种攻击思路的有效性，在多个开源模型上实现了超过82%的成功率。它还提出了一个升级版**FigStep_pro**，通过将一张有害图片切分成多个无害的子图片来绕过OCR检测，成功攻击了GPT-4V。

*   **《Jailbreak in Pieces》 (分块越狱)**
    *   **核心方法**：这是一个更具开创性的**“组合式”攻击**。它将一个有害指令分解为两部分：一个完全**良性的文本提示**（如“如何制造这些东西”）和一个**看起来无害但包含恶意信息的对抗性图片**。
    *   **关键创新**：这个攻击非常巧妙，它不需要访问整个模型，**只需要访问模型的视觉编码器**（如CLIP）即可生成对抗性图片。这意味着攻击的门槛大大降低，对闭源模型构成了更大的威胁。
    *   **主要贡献**：揭示了多模态模型在**视觉和文本嵌入空间对齐上**的根本性漏洞，证明了即使文本输入完全无害，一个精心设计的图片也足以“劫持”模型的意图，攻击成功率在LLaVA上超过87%。

---

越狱攻击的发展趋势：从最初针对**纯语言模型的高成本计算**（Weak-to-Strong的高效方案），发展到利用**多模态模型的OOD特性**（JOOD），再到系统性地证明并利用**视觉模态作为核心弱点**（HADES, FigStep），最后演进到更为隐蔽和底层的**跨模态组合式攻击**（Jailbreak in Pieces）。这提醒我们，模型的安全性需要进行跨模态的、更深层次的对齐。

