# 学习笔记：多输出神经元的反向传播

> [!NOTE] 核心纲要
> 本篇笔记是上一节“单个神经元反向传播”的自然延伸。我们将把模型从**单输出**推广到更普遍的**多输出**情况（即一个全连接层），并探索其反向传播的原理与实践。您将会发现，其核心思想与单输出情况惊人地一致。

---
## 第一部分：模型定义 - 多输出感知机的前向传播 ➡️

> [!info] 多输出神经元模型的工作流程
> ![[Pasted image 20250725001927.png]]
> 前向传播的流程与单输出类似，但现在是并行的、面向向量的。
> 
> 1.  **输入 (Input)**:
>     - 输入仍然是一个特征向量 $x^0 = (x^0_0, x^0_1, \dots, x^0_n)$。
> 
> 2.  **线性加权和 (Linear Combination)**:
>     - ==关键变化==：现在我们有一个**权重矩阵 `W`**，其中 $w^1_{jk}$ 代表从输入 `j` 到**输出 `k`** 的权重。每一个输出神经元 `k` 都会独立计算自己的加权和。
>     - $x^1_k = \sum_{j=0}^{n} w^1_{jk} x^0_j$
>     - 在代码中，这对应于一次高效的**矩阵乘法**。
> 
> 3.  **激活函数 (Activation)**:
>     - 激活函数 `σ` 被**按元素 (element-wise)** 应用于所有加权和的结果上，得到一个输出向量 $O^1 = (O^1_0, \dots, O^1_m)$。
>     - $O^1_k = \sigma(x^1_k)$
> 
> 4.  **计算损失 (Loss)**:
>     - ==关键变化==：总误差 `E` 是**所有输出神经元误差的总和**。
>     - $$ E = \sum_{k=0}^{m} E_k = \sum_{k=0}^{m} \frac{1}{2}(O^1_k - t_k)^2 $$

---
## 第二部分：理论核心 - 多输出下的手动反向传播 🧠

> [!abstract] 目标：计算梯度 $\frac{\partial E}{\partial w^1_{jk}}$
>![[Pasted image 20250725002023.png]]
> 我们的目标是计算**总损失 `E`** 相对于**任意一个权重** $w^1_{jk}$ 的梯度。
> 
> > [!tip] 核心洞察：路径的唯一性
> > 权重 $w^1_{jk}$ （连接输入 `j` 和输出 `k`）只会影响到输出 $O^1_k$，进而**只会影响到总误差 `E` 中的第 `k` 项 $E_k$**。它与 $E_0, E_1, \dots$ 等其他误差项完全无关。
> 
> 这个洞察极大地简化了求导过程：
> $$ \frac{\partial E}{\partial w^1_{jk}} = \frac{\partial}{\partial w^1_{jk}} \left( \sum_{i=0}^{m} E_i \right) = \frac{\partial E_k}{\partial w^1_{jk}} = \frac{\partial}{\partial w^1_{jk}} \left( \frac{1}{2}(O^1_k - t_k)^2 \right) $$
> **问题瞬间被简化，变回了和上一节课完全一样的单输出情况！**
> 
> #### 链式法则三部曲 (与单输出完全相同)
> 1.  **第一环**: $\frac{\partial E}{\partial O^1_k} = (O^1_k - t_k)$
> 2.  **第二环**: $\frac{\partial O^1_k}{\partial x^1_k} = O^1_k(1 - O^1_k)$
> 3.  **第三环**: $\frac{\partial x^1_k}{\partial w^1_{jk}} = x^0_j$
> 
> ---
> > [!success] 最终梯度公式
> > 将三环相乘，我们得到了最终的梯度公式，其**形式**与单输出情况完全一致！
> >
> > $$ \frac{\partial E}{\partial w^1_{jk}} = (O^1_k - t_k) \cdot O^1_k(1 - O^1_k) \cdot x^0_j $$
> > 这完美体现了反向传播算法的**通用性**和**优雅性**。

---
## 第三部分：PyTorch 实战 - 矩阵运算的威力 🚀

> [!example] 代码实现：从理论到实践
> 下面的代码用几行矩阵运算，就高效地实现了我们刚才推导的整个过程。
> ![[Pasted image 20250725002115.png]]
> 
> #### 代码逐行解析
> - **`In [55]`, `[56]`**:
>   - `x = torch.randn(1, 10)`: 创建一个有10个特征的输入样本。
>   - `w = torch.randn(2, 10, requires_grad=True)`: ==这是关键变化==。权重 `w` 是一个 `[2, 10]` 的矩阵，代表我们有 **2 个输出神经元**，每个神经元有10个权重。
> 
> - **`In [57]`**: `o = torch.sigmoid(x @ w.t())`
>   - `w.t()` 将 `w` 转置为 `[10, 2]`。
>   - `x @ w.t()` 执行矩阵乘法 `[1, 10] @ [10, 2]`，得到一个 `[1, 2]` 的输出张量 `o`。这一步同时完成了对2个输出神经元的**加权求和**与**激活**。
> 
> > [!danger] 注意：原图代码存在尺寸不匹配问题
> > 在 `In [59]` 中, `loss = F.mse_loss(torch.ones(1,1), o)` 会报错，因为输出 `o` 的形状是 `[1, 2]`，而目标 `torch.ones(1,1)` 的形状是 `[1, 1]`。
> > **正确写法**应该是让目标的形状与输出相匹配: `loss = F.mse_loss(torch.ones(1, 2), o)`。
> 
> - **`In [61]`**: `loss.backward()`
>   - 魔法再次发生！PyTorch 自动计算总损失 `E` 相对于权重矩阵 `w` 中**每一个元素**的梯度。
> - **`In [62]`**: `w.grad`
>   - `w.grad` 是一个 `[2, 10]` 的梯度矩阵，形状与 `w` 完全相同，其中每个元素都对应着我们手动推导出的 $\frac{\partial E}{\partial w^1_{jk}}$。

> [!summary] 总结
> - 从单输出到多输出，反向传播的核心原理和链式法则**完全适用**。
> - 关键区别在于，总损失是所有输出节点损失的**总和**，但在对某个特定权重求导时，我们只需关注与该权重直接相关的那个输出节点。
> - 现代深度学习框架通过**矩阵运算**将这个过程极大地简化和加速，让我们无需关心底层的循环，只需定义好前向传播的矩阵操作，即可通过 `.backward()` 自动完成一切。