# 📚 贝叶斯统计：共轭先验、序贯更新与点估计 (Lecture 7-8 Detailed Notes)

## 1. 复习：贝叶斯推断的核心框架
在进入复杂推导前，先回顾贝叶斯推断的基本逻辑：
$$Posterior \propto Likelihood \times Prior$$
即：
$$f(\theta | \mathbf{x}) = \frac{f(\mathbf{x} | \theta) \pi(\theta)}{g(\mathbf{x})} \propto f(\mathbf{x} | \theta) \pi(\theta)$$
* **$f(\mathbf{x} | \theta)$ (Likelihood):** 似然函数，包含了样本信息。
* **$\pi(\theta)$ (Prior):** 先验分布，包含了我们对参数的主观信念。
* **$g(\mathbf{x})$ (Marginal):** 归一化常数（Marginal Distribution），对于参数 $\theta$ 而言是常数。

---

## 2. 深度推导：正态-正态共轭模型 (Normal-Normal Model)

### 2.1 模型设定
假设我们观测到样本 $X_1, \dots, X_n$，其均值为 $\theta$（未知），方差 $\sigma^2$（已知）。
* **似然 (Likelihood):** $X_i | \theta \sim N(\theta, \sigma^2)$。
* **先验 (Prior):** 参数 $\theta$ 本身也服从正态分布 $\theta \sim N(\mu_0, v_0^2)$。

> [!abstract] 核心结论
> 如果似然是正态分布（方差已知），先验也是正态分布，那么**后验分布依然是正态分布**。
> $$\theta | \mathbf{x} \sim N(\mu_1, v_1^2)$$

### 2.2 详细推导过程 (代数配方法)

老师强调这个推导是纯代数计算，必须掌握。关键在于识别 $\theta$ 的二次型。

**步骤 1：写出先验与似然的核 (Kernel)**
* 先验 $\pi(\theta) \propto \exp\left( -\frac{(\theta - \mu_0)^2}{2v_0^2} \right)$。
* 似然 $L(\theta) = \prod_{i=1}^n f(x_i|\theta) \propto \prod \exp\left( -\frac{(x_i - \theta)^2}{2\sigma^2} \right) = \exp\left( -\frac{\sum(x_i - \theta)^2}{2\sigma^2} \right)$。

**步骤 2：化简求和项 (The Sum of Squares Trick)**
为了合并，我们需要处理 $\sum(x_i - \theta)^2$。利用恒等变形（加一项减一项 $\bar{x}$）：
$$\sum_{i=1}^n (x_i - \theta)^2 = \sum_{i=1}^n (x_i - \bar{x} + \bar{x} - \theta)^2$$
展开后，交叉项 $\sum 2(x_i - \bar{x})(\bar{x} - \theta)$ 为 0。
$$= \sum (x_i - \bar{x})^2 + n(\bar{x} - \theta)^2$$
其中 $\sum (x_i - \bar{x})^2$ 不包含 $\theta$，在推导后验时可以视为常数扔掉。

**步骤 3：合并指数项**
后验分布 $f(\theta|\mathbf{x}) \propto \text{Prior} \times \text{Likelihood}$：
$$\propto \exp\left( -\frac{1}{2} \left[ \frac{(\theta - \mu_0)^2}{v_0^2} + \frac{n(\theta - \bar{x})^2}{\sigma^2} \right] \right)$$

**步骤 4：系数对比法 (Coefficient Matching)**
我们要把上式配凑成标准正态分布的形式：$\exp\left( -\frac{1}{2v_1^2}(\theta - \mu_1)^2 \right)$。
将中括号内的内容展开，对比 $\theta^2$ 和 $\theta$ 的系数：

1.  **对比 $\theta^2$ 系数（计算后验精度）：**
    $$\frac{1}{v_1^2} = \frac{1}{v_0^2} + \frac{n}{\sigma^2}$$
    *(解释：后验精度 = 先验精度 + 数据精度)*

2.  **对比 $\theta$ 系数（计算后验均值）：**
    通过展开项对比，可以解出 $\mu_1$：
    $$\mu_1 = \frac{\frac{\mu_0}{v_0^2} + \frac{n\bar{x}}{\sigma^2}}{\frac{1}{v_0^2} + \frac{n}{\sigma^2}}$$

> [!danger] 讲义勘误 (Typo in Notes)
> 老师特别提到，讲义上关于方差或均值的公式有一处写错了。
> 原话提到：“这个地方的 $\mu_0$ 是没有的……$\sigma^2 + n...$”。请务必对照推导结果，不要死记讲义上可能错误的公式。

---

## 3. 实例：食品卡路里标注 (Calorie Content Example)

### 3.1 题目背景
* **目的：** 检测食品标签上的卡路里误差 $\theta$。
* **数据：** $n=22$, 样本均值 $\bar{x} = 0.12$。
* **模型：** 误差 $X \sim N(\theta, \sigma^2=100)$。
* **先验：** $\theta \sim N(0, 60)$ (即 $\mu_0=0, v_0^2=60$)。

### 3.2 计算步骤
我们想求厂家严重低标卡路里（即 $\theta > 1$）的概率 $P(\theta > 1 | \mathbf{x})$。

1.  **计算后验参数：**
    代入公式求出后验均值 $\mu_1$ 和后验方差 $v_1^2$。
    老师提到计算出的 $v_1^2 \approx 0.1154$（需自行核对计算）。

2.  **标准化 (Standardization)：**
    因为后验是正态分布，求概率需要转化为标准正态分布 $Z$：
    $$Z = \frac{\theta - \mu_1}{\sqrt{v_1^2}}$$
    目标概率为：
    $$P(\theta > 1 | \mathbf{x}) = P\left( Z > \frac{1 - \mu_1}{v_1} \right) = 1 - \Phi(z_{score})$$
    查表即可得到结果。

---

## 4. 序贯贝叶斯更新 (Sequential Bayesian Updating)

### 4.1 核心概念
贝叶斯推断的一个重要特性是**序贯性**：
> "今天的后验分布，就是明天的先验分布。"
> 观测第一次数据得到的后验，作为观测第二次数据的先验，以此类推。

### 4.2 抛硬币实例 (Coin Toss)
* **问题：** 硬币正面概率 $p$ 未知。
* **初始先验：** Uniform Distribution $U(0,1)$，即 Beta(1,1)。PDF 为 $1$。
* **似然函数：** 伯努利分布 (Bernoulli)。

**演变过程：**

| 步骤 | 观测数据 | 当前先验 $\pi(p)$ | 似然 $L(p)$ | 后验计算 $\pi \times L$ | 归一化后验 $f(p|\cdot)$ | 形状描述 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **0** | 无 | 1 | - | - | $1$ | 水平直线 |
| **1** | 正面 (Head) | 1 | $p$ | $1 \times p = p$ | $2p$ | 斜线 (线性函数) |
| **2** | 正面 (Head) | $2p$ | $p$ | $2p \times p \propto p^2$ | $3p^2$ | 抛物线 (顶点在1) |
| **3** | **反面 (Tail)** | $3p^2$ | $1-p$ | $3p^2 \times (1-p)$ | **$12p^2(1-p)$** | **先升后降** (Beta(3,2)) |

> [!tip] 规律总结
> * **前两次 (Head, Head):** 分布越来越偏向右侧 ($p=1$)，表示正面概率很大的信念增强。
> * **第三次 (Tail):** 观测到反面后，似然函数乘以了 $(1-p)$，强行将 $p=1$ 处的概率压为 0。曲线变成先升后降（峰值在 $2/3$ 处），体现了数据对信念的**修正**作用。

---

## 5. 贝叶斯点估计 (Bayesian Point Estimation)

### 5.1 为什么要点估计？
有时候我们需要汇报一个单一的数值（Estimate），而不是整个分布。例如为了避免法律纠纷或进行明确决策。

### 5.2 关键定义
* **估计量 (Estimator) $\delta(\mathbf{X})$:** 是一个**函数**。它接受观测数据 $\mathbf{X}$ 作为输入。
* **估计值 (Estimate) $\delta(\mathbf{x})$:** 当观测到具体数值 $\mathbf{x}$ 后，算出的具体数字。
* **损失函数 (Loss Function) $L(\theta, a)$:** 衡量当真实值为 $\theta$ 而我们估计为 $a$ 时的“代价”或“尴尬程度”。

### 5.3 贝叶斯决策准则
我们要找一个估计量，使得**后验期望损失 (Posterior Expected Loss)** 最小。
$$\text{Target: } \min_a E[L(\theta, a) | \mathbf{X}=\mathbf{x}]$$
$$E[L(\theta, a) | \mathbf{x}] = \int L(\theta, a) f(\theta | \mathbf{x}) d\theta$$

> [!def] 贝叶斯估计量 (Bayes Estimator)
> 对于每一个观测值 $\mathbf{x}$，能让上述期望损失最小的那个 $a$，记为 $\delta^*(\mathbf{x})$，就是贝叶斯估计量。

---

## 6. 平方误差损失 (Squared Error Loss) 的详细证明

### 6.1 命题
如果损失函数是**平方误差**：
$$L(\theta, a) = (\theta - a)^2$$
那么，贝叶斯估计量 $\delta^*(\mathbf{x})$ 就是参数 $\theta$ 的**后验均值 (Posterior Mean)**。

### 6.2 证明过程 (Add and Subtract Trick)
我们要最小化 $E[(\theta - a)^2 | \mathbf{x}]$。
令 $\hat{\theta} = E[\theta | \mathbf{x}]$ （即后验均值）。

$$
\begin{aligned}
E[(\theta - a)^2 | \mathbf{x}] &= E[(\theta - \hat{\theta} + \hat{\theta} - a)^2 | \mathbf{x}] \\
&= E[\underbrace{(\theta - \hat{\theta})^2}_{Part 1} + \underbrace{2(\theta - \hat{\theta})(\hat{\theta} - a)}_{Part 2} + \underbrace{(\hat{\theta} - a)^2}_{Part 3} | \mathbf{x}]
\end{aligned}
$$

**逐项分析：**
1.  **Part 1:** $E[(\theta - \hat{\theta})^2 | \mathbf{x}] = \text{Var}(\theta | \mathbf{x})$。这是后验方差，对于给定的分布是常数，与 $a$ 无关。
2.  **Part 2 (关键步骤):**
    注意 $(\hat{\theta} - a)$ 对于期望 $E[\cdot|\mathbf{x}]$ 来说是常数（因为它不包含随机变量 $\theta$）。
    $$E[2(\theta - \hat{\theta})(\hat{\theta} - a) | \mathbf{x}] = 2(\hat{\theta} - a) \times \underbrace{E[(\theta - \hat{\theta}) | \mathbf{x}]}_{0} = 0$$
    *原因：$\hat{\theta}$ 本身就是均值，变量减去其均值的期望恒为 0*。
3.  **Part 3:** $(\hat{\theta} - a)^2$。

**结论：**
$$\text{Expected Loss} = \text{Var}(\theta | \mathbf{x}) + (\hat{\theta} - a)^2$$
要让这个式子最小，只能让非负项 $(\hat{\theta} - a)^2 = 0$，即：
$$a = \hat{\theta} = E[\theta | \mathbf{x}]$$
**证毕。**

---

## 7. 综合实例：Beta-Bernoulli 模型的贝叶斯估计

### 7.1 题目信息
* **样本:** $X_1, \dots, X_n \sim \text{Bernoulli}(\theta)$。
* **先验:** $\theta \sim \text{Beta}(\alpha, \beta)$。
* **损失:** Squared Error Loss。
* **目标:** 求 $\theta$ 的 Bayes Estimator。

### 7.2 求解流程
1.  **似然函数:**
    $$f(\mathbf{x}|\theta) = \prod \theta^{x_i}(1-\theta)^{1-x_i} = \theta^{\sum x_i}(1-\theta)^{n - \sum x_i}$$
    令 $Y = \sum x_i$。

2.  **先验分布 (Beta):**
    $$\pi(\theta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}$$
    *(注：Beta分布的归一化常数是 $\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}$)*。

3.  **后验分布:**
    $$f(\theta | \mathbf{x}) \propto \theta^Y (1-\theta)^{n-Y} \cdot \theta^{\alpha-1}(1-\theta)^{\beta-1}$$
    合并指数：
    $$f(\theta | \mathbf{x}) \propto \theta^{(\alpha+Y)-1} (1-\theta)^{(\beta+n-Y)-1}$$
    这依然是一个 **Beta 分布**：
    $$\theta | \mathbf{x} \sim \text{Beta}(\alpha + Y, \beta + n - Y)$$

4.  **计算估计量:**
    因为是 Squared Error Loss，估计量 = 后验均值。
    对于 $\text{Beta}(A, B)$ 分布，其均值为 $\frac{A}{A+B}$。
    
    **最终结果：**
    $$\delta^*(\mathbf{x}) = \frac{\alpha + Y}{(\alpha + Y) + (\beta + n - Y)} = \frac{\alpha + Y}{\alpha + \beta + n}$$

> [!example] 课后思考题：Normal Mean Estimate
> 如果 $X \sim N(\theta, \sigma^2)$，先验 $\theta \sim N(\mu_0, v_0^2)$，在平方损失下，$\theta$ 的贝叶斯估计量是什么？
> **答案：** 不需要重新积分。根据 **Section 2** 的推导，后验是正态分布 $N(\mu_1, v_1^2)$。根据 **Section 6** 的结论，估计量就是后验均值。
> 所以，估计量直接就是 $\mu_1$。


