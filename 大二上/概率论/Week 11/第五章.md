# 第五章 特殊分布 (SPECIAL DISTRIBUTIONS)

[!summary] 章节概览 (离散部分)
* 5.1 引言 (Introduction)
* 5.2 伯努利与二项分布 (The Bernoulli and Binomial Distributions)
* 5.3 超几何分布 (The Hypergeometric Distributions)
* 5.4 泊松分布 (The Poisson Distributions)
* 5.5 负二项分布 (The Negative Binomial Distributions)

---

## 5.1 引言 (Introduction)

[!note] 本章目标
本章将定义和讨论几个在概率论和统计学应用中广泛使用的**特殊分布族**。我们将涵盖离散和连续分布，以及单变量、双变量和多变量类型。

[!info] 本笔记内容 (离散分布)
本笔记将首先聚焦于以下**离散单变量分布**：
* 伯努利分布 (Bernoulli)
* 二项分布 (Binomial)
* 超几何分布 (Hypergeometric)
* 泊松分布 (Poisson)
* 负二项分布 (Negative Binomial)
* 几何分布 (Geometric)

[!tip] 学习路径
对于每一个分布族，我们将：
1.  **描述**它在应用问题中是如何产生的。
2.  **展示**其概率质量函数 (p.f.)。
3.  **讨论**其基本性质（如均值、方差、M.G.F.）。

---
---

## 5.2 伯努利与二项分布 (The Bernoulli and Binomial Distributions)

[!abstract] 核心思想
最简单的实验只有两种可能的结果（如“成功”与“失败”，或“1”与“0”）。
* **伯努利分布**：描述**单次**这种实验的结果。
* **二项分布**：描述**n次独立的、相同的**伯努利实验中，“成功”的总次数。

---
### 伯努利分布 (The Bernoulli Distributions)

> [!example] 例子 5.2.1 - 临床试验
> 在一次临床试验中，对一个病人的治疗结果要么“成功”，要么“失败”。
> * 我们可以令 $X=1$ 代表成功，$X=0$ 代表失败。
> * 这个随机变量 $X$ 的分布完全由一个参数 $p = Pr(X=1)$ 决定。
> * 所有对应 $0 \le p \le 1$ 的此类分布集合，构成了**伯努利分布族**。

> [!definition] 定义 5.2.1 - 伯努利分布
> 一个随机变量 $X$ 服从参数为 $p$ 的**伯努利分布**（其中 $0 \le p \le 1$），如果 $X$ 只能取 0 和 1 两个值，且其概率为：
> * $Pr(X=1) = p$
> * $Pr(X=0) = 1-p$
>
> 其概率质量函数 (p.f.) 可以紧凑地写为：
> $$
> f(x|p) = p^x (1-p)^{1-x} \quad \text{for } x=0, 1
> $$

[!success] 伯努利分布的性质
如果 $X \sim \text{Bernoulli}(p)$：
* **均值 (Mean)**：
    $E(X) = 1 \cdot p + 0 \cdot (1-p) = p$
* **方差 (Variance)**：
    因为 $X^2 = X$（$1^2=1, 0^2=0$），所以 $E(X^2) = E(X) = p$。
    $Var(X) = E(X^2) - [E(X)]^2 = p - p^2 = p(1-p)$
* **矩生成函数 (M.G.F.)**：
    $\psi(t) = E(e^{tX}) = e^{t \cdot 1} \cdot p + e^{t \cdot 0} \cdot (1-p)$
    $$
    \psi(t) = pe^t + (1-p) \quad \text{for } -\infty < t < \infty
    $$

> [!definition] 定义 5.2.2 - 伯努利试验 (Bernoulli Trials)
> 如果一个序列中的随机变量 $X_1, X_2, \dots$ 是**独立同分布 (i.i.d.)** 的，且每个 $X_i$ 都服从参数为 $p$ 的伯努利分布，则称 $X_1, X_2, \dots$ 是参数为 $p$ 的**伯努利试验**。
> (一个无限序列的伯努利试验也称为**伯努利过程**)。

> [!example] 例子 5.2.2 & 5.2.3 - 试验的例子
> * **抛硬币**：重复抛一枚均匀的硬币。令 $X_i=1$ 为正面，$X_i=0$ 为反面。$X_1, X_2, \dots$ 构成了 $p=1/2$ 的伯努利试验。
> * **缺陷部件**：某机器生产的部件有 10% 是有缺陷的，且部件之间相互独立。随机抽查 $n$ 个部件。令 $X_i=1$ 为第 $i$ 个部件有缺陷。$X_1, \dots, X_n$ 构成了 $p=0.1$ 的伯努利试验。

---
### 二项分布 (The Binomial Distributions)

> [!definition] 定义 5.2.3 - 二项分布
> 一个随机变量 $X$ 服从参数为 $n$ 和 $p$ 的**二项分布**（$n$ 为正整数，$0 \le p \le 1$），如果 $X$ 是一个离散随机变量，其 p.f. 如下：
> $$
> f(x|n,p) = \binom{n}{x} p^x (1-p)^{n-x} \quad \text{for } x=0, 1, \dots, n
> $$
> 记作 $X \sim \text{Binomial}(n, p)$。

> [!theorem] 定理 5.2.1 - 伯努利试验与二项分布的关系
> 如果 $X_1, \dots, X_n$ 构成 $n$ 次参数为 $p$ 的伯努利试验，那么它们的**和**：
> $$
> X = X_1 + \dots + X_n
> $$
> 服从参数为 $n$ 和 $p$ 的二项分布。

[!success] 二项分布的性质
利用 $X = \sum X_i$ 的关系，我们可以轻松导出 $X \sim \text{Binomial}(n, p)$ 的性质：
* **均值 (Mean)**：(利用期望的线性性)
    $E(X) = E(\sum X_i) = \sum E(X_i) = \sum p = np$
* **方差 (Variance)**：(利用*独立性*，方差的和=和的方差)
    $Var(X) = Var(\sum X_i) = \sum Var(X_i) = \sum p(1-p) = np(1-p)$
* **矩生成函数 (M.G.F.)**：(利用*独立性*，MGF的积=和的MGF)
    $\psi(t) = E(e^{tX}) = \prod E(e^{tX_i}) = \prod (pe^t + 1-p)$
    $$
    \psi(t) = (pe^t + 1-p)^n
    $$

> [!theorem] 定理 5.2.2 - 二项分布的可加性
> 如果 $X_1, \dots, X_k$ 是**独立**的随机变量，且 $X_i \sim \text{Binomial}(n_i, p)$（注意：**$p$ 必须相同**），那么它们的和 $X = X_1 + \dots + X_k$ 也服从二项分布：
> $$
> X \sim \text{Binomial}(n_1 + \dots + n_k, p)
> $$
>
> **(直观理解**：$X_1$ 是 $n_1$ 次试验的和，$X_2$ 是 $n_2$ 次试验的和... 那么 $X$ 就是 $n_1 + \dots + n_k$ 次试验的总和。)

> [!example] 例子 5.2.6 - 案例：Castaneda v. Partida
> 这是一个关于法庭使用二项分布的真实案例。
> * **背景**：某地人口中 79.1% 是墨西哥裔。
> * **数据**：在 2.5 年间，共 $n=220$ 人被传唤为大陪审团成员，其中 $X=100$ 人是墨西哥裔。
> * **控诉**：声称 $X=100$ 这个数字太低，是歧视的证据。
> * **法庭计算**：法庭**假设没有歧视**（即每次挑选都是独立的，且 $p=0.791$）。然后计算 $X \sim \text{Binomial}(220, 0.791)$ 时，$Pr(X \le 100)$ 的概率。
> * **结果**：这个概率小于 $10^{-25}$，非常小。
>
> > [!warning] 统计学家的 critique
> > 法庭计算的是 $Pr(\text{数据} | \text{无歧视})$。但真正应该关心的是 $Pr(\text{无歧视} | \text{数据})$。这需要使用贝叶斯定理（将在 5.8 节中探讨）。

> [!danger] 重要的注意事项
> 一个随机变量 $X$ 必须是**独立且同参数**的伯努利试验的和，才能服从二项分布。
> 1.  如果各次试验**不独立**...
> 2.  如果各次试验的成功概率 $p$ **不相同**...
> 那么 $X$ 的分布**不是**二项分布。

> [!example] 例子 5.2.7 - 组合测试 (Group Testing)
> 这是一个利用二项分布提高效率的绝佳例子。
> * **问题**：为 1000 人检测一种罕见病，患病率 $p=0.002$。逐一检测需要 1000 次测试。
> * **一级策略 (One-Stage)**：
>    1.  将 1000 人分成 10 组，每组 100 人。
>    2.  混合每组的血液样本，进行 10 次“组测试”。
>    3.  如果一个组的测试呈阳性，则该组 100 人再逐一检测。
> * **一级策略分析**：
>    * $Z_{1,i}$ = 第 $i$ 组 (100人) 中的患病人数。$Z_{1,i} \sim \text{Binomial}(100, 0.002)$。
>    * $Y_{1,i}$ = 第 $i$ 组是否呈阳性（即 $Z_{1,i} > 0$）。
>    * 第 $i$ 组呈阳性的概率 $p_Y = Pr(Z_{1,i} > 0) = 1 - Pr(Z_{1,i} = 0)$
>        $p_Y = 1 - \binom{100}{0}(0.002)^0 (0.998)^{100} = 1 - 0.998^{100} \approx 0.181$
>    * $Y_1 = \sum Y_{1,i}$ = 呈阳性的**组数**。$Y_1 \sim \text{Binomial}(10, 0.181)$。
>    * **总测试次数** $T = 10 \text{ (组测试)} + 100 \cdot Y_1 \text{ (个人测试)}$
>    * **期望总测试次数** $E(T) = 10 + 100 \cdot E(Y_1) = 10 + 100 \cdot (10 \times 0.181) = 10 + 181 = 191$。
> > [!tip] 一级策略结论
> > 期望 191 次测试，远少于 1000 次。

---
---

## 5.3 超几何分布 (The Hypergeometric Distributions)

[!abstract] 核心思想
超几何分布与二项分布非常相似，都用于计算“成功”的次数。
* **二项分布 (Binomial)**：基于**有放回**抽样（或从无限总体中抽样）。各次试验**独立**。
* **超几何分布 (Hypergeometric)**：基于**无放回**抽样（从有限总体中抽样）。各次试验**不独立**。

---
### 定义与例子

> [!example] 例子 5.3.1 - 无放回抽样
> * **情景**：一个盒子里有 $A$ 个红球和 $B$ 个蓝球。
> * **试验**：**无放回**地随机抽取 $n$ 个球 ($n \le A+B$)。
> * **变量**：$X$ = 抽到的红球总数。
> * **分析**：
>    * 令 $X_i=1$ 如果第 $i$ 次抽到红球。
>    * 这些 $X_i$ 是伯努利变量，但它们**不独立**。
>    * 例如，$Pr(X_2=1 | X_1=1) = \frac{A-1}{A+B-1}$，而 $Pr(X_2=1 | X_1=0) = \frac{A}{A+B-1}$。
>    * > [!bug] 因为概率发生了变化，所以 $X = \sum X_i$ **不服从**二项分布。

> [!theorem] 定理 5.3.1 - 超几何分布的概率函数
> 在例子 5.3.1 的情景中，$X$ 的 p.f. 为：
> $$
> f(x|A,B,n) = \frac{\binom{A}{x} \binom{B}{n-x}}{\binom{A+B}{n}}
> $$
> 其中 $x$ 必须满足 $\max(0, n-B) \le x \le \min(n, A)$。
>
> > [!question] 推导 (组合数学)
> > * **分母**：从 $A+B$ 个总球中抽取 $n$ 个，共有 $\binom{A+B}{n}$ 种可能。
> > * **分子**：
> >    1.  从 $A$ 个红球中抽取 $x$ 个，有 $\binom{A}{x}$ 种方法。
> >    2.  从 $B$ 个蓝球中抽取 $n-x$ 个，有 $\binom{B}{n-x}$ 种方法。
> > * 总共的“成功”组合数为 $\binom{A}{x} \binom{B}{n-x}$。
> > * > [!quote] 概率 = 成功组合数 / 总组合数。

> [!definition] 定义 5.3.1 - 超几何分布
> 具有上述 p.f. 的随机变量 $X$，称为服从参数为 $A, B, n$ 的**超几何分布**。

> [!example] 例子 5.3.2 - 从数据集中抽样
> * **情景**：某临床试验的安慰剂组有 34 名患者，其中 10 人“成功”（无复发），24 人“失败”（复发）。
> * **试验**：从这 34 人中**无放回**地抽取 11 人进行复查。
> * **变量**：$X$ = 这 11 人中的“成功”人数。
> * **分布**：$X$ 服从超几何分布，参数 $A=10$ (总成功数), $B=24$ (总失败数), $n=11$ (抽样数)。$X$ 的取值范围是 0 到 10（不可能抽到 11 个成功者，因为总体中只有 10 个）。

---
### 均值与方差

> [!success] 定理 5.3.2 - 超几何分布的均值和方差
> 如果 $X \sim \text{Hypergeometric}(A, B, n)$（且 $A, B, n > 0$），则：
>
> * **均值**： $E(X) = \frac{nA}{A+B}$
> * **方差**： $Var(X) = \frac{nAB}{(A+B)^2} \cdot \left( \frac{A+B-n}{A+B-1} \right)$

> [!question] 均值与方差的推导
> **均值 (E(X))**：
> 1.  我们再次使用指示器变量 $X = X_1 + \dots + X_n$，其中 $X_i=1$ 如果第 $i$ 次抽到红球。
> 2.  虽然 $X_i$ 不独立，但期望的线性性**仍然成立**。
> 3.  $E(X) = \sum_{i=1}^n E(X_i)$。
> 4.  对于**任何** $i$（第1次、第2次、...、第 $i$ 次），在抽样前，它抽到红球的概率（即边际概率）都是 $Pr(X_i=1) = \frac{A}{A+B}$。
> 5.  因此 $E(X_i) = \frac{A}{A+B}$。
> 6.  $E(X) = \sum_{i=1}^n \frac{A}{A+B} = \frac{nA}{A+B}$。
>
> **方差 (Var(X))**：
> 1.  因为 $X_i$ 不独立， $Var(X) = \sum Var(X_i) + 2 \sum_{i<j} Cov(X_i, X_j)$。
> 2.  $Var(X_i)$ 是伯努利变量的方差： $p(1-p) = \frac{A}{A+B} (1 - \frac{A}{A+B}) = \frac{AB}{(A+B)^2}$。
> 3.  所有协方差项 $Cov(X_i, X_j)$ (当 $i \ne j$) 都是相等的（由于对称性）。
> 4.  $Var(X) = n \cdot \frac{AB}{(A+B)^2} + n(n-1) Cov(X_1, X_2)$。
> 5.  **使用一个技巧**：如果 $n = A+B$（即我们抽出了所有的球），那么 $X$ 必然等于 $A$（一个常数）。常数的方差为 0。
> 6.  令 $n=A+B$， $Var(X)=0$，代入上式：
>    $0 = (A+B) \frac{AB}{(A+B)^2} + (A+B)(A+B-1) Cov(X_1, X_2)$
>    $Cov(X_1, X_2) = - \frac{AB}{(A+B)^2 (A+B-1)}$
> 7.  将这个 $Cov$ 值代回第4步的 $Var(X)$ 公式：
>    $Var(X) = n \frac{AB}{(A+B)^2} - n(n-1) \frac{AB}{(A+B)^2 (A+B-1)}$
>    $Var(X) = \frac{nAB}{(A+B)^2} \left[ 1 - \frac{n-1}{A+B-1} \right]$
>    $Var(X) = \frac{nAB}{(A+B)^2} \left[ \frac{(A+B-1) - (n-1)}{A+B-1} \right]$
>    $Var(X) = \frac{nAB}{(A+B)^2} \left( \frac{A+B-n}{A+B-1} \right)$。

---
### 超几何分布与二项分布的对比

[!info] 抽样方法的对比
令 $T = A+B$ 为总体大小，$p = A/T$ 为成功比例。
* **有放回抽样 (二项分布)**：
    * $E(X) = np$
    * $Var(X) = np(1-p)$
* **无放回抽样 (超几何分布)**：
    * $E(X) = n \frac{A}{T} = np$  (**均值相同！**)
    * $Var(X) = n \frac{A}{T} \frac{B}{T} \left( \frac{T-n}{T-1} \right) = np(1-p) \left( \frac{T-n}{T-1} \right)$

[!tip] 有限总体修正 (Finite Population Correction)
$\alpha = \frac{T-n}{T-1}$
这个因子 $\alpha$ 被称为**有限总体修正**。
* 它表示由于“无放回”抽样导致方差**减小**的程度。
* 如果 $n=1$， $\alpha = \frac{T-1}{T-1} = 1$。 (抽1次，有无放回都一样)。
* 如果 $n=T$， $\alpha = \frac{T-T}{T-1} = 0$。 (抽光了，没有不确定性，方差为0)。
* 如果 $T \to \infty$（总体极大），$\alpha \to 1$。

> [!theorem] 定理 5.3.4 - 超几何分布对二项分布的近似
> **核心思想**：当总体 $T$ 相对于样本 $n$ 来说非常大时（$n$ 只是 $T$ 的一小部分），“无放回”抽样（超几何）和“有放回”抽样（二项）**几乎没有区别**。
>
> **结论**：如果 $A, B$ 非常大，而 $p = A/(A+B)$ 固定，则 $f(x | A, B, n)$ （超几何） $\approx$ $f(x | n, p)$ （二项）。
>
> **应用 (例子 5.3.3)**：
> 这就是为什么在“法庭案例”（Example 5.2.6）中，我们可以使用**二项分布**。尽管陪审团是从一个**有限**人口中**无放回**抽取的（技术上是超几何分布），但总人口 $T$ 远大于样本 $n=220$，所以二项分布是一个极好的近似。

---
---

## 5.4 泊松分布 (The Poisson Distributions)

[!abstract] 核心思想
泊松分布用于对**固定时间（或空间）内**发生的**随机事件次数**进行建模。
* 例如：一小时内到达商店的顾客数、电话总机的来电数、一页书上的错别字数、放射性物质在1分钟内的衰变次数。
* 它也是二项分布在 $n$ 很大且 $p$ 很小时的一个有用近似。

---
### 泊松分布的定义与性质

> [!example] 例子 5.4.1 - 泊松分布的推导（从二项分布出发）
> * **情景**：一个商店老板估计平均每小时有 $\lambda = 4.5$ 个顾客。他想知道某一小时内顾客总数 $X$ 的分布。
> * **近似**：
>    1.  将1小时（3600秒）分成 $n=3600$ 个小区间（每区间1秒）。
>    2.  假设每秒钟**最多**只有 1 个顾客到达。
>    3.  顾客在任一秒到达的概率是 $p = 4.5 / 3600 = 0.00125$。
>    4.  $X$ = 1小时内的顾客总数，近似服从 $X \sim \text{Binomial}(n=3600, p=0.00125)$。
> * **困难**：$\binom{3600}{x}(0.00125)^x \dots$ 这个计算太繁琐了。
> * **寻找规律**：
>    1.  考察连续项的**比率** $\frac{f(x+1)}{f(x)} = \frac{(n-x)p}{(x+1)(1-p)}$。
>    2.  因为 $n=3600$ 极大，$p=0.00125$ 极小，所以 $n-x \approx n$ 且 $1-p \approx 1$。
>    3.  $\frac{f(x+1)}{f(x)} \approx \frac{np}{x+1}$。
>    4.  令 $\lambda = np = 3600 \times 0.00125 = 4.5$。
>    5.  $f(x+1) \approx f(x) \cdot \frac{\lambda}{x+1}$。
> * **迭代**：
>    $f(1) \approx f(0) \cdot \frac{\lambda}{1}$
>    $f(2) \approx f(1) \cdot \frac{\lambda}{2} \approx f(0) \cdot \frac{\lambda^2}{2!}$
>    $f(3) \approx f(2) \cdot \frac{\lambda}{3} \approx f(0) \cdot \frac{\lambda^3}{3!}$
>    **$f(x) \approx f(0) \cdot \frac{\lambda^x}{x!}$**
> * **标准化**：
>    1.  p.f. 的总和必须为 1： $\sum_{x=0}^\infty f(x) = f(0) \sum_{x=0}^\infty \frac{\lambda^x}{x!} = 1$。
>    2.  根据泰勒级数， $\sum_{x=0}^\infty \frac{\lambda^x}{x!} = e^\lambda$。
>    3.  所以 $f(0) \cdot e^\lambda = 1 \implies f(0) = e^{-\lambda}$。
> * **结果**：我们推导出了一个新的 p.f.： $f(x) = \frac{e^{-\lambda} \lambda^x}{x!}$。

> [!definition] 定义 5.4.1 - 泊松分布
> 一个随机变量 $X$ 服从均值为 $\lambda$ 的**泊松分布**（$\lambda > 0$），如果其 p.f. 为：
> $$
> f(x|\lambda) = \frac{e^{-\lambda} \lambda^x}{x!} \quad \text{for } x=0, 1, 2, \dots
> $$
> 记作 $X \sim \text{Poisson}(\lambda)$。

[!success] 泊松分布的性质
如果 $X \sim \text{Poisson}(\lambda)$：
* **均值 (Mean)**： $E(X) = \lambda$
    * *推导*： $E(X) = \sum_{x=0}^\infty x \frac{e^{-\lambda} \lambda^x}{x!}$ ( $x=0$ 项为 0)
        $= \sum_{x=1}^\infty \frac{e^{-\lambda} \lambda^x}{(x-1)!} = \lambda \sum_{x=1}^\infty \frac{e^{-\lambda} \lambda^{x-1}}{(x-1)!}$
        (令 $y=x-1$)
        $= \lambda \sum_{y=0}^\infty \frac{e^{-\lambda} \lambda^y}{y!} = \lambda \cdot 1 = \lambda$。

* **方差 (Variance)**： $Var(X) = \lambda$ (**均值和方差相等！**)
    * *推导*：我们先求 $E[X(X-1)]$：
        $E[X(X-1)] = \sum_{x=0}^\infty x(x-1) \frac{e^{-\lambda} \lambda^x}{x!}$ ( $x=0, 1$ 项为 0)
        $= \sum_{x=2}^\infty \frac{e^{-\lambda} \lambda^x}{(x-2)!} = \lambda^2 \sum_{x=2}^\infty \frac{e^{-\lambda} \lambda^{x-2}}{(x-2)!}$
        (令 $y=x-2$)
        $= \lambda^2 \sum_{y=0}^\infty \frac{e^{-\lambda} \lambda^y}{y!} = \lambda^2 \cdot 1 = \lambda^2$。
    * $E(X^2) = E[X(X-1)] + E(X) = \lambda^2 + \lambda$。
    * $Var(X) = E(X^2) - [E(X)]^2 = (\lambda^2 + \lambda) - (\lambda)^2 = \lambda$。

* **矩生成函数 (M.G.F.)**：
    $\psi(t) = E(e^{tX}) = \sum_{x=0}^\infty e^{tx} \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \sum_{x=0}^\infty \frac{(\lambda e^t)^x}{x!}$
    (利用 $e^k = \sum k^x/x!$ 的级数)
    $= e^{-\lambda} \cdot e^{\lambda e^t}$
    $$
    \psi(t) = e^{\lambda(e^t - 1)}
    $$

> [!theorem] 定理 5.4.4 - 泊松分布的可加性
> 如果 $X_1, \dots, X_k$ 是**独立**的随机变量，且 $X_i \sim \text{Poisson}(\lambda_i)$，那么它们的和 $X = X_1 + \dots + X_k$ 也服从泊松分布：
> $$
> X \sim \text{Poisson}(\lambda_1 + \dots + \lambda_k)
> $$
> * *推导 (M.G.F.)*：
>    $\psi_X(t) = \prod \psi_{X_i}(t) = \prod e^{\lambda_i(e^t - 1)} = e^{(\sum \lambda_i)(e^t - 1)}$。
>    这正是 $\text{Poisson}(\sum \lambda_i)$ 的 M.G.F.。

> [!example] 例子 5.4.3 - 泊松可加性的应用
> * $X$ = 第 1 小时顾客数 $\sim \text{Poisson}(4.5)$
> * $Y$ = 第 2 小时顾客数 $\sim \text{Poisson}(4.5)$
> * 假设 $X, Y$ 独立，则 $T = X+Y$ = 2 小时内的总顾客数
> * $T \sim \text{Poisson}(4.5 + 4.5 = 9)$。
> * $Pr(T \ge 12) = 1 - Pr(T \le 11)$。查 $\lambda=9$ 的泊松表，可得 $1 - 0.8030 = 0.1970$。

---
### 泊松近似 (Poisson Approximation)

> [!danger] 定理 5.4.5 - 泊松对二项的近似
> 当 $n$ 很大且 $p$ 很小时，$\text{Binomial}(n, p)$ 分布可以被 $\text{Poisson}(\lambda=np)$ 分布很好地近似。

> [!tip] 何时使用？
> 这是一个经验法则：
> * 当 $n$ 很大（例如 $n \ge 50$）
> * 且 $p$ 很小（例如 $p \le 0.01$）
> * 使得 $\lambda = np$ 是一个适中的数（例如 $\lambda \le 20$）
> 此时近似效果最好。

> [!example] 例子 5.4.4 - 应用泊松近似
> * **问题**：某群体患病率 $p=0.01$。随机抽取 $n=200$ 人，求至少 4 人患病的概率。
> * **精确解 (二项分布)**：
>    $X \sim \text{Binomial}(200, 0.01)$。
>    $Pr(X \ge 4) = \sum_{x=4}^{200} \binom{200}{x} (0.01)^x (0.99)^{200-x}$。 (计算繁琐)
> * **近似解 (泊松分布)**：
>    $n=200$ 很大，$p=0.01$ 很小。
>    $\lambda = np = 200 \times 0.01 = 2$。
>    我们用 $X' \sim \text{Poisson}(2)$ 来近似。
>    $Pr(X' \ge 4) = 1 - Pr(X' \le 3) = 1 - [f(0) + f(1) + f(2) + f(3)]$
>    $= 1 - [e^{-2}\frac{2^0}{0!} + e^{-2}\frac{2^1}{1!} + e^{-2}\frac{2^2}{2!} + e^{-2}\frac{2^3}{3!}]$
>    $= 1 - e^{-2} (1 + 2 + 2 + 4/3) \approx 0.1428$。
> * **对比**：精确的二项概率是 0.1420，近似值 0.1428 非常接近。

---
### 泊松过程 (Poisson Process)

泊松分布是用来描述一个**泊松过程**的结果的。

> [!definition] 定义 5.4.2 - 泊松过程
> 一个“到达”事件流被称为一个速率为 $\lambda$ 的**泊松过程**，如果它满足：
>
> 1.  **泊松计数**：在任何长度为 $\tau$ 的**固定时间区间**内，到达的事件数量 $X$ 服从 $\text{Poisson}(\lambda \tau)$ 分布。
> 2.  **独立增量**：在**不相交**（disjoint）的时间区间内发生的事件数量是**相互独立**的。

> [!example] 例子 5.4.6 - 放射性粒子
> * **情景**：粒子按泊松过程到达，平均速率 $\lambda = 3$ 粒子/分钟。
> * **问题**：在 2 分钟内，有 10 个或更多粒子到达的概率是多少？
> * **解答**：
>    * 时间区间 $\tau = 2$ 分钟。
>    * 均值 $\mu = \lambda \tau = 3 \times 2 = 6$。
>    * $X$ = 2分钟内的粒子数 $\sim \text{Poisson}(6)$。
>    * 查表 $Pr(X \ge 10) = 0.0838$。

> [!warning] 泊松过程的三个基本假设
> 泊松过程（定义 5.4.2）可以从三个更基本的假设推导出来：
> 1.  **独立性**：在不相交区间内的到达次数是独立的。
> 2.  **平稳性/稀有性**：在一个极短的区间 $t$ 内，**至少有 1 次**到达的概率约等于 $\lambda t$。（即 $Pr(\ge 1) = \lambda t + o(t)$）
> 3.  **无同时性**：在一个极短的区间 $t$ 内，**有 2 次或更多**到达的概率是 $o(t)$（比 $t$ 更快地趋近于0，基本为0）。

---
---

## 5.5 负二项分布 (The Negative Binomial Distributions)

[!abstract] 核心思想
负二项分布是二项分布的“反面”。
* **二项分布**：固定**试验次数 $n$**，计算**成功次数 $x$**。
* **负二项分布**：固定**成功次数 $r$**，计算**失败次数 $x$**。

---
### 定义与解释

> [!example] 例子 5.5.1 - 缺陷部件
> * **情景**：一台机器生产部件，次品率 $p$（次品=成功）。
> * **试验**：一名检查员持续观察，直到她看到**第 4 个**（$r=4$）次品为止。
> * **变量**：$X$ = 在看到第 4 个次品之前，所观察到的**正品（失败）数量**。
> * **分布**：$X$ 的分布是什么？

> [!theorem] 定理 5.5.1 - 负二项分布的概率函数
> 在一个 $p$ 为成功概率的伯努利试验序列中，在第 $r$ 次成功发生之前，所观察到的**失败次数 $X$** 的 p.f. 为：
> $$
> f(x|r, p) = \binom{r+x-1}{x} p^r (1-p)^x \quad \text{for } x=0, 1, 2, \dots
> $$
>
> > [!question] 推导
> > 1.  事件 "$X=x$"（$x$ 次失败后迎来第 $r$ 次成功）等价于“在第 $n = r+x$ 次试验时，发生了第 $r$ 次成功”。
> > 2.  这个事件可以分解为两个**独立**部分：
> >    * (A) 在**前 $n-1 = r+x-1$ 次**试验中，必须**恰好**有 $r-1$ 次成功。
> >    * (B) 在**第 $n = r+x$ 次**试验时，必须是 1 次成功。
> > 3.  $Pr(A)$ 是一个二项概率： $Pr(A) = \binom{r+x-1}{r-1} p^{r-1} (1-p)^{(r+x-1)-(r-1)} = \binom{r+x-1}{r-1} p^{r-1} (1-p)^x$。
> > 4.  $Pr(B)$ 是一个伯努利概率： $Pr(B) = p$。
> > 5.  $Pr(X=x) = Pr(A) \times Pr(B) = \left[ \binom{r+x-1}{r-1} p^{r-1} (1-p)^x \right] \cdot p$
> >     $= \binom{r+x-1}{r-1} p^r (1-p)^x$
> > 6.  利用组合恒等式 $\binom{k}{a} = \binom{k}{k-a}$，我们有 $\binom{r+x-1}{r-1} = \binom{r+x-1}{(r+x-1)-(r-1)} = \binom{r+x-1}{x}$。
> > 7.  代入得到 $f(x|r, p) = \binom{r+x-1}{x} p^r (1-p)^x$。

> [!definition] 定义 5.5.1 - 负二项分布
> 具有上述 p.f. 的随机变量 $X$，称为服从参数为 $r$ 和 $p$ 的**负二项分布**（$r \ge 1$ 整数, $0 < p < 1$）。
> 记作 $X \sim \text{NegBin}(r, p)$。

---
### 几何分布 (The Geometric Distributions)

[!info]
几何分布是负二项分布的最简单特例，即 $r=1$ 的情况。
$X$ = 在**第 1 次**成功之前，所观察到的**失败次数**。

> [!definition] 定义 5.5.2 - 几何分布
> $X$ 服从参数为 $p$ 的**几何分布**，如果其 p.f. 为（令 $r=1$）：
> $f(x|1, p) = \binom{1+x-1}{x} p^1 (1-p)^x = \binom{x}{x} p (1-p)^x$
> $$
> f(x|1, p) = p(1-p)^x \quad \text{for } x=0, 1, 2, \dots
> $$
> 记作 $X \sim \text{Geometric}(p)$。

> [!example] 例子 5.5.3 - 彩票中的“三条”
> * **情景**：每日彩票开奖（3个0-9的数字）。“三条”（如777）是一个“成功”事件。
> * **概率**：有 1000 种可能 (000-999)，其中 10 种是“三条”(000, 111, ..., 999)。
> * $p = 10/1000 = 0.01$。
> * $X$ = 在第一次“三条”出现之前，**没有**出现“三条”的天数（失败次数）。
> * $X \sim \text{Geometric}(0.01)$。

> [!theorem] 定理 5.5.2 - 几何分布与负二项分布的关系
> 如果 $X_1, \dots, X_r$ 是 i.i.d. 的随机变量，且 $X_i \sim \text{Geometric}(p)$，
> 那么它们的和 $X = X_1 + \dots + X_r$ 服从 $\text{NegBin}(r, p)$ 分布。
> * **直观理解**：
>    * $X_1$ = 第1次成功前的失败次数。
>    * $X_2$ = 第1次成功后、第2次成功前的失败次数。
>    * ...
>    * $X_r$ = 第 $(r-1)$ 次成功后、第 $r$ 次成功前的失败次数。
>    * $X_1 + \dots + X_r$ = 第 $r$ 次成功前的**总失败次数**。这正是负二项分布的定义。

---
### 属性

[!success] 负二项分布的性质
如果 $X \sim \text{NegBin}(r, p)$：
* **M.G.F.** (定理 5.5.3)：
    $$
    \psi(t) = \left( \frac{p}{1-(1-p)e^t} \right)^r \quad \text{for } t < -\ln(1-p)
    $$
    * *推导*：先求 几何分布($r=1$)的 M.G.F. $\psi_1(t)$：
        $\psi_1(t) = E(e^{tX_1}) = \sum_{x=0}^\infty e^{tx} p(1-p)^x = p \sum_{x=0}^\infty [(1-p)e^t]^x$
        这是一个公比 $\alpha = (1-p)e^t$ 的几何级数，和为 $p \cdot \frac{1}{1-\alpha}$。
        $\psi_1(t) = \frac{p}{1-(1-p)e^t}$。
        因为 $X = X_1 + \dots + X_r$ (i.i.d.)，所以 $\psi(t) = [\psi_1(t)]^r$。

* **均值 (Mean)** (定理 5.5.4)：
    $$
    E(X) = \frac{r(1-p)}{p}
    $$
* **方差 (Variance)** (定理 5.5.4)：
    $$
    Var(X) = \frac{r(1-p)}{p^2}
    $$
    * *推导*：
        1.  先求 几何分布($r=1$)： $E(X_1) = \psi_1'(0) = \frac{1-p}{p}$。（通过对MGF求导）
        2.  $Var(X_1) = \psi_1''(0) - [\psi_1'(0)]^2 = \frac{1-p}{p^2}$。（通过对MGF求二次导）
        3.  因为 $X = X_1 + \dots + X_r$ (i.i.d.)：
        4.  $E(X) = \sum E(X_i) = r \cdot E(X_1) = \frac{r(1-p)}{p}$。
        5.  $Var(X) = \sum Var(X_i) = r \cdot Var(X_1) = \frac{r(1-p)}{p^2}$。

> [!example] 例子 5.5.4 - 彩票的期望
> * $X \sim \text{Geometric}(p=0.01)$ 是失败天数。
> * 期望的**失败天数** $E(X) = \frac{r(1-p)}{p} = \frac{1(1-0.01)}{0.01} = 99$ 天。
> * 期望的**总天数**（直到第一次成功）是 $Y = X+1$。
> * $E(Y) = E(X+1) = E(X) + 1 = 99 + 1 = 100$ 天。

> [!theorem] 定理 5.5.5 - 几何分布的“无记忆性” (Memoryless Property)
> 如果 $X \sim \text{Geometric}(p)$，那么对于任意 $k \ge 0$ 和 $t \ge 0$：
> $$
> Pr(X \ge t+k \mid X \ge k) = Pr(X \ge t)
> $$
> (注：课本原文为 $Pr(X=t+k \mid X \ge k) = Pr(X=t)$，但 $Pr(X \ge t+k \dots)$ 是更标准的形式，两者等价)

> [!bug] "手热"谬误 (Gambler's Fallacy) - 例子 5.5.5
> * **通俗解释**：
>    * "$X \ge k$" 意味着“我们已经失败了 $k$ 次，但仍未成功”。
>    * "$X \ge t+k$" 意味着“在已经失败 $k$ 次之后，我们至少还要再失败 $t$ 次”。
>    * **这个定理说明**：鉴于我们已经失败了 $k$ 次，未来还需要**至少** $t$ 次失败的概率，和从一开始就需要**至少** $t$ 次失败的概率是一样的。
>    * **过去的失败历史不会改变未来的概率**。伯努利试验“没有记忆”。
> * **例子 5.5.5**：
>    * 在彩票的例子中，一个玩家已经等了 120 天（120 次失败）都没有等到“三条”。
>    * 他可能会觉得“三条”**快要来了**（"due"）。
>    * “无记忆性”告诉我们这是**错误**的。
>    * 鉴于他已经失败了 120 次，他**未来**期望的失败次数**仍然**是 $E(X) = 99$ 天。
>    * 他期望的**总**失败次数变成了 $120 + 99 = 219$ 天。

---
---

## 5.6 正态分布 (The Normal Distributions)

[!note] 5.6 节引言
**正态分布**（Normal Distributions），也常称为“高斯分布”，是概率和统计中迄今为止**最重要**的连续分布族。

[!summary] 正态分布的重要性
正态分布的卓越地位主要有三个原因：
1.  **数学便利性**：从正态分布中抽样，其样本的许多重要函数（如均值）的分布是精确已知的，这为统计推断提供了极大的便利。
2.  **自然拟合性**：许多科学家观察到，在各种物理实验中，随机变量的分布（例如，一个同质群体中人的身高、体重，或钢材的抗拉强度）通常都近似服从正态分布。
3.  **中心极限定理 (Central Limit Theorem)**：这是最关键的原因（将在第6.3节讲述）。即使原始分布不是正态的，从该分布中抽取的大样本的**均值**（Average）的分布将**近似**为正态分布。

> [!example] 例子 5.6.1 - 汽车排放
> 研究者测量了46台汽车发动机的氮氧化物排放量（克/英里）。图5.1中的直方图显示了这些数据的分布。当我们想要对排放的概率进行建模时，正态分布族（如图5.2中的曲线所示）被证明是一个非常有价值的模型，它与数据的直方图拟合得相当好。

---
### 正态分布的性质

> [!definition] 定义 5.6.1 - 正态分布 (Normal Distribution)
>
> 一个随机变量 $X$ 服从均值为 $\mu$、方差为 $\sigma^2$ 的**正态分布**（其中 $-\infty < \mu < \infty$ 且 $\sigma > 0$），如果 $X$ 的 p.d.f. 为：
> $$
> f(x|\mu, \sigma^2) = \frac{1}{(2\pi)^{1/2}\sigma} \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \quad \text{for } -\infty < x < \infty
> $$
>
> 记作 $X \sim N(\mu, \sigma^2)$。

> [!theorem] 定理 5.6.1 - 验证 p.d.f.
>
> $f(x|\mu, \sigma^2)$ 是一个合法的 p.d.f.，即 $\int_{-\infty}^{\infty} f(x|\mu, \sigma^2) dx = 1$。
>
> > [!question] 证明
> > 1.  显然 $f(x) \ge 0$。我们需要证明其积分为 1。
> > 2.  令 $y = (x-\mu)/\sigma$，$dx = \sigma dy$。积分变为：
> >    $$\int_{-\infty}^{\infty} \frac{1}{(2\pi)^{1/2}\sigma} e^{-y^2/2} (\sigma dy) = \frac{1}{(2\pi)^{1/2}} \int_{-\infty}^{\infty} e^{-y^2/2} dy $$
> > 3.  我们必须证明 $I = \int_{-\infty}^{\infty} e^{-y^2/2} dy = (2\pi)^{1/2}$。
> > 4.  使用一个技巧：计算 $I^2$ 并转换为极坐标：
> >    $$ I^2 = I \cdot I = \left( \int_{-\infty}^{\infty} e^{-y^2/2} dy \right) \left( \int_{-\infty}^{\infty} e^{-z^2/2} dz \right) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-(y^2+z^2)/2} dy dz$$
> > 5.  令 $y = r \cos \theta$，$z = r \sin \theta$。$y^2+z^2 = r^2$，$dy dz = r dr d\theta$。
> >    $$I^2 = \int_{0}^{2\pi} \int_{0}^{\infty} e^{-r^2/2} r dr d\theta$$
> > 6.  (内层积分)：令 $v = r^2/2$，$dv = r dr$。$\int_{0}^{\infty} e^{-v} dv = [-e^{-v}]_0^\infty = 1$。
> > 7.  (外层积分)：$\int_{0}^{2\pi} 1 d\theta = 2\pi$。
> > 8.  因此 $I^2 = 2\pi$，所以 $I = (2\pi)^{1/2}$。代回第2步，总积分为 $\frac{1}{(2\pi)^{1/2}} \cdot (2\pi)^{1/2} = 1$。

> [!theorem] 定理 5.6.2 - 正态分布的 M.G.F.
>
> $X \sim N(\mu, \sigma^2)$ 的矩生成函数 (m.g.f.) 为：
> $$
> \psi(t) = \exp \left( \mu t + \frac{1}{2}\sigma^2 t^2 \right) \quad \text{for } -\infty < t < \infty
> $$
>
> > [!question] 证明
> > 9.  $\psi(t) = E(e^{tX}) = \int_{-\infty}^{\infty} e^{tx} f(x|\mu, \sigma^2) dx$。
> > 10.  关键步骤是**配方法 (completing the square)** 来合并指数项：
> >    $$tx - \frac{(x-\mu)^2}{2\sigma^2} = \mu t + \frac{1}{2}\sigma^2 t^2 - \frac{[x - (\mu + \sigma^2 t)]^2}{2\sigma^2}$$
> >    
> > 11.  代回积分：
> >    $$ \psi(t) = \exp \left( \mu t + \frac{1}{2}\sigma^2 t^2 \right) \int_{-\infty}^{\infty} \underbrace{\frac{1}{(2\pi)^{1/2}\sigma} \exp \left\{ -\frac{[x - (\mu + \sigma^2 t)]^2}{2\sigma^2} \right\}}_{C} dx $$
> >    
> > 12.  观察 $C$ 部分。它是一个**新的**正态分布的 p.d.f.，其均值为 $(\mu + \sigma^2 t)$，方差为 $\sigma^2$。
> > 13.  根据定理 5.6.1，任何正态 p.d.f. 的积分都等于 1。
> > 14.  因此 $\psi(t) = \exp \left( \mu t + \frac{1}{2}\sigma^2 t^2 \right) \cdot 1$，得证。

> [!theorem] 定理 5.6.3 - 均值与方差
>
> $N(\mu, \sigma^2)$ 分布的均值确实是 $\mu$，方差确实是 $\sigma^2$。
>
> > [!question] 证明
> > 15.  我们对 M.G.F. $\psi(t) = \exp(\mu t + \frac{1}{2}\sigma^2 t^2)$ 求导：
> > 16.  **一阶导**：
> >    $\psi'(t) = (\mu + \sigma^2 t) \cdot \exp(\mu t + \frac{1}{2}\sigma^2 t^2)$
> >    $E(X) = \psi'(0) = (\mu + 0) \cdot \exp(0) = \mu$
> > 17.  **二阶导** (使用乘法法则)：
> >    $\psi''(t) = \sigma^2 \cdot \exp(\dots) + (\mu + \sigma^2 t) \cdot (\mu + \sigma^2 t) \cdot \exp(\dots)$ (注：课本中 $\psi''(t)$ 的公式 $([\mu+\sigma^{2}t]^{2}+\sigma^{2})exp(\dots)$ 是对的，推导过程略有不同，但结果一致)
> >    $E(X^2) = \psi''(0) = (\sigma^2) \cdot \exp(0) + (\mu + 0)^2 \cdot \exp(0) = \sigma^2 + \mu^2$
> > 18.  **方差**：
> >    $Var(X) = E(X^2) - [E(X)]^2 = (\sigma^2 + \mu^2) - \mu^2 = \sigma^2$
> >
> > [!note] M.G.F. 对所有 $t$ 都存在，因此正态分布的所有阶矩 $E(X^k)$ 都存在且有限。

> [!example] 例子 5.6.3 - 股票价格变化
>
> 一种流行的股票价格模型是 $S_u = S_0 e^{Z_u}$，其中 $S_0$ 是现价，$u$ 是时间长度，$Z_u \sim N(\mu u, \sigma^2 u)$。
> $S_u$ 的期望值可以通过 $Z_u$ 的 M.G.F. $\psi(t) = \exp(\mu u t + \frac{1}{2}\sigma^2 u t^2)$ 来计算：
> $$
> E(S_u) = S_0 E(e^{Z_u}) = S_0 \cdot \psi(1) = S_0 \exp\left(\mu u + \frac{\sigma^2 u}{2}\right)
> $$

> [!note] 正态分布的形状
> * p.d.f. $f(x|\mu, \sigma^2)$ 关于 $x=\mu$ **对称**。
> * 均值 = 中位数 = 众数 = $\mu$。
> * p.d.f. 在 $x=\mu$ 处达到最大值。
> * p.d.f. 在 $x = \mu + \sigma$ 和 $x = \mu - \sigma$ 处有**拐点** (inflection points)。
> * 曲线呈“钟形”。（如图 5.3 所示）
> * **$\sigma$ 的影响**：$\sigma$ 越小，p.d.f. 越高瘦，数据越集中；$\sigma$ 越大，p.d.f. 越矮胖，数据越分散。（如图 5.4 所示）

> [!theorem] 定理 5.6.4 - 线性变换
>
> 如果 $X \sim N(\mu, \sigma^2)$ 且 $Y = aX + b$ (其中 $a \ne 0$)，那么 $Y$ 也服从正态分布：
> $$
> Y \sim N(a\mu + b, a^2\sigma^2)
> $$
>
> > [!question] 证明
> > 使用 M.G.F.：
> > $\psi_Y(t) = E(e^{tY}) = E(e^{t(aX+b)}) = E(e^{bt} \cdot e^{(at)X})$
> > $= e^{bt} \cdot E(e^{(at)X}) = e^{bt} \cdot \psi_X(at)$
> > $= e^{bt} \cdot \exp \left( \mu (at) + \frac{1}{2}\sigma^2 (at)^2 \right)$
> > $= \exp \left( bt + a\mu t + \frac{1}{2}a^2\sigma^2 t^2 \right)$
> > $= \exp \left( (a\mu+b)t + \frac{1}{2}(a^2\sigma^2)t^2 \right)$
> > 这正是 $N(a\mu+b, a^2\sigma^2)$ 的 M.G.F.。

---
### 标准正态分布 (Standard Normal Distribution)

> [!definition] 定义 5.6.2 - 标准正态分布
>
> 均值为 0、方差为 1 的正态分布 $N(0, 1)$ 被称为**标准正态分布**。
> * 其 p.d.f. 通常记作 $\phi(x)$： $\phi(x) = \frac{1}{(2\pi)^{1/2}} e^{-x^2/2}$
> * 其 c.d.f. 通常记作 $\Phi(x)$： $\Phi(x) = \int_{-\infty}^{x} \phi(u) du$

> [!warning] c.d.f. $\Phi(x)$
>
> c.d.f. $\Phi(x)$ 没有一个“封闭形式”的初等函数表达式。它的值必须通过查表或计算机数值近似来获得。

> [!theorem] 定理 5.6.5 & 5.6.6 - 标准化的威力
>
> 1.  **对称性**： $\Phi(-x) = 1 - \Phi(x)$
> 2.  **标准化**：如果 $X \sim N(\mu, \sigma^2)$，那么 $Z = \frac{X - \mu}{\sigma}$ 服从 $N(0, 1)$。
> 3.  **计算任意正态 c.d.f.**：
>    $F_X(x) = Pr(X \le x) = Pr\left( Z \le \frac{x-\mu}{\sigma} \right)$
>    $$ F_X(x) = \Phi\left(\frac{x-\mu}{\sigma}\right)$$
>    
> 4.  **计算任意正态分位数**：
>    $p = F_X(x_p) \implies x_p = F_X^{-1}(p)$
>    $$F_X^{-1}(p) = \mu + \sigma \Phi^{-1}(p)$$
>    

> [!example] 例子 5.6.4 - 计算正态概率
>
> * **问题**：设 $X \sim N(5, 4)$ (即 $\mu=5, \sigma=2$)。求 $Pr(1 < X < 8)$。
> * **解答**：
>    1.  **标准化**：
>        $Pr(1 < X < 8) = Pr\left( \frac{1-5}{2} < \frac{X-5}{2} < \frac{8-5}{2} \right)$
>        $= Pr(-2 < Z < 1.5)$
>    2.  **查表**：
>        $= Pr(Z < 1.5) - Pr(Z \le -2)$
>        $= \Phi(1.5) - \Phi(-2)$
>    3.  **利用对称性** $\Phi(-2) = 1 - \Phi(2)$：
>        $= \Phi(1.5) - (1 - \Phi(2))$
>        $= 0.9332 - (1 - 0.9773) = 0.9105$

> [!example] 例子 5.6.5 - 计算正态分位数
>
> * **问题**：汽车排放 $X \sim N(1.329, 0.4844^2)$。求 0.05 分位数（即 $x$ 使得 $Pr(X \le x) = 0.05$）。
> * **解答**：
>    1.  **公式**：$x = F^{-1}(0.05) = \mu + \sigma \Phi^{-1}(0.05)$
>    2.  **求 $\Phi^{-1}(0.05)$**：
>        利用对称性 $\Phi^{-1}(0.05) = - \Phi^{-1}(1 - 0.05) = - \Phi^{-1}(0.95)$。
>        查表，$\Phi(1.645) \approx 0.95$。所以 $\Phi^{-1}(0.95) \approx 1.645$。
>        $\Phi^{-1}(0.05) \approx -1.645$。
>    3.  **计算**：
>        $x = 1.329 + 0.4844 \cdot (-1.645) = 1.329 - 0.7968 \approx 0.5322$

> [!note] "k-sigma" 法则 (见表 5.2)
>
> 对于任何正态分布 $N(\mu, \sigma^2)$：
> * $Pr(|X-\mu| \le 1\sigma) = Pr(|Z| \le 1) \approx 0.6826$ (约 68%)
> * $Pr(|X-\mu| \le 2\sigma) = Pr(|Z| \le 2) \approx 0.9544$ (约 95%)
> * $Pr(|X-\mu| \le 3\sigma) = Pr(|Z| \le 3) \approx 0.9974$ (约 99.7%)
>
> 即使在均值 $\mu$ 的 4 个标准差 ($\pm 4\sigma$) 之外，概率也仅有 0.00006。

---
### 正态变量的线性组合

> [!theorem] 定理 5.6.7 & 推论 5.6.1 - 独立正态变量的线性组合
>
> 如果 $X_1, \dots, X_k$ 是**独立**的正态随机变量， $X_i \sim N(\mu_i, \sigma_i^2)$，
> 那么它们的**任意线性组合** $Y = a_1 X_1 + \dots + a_k X_k + b$
> **仍然是正态分布**！
>
> 其均值和方差为：
> * $E(Y) = a_1\mu_1 + \dots + a_k\mu_k + b$
> * $Var(Y) = a_1^2\sigma_1^2 + \dots + a_k^2\sigma_k^2$ (由于独立性)
>
> > [!question] 证明
> > 通过 M.G.F.：
> > $\psi_Y(t) = E(e^{tY}) = E(e^{t(a_1 X_1 + \dots + b)}) = e^{bt} \prod E(e^{t a_i X_i})$
> > $= e^{bt} \prod \psi_{X_i}(a_i t)$
> > $= e^{bt} \prod \exp\left( \mu_i (a_i t) + \frac{1}{2}\sigma_i^2 (a_i t)^2 \right)$
> > $= \exp\left( bt + (\sum a_i \mu_i) t + \frac{1}{2} (\sum a_i^2 \sigma_i^2) t^2 \right)$
> > $= \exp\left( (b + \sum a_i \mu_i) t + \frac{1}{2} (\sum a_i^2 \sigma_i^2) t^2 \right)$
> > 这正是一个正态分布的 M.G.F.。

> [!example] 例子 5.6.6 - 男女身高
>
> * **情景**：
>    * $W \sim N(\mu_W=65, \sigma_W^2=1^2)$ (女身高)
>    * $M \sim N(\mu_M=68, \sigma_M^2=3^2)$ (男身高)
>    * $W$ 和 $M$ 独立。
> * **问题**：求 $Pr(W > M)$。
> * **解答**：
>    1.  $Pr(W > M) = Pr(W - M > 0)$。
>    2.  $Y = W - M$ 是独立正态变量的线性组合 ($a_1=1, a_2=-1, b=0$)。
>    3.  $Y$ 也服从正态分布。
>    4.  $E(Y) = E(W) - E(M) = 65 - 68 = -3$。
>    5.  $Var(Y) = \text{Var}(W) + \text{Var}(-M) = Var(W) + (-1)^2 Var(M) = 1^2 + 3^2 = 10$。
>    6.  $Y \sim N(-3, 10)$。$\sigma_Y = \sqrt{10}$。
>    7.  $Pr(Y > 0) = Pr\left( Z > \frac{0 - (-3)}{\sqrt{10}} \right) = Pr\left( Z > \frac{3}{\sqrt{10}} \right)$
>        $= Pr(Z > 0.949) = 1 - \Phi(0.949) \approx 0.171$。

> [!definition] 定义 5.6.3 - 样本均值 (Sample Mean)
>
> $X_1, \dots, X_n$ 的**样本均值** $\overline{X}_n$ 定义为：
> $$
> \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
> $$
>

> [!theorem] 推论 5.6.2 - 正态样本均值的分布
>
> 如果 $X_1, \dots, X_n$ 是来自 $N(\mu, \sigma^2)$ 的**随机样本**（即 i.i.d.），
> 那么 $\overline{X}_n$ 也服从正态分布：
> $$
> \overline{X}_n \sim N\left(\mu, \frac{\sigma^2}{n}\right)
> $$
>
> > [!question] 证明
> > 这是线性组合 ($a_i = 1/n, b=0$) 的一个特例。
> > * $E(\overline{X}_n) = \sum \frac{1}{n} \mu = n(\frac{1}{n}\mu) = \mu$。
> > * $Var(\overline{X}_n) = \sum (\frac{1}{n})^2 \sigma^2 = n(\frac{1}{n^2}\sigma^2) = \frac{\sigma^2}{n}$。

> [!example] 例子 5.6.7 & 5.6.8 - 样本均值的应用
>
> * **情景**：从 $N(\mu, \sigma^2=9)$ 中抽取 $n$ 个样本。$\overline{X}_n \sim N(\mu, 9/n)$。 $Z = \frac{\overline{X}_n - \mu}{3/\sqrt{n}}$ 服从 $N(0, 1)$。
> * **问题1 (求n)**：$n$ 最小为多少，才能使 $Pr(|\overline{X}_n - \mu| \le 1) \ge 0.95$？
> * **解答1**：
>    1.  $Pr(|\overline{X}_n - \mu| \le 1) = Pr\left( |Z| \le \frac{1}{3/\sqrt{n}} \right) = Pr\left( |Z| \le \frac{\sqrt{n}}{3} \right)$。
>    2.  我们需要 $Pr(|Z| \le \frac{\sqrt{n}}{3}) \ge 0.95$。
>    3.  查表可知，$Pr(|Z| \le 1.96) = 0.95$。
>    4.  因此需要 $\frac{\sqrt{n}}{3} \ge 1.96 \implies \sqrt{n} \ge 5.88 \implies n \ge 34.57$。
>    5.  $n$ 必须至少为 35。
> * **问题2 (置信区间)**：如果 $n=36$，求一个有 95% 概率包含 $\mu$ 的区间。
> * **解答2**：
>    1.  $0.95 = Pr(|Z| < 1.96) = Pr\left( \left| \frac{\overline{X}_n - \mu}{3/\sqrt{36}} \right| < 1.96 \right)$
>    2.  $0.95 = Pr(|\overline{X}_n - \mu| < 1.96 \cdot \frac{3}{6}) = Pr(|\overline{X}_n - \mu| < 0.98)$
>    3.  $|\overline{X}_n - \mu| < 0.98$ 等价于 $\overline{X}_n - 0.98 < \mu < \overline{X}_n + 0.98$。
>    4.  结论：随机区间 $(\overline{X}_n - 0.98, \overline{X}_n + 0.98)$ 有 95% 的概率会包含未知的真实均值 $\mu$。

---
### 对数正态分布 (The Lognormal Distributions)

> [!definition] 定义 5.6.4 - 对数正态分布
>
> 如果一个随机变量 $Y = \log(X)$ 服从正态分布 $N(\mu, \sigma^2)$，
> 那么我们称 $X$ 服从**对数正态分布** (Lognormal Distribution)，参数为 $\mu$ 和 $\sigma^2$。
>
> > [!warning] 注意
> > $\mu$ 和 $\sigma^2$ 是 $X$ 的**对数**的均值和方差，**不是** $X$ 本身的均值和方差。

> [!example] 例子 5.6.9 - 滚珠轴承的失效时间
>
> 失效时间 $X$ 经常用对数正态分布建模。假设 $\log(X) \sim N(\mu=4.15, \sigma^2=0.5334^2)$。
> * **求 $X$ 的 c.d.f.**：
>    $F(x) = Pr(X \le x) = Pr(\log(X) \le \log(x))$
>    $= Pr\left( \frac{\log(X) - \mu}{\sigma} \le \frac{\log(x) - \mu}{\sigma} \right)$
>    $$F(x) = \Phi\left( \frac{\log(x) - 4.15}{0.5334} \right) $$
>    
> * **求 0.9 分位数**：
>    $x_p = F^{-1}(p) \implies \log(x_p) = F_{\log(X)}^{-1}(p) = \mu + \sigma \Phi^{-1}(p)$
>    $x_{0.9} = \exp[ 4.15 + 0.5334 \cdot \Phi^{-1}(0.9) ]$
>    $x_{0.9} = \exp[ 4.15 + 0.5334 \cdot (1.28) ] = \exp[4.833] \approx 125.6$

> [!note] 对数正态分布的矩
>
> 如果 $\log(X) \sim N(\mu, \sigma^2)$，且 $Y = \log(X)$ 的 M.G.F. 为 $\psi_Y(t) = \exp(\mu t + \frac{1}{2}\sigma^2 t^2)$。
> 我们可以利用 $E(X^t) = E(e^{tY}) = \psi_Y(t)$。
> * **均值 (t=1)**： $E(X) = \psi_Y(1) = \exp\left(\mu + \frac{1}{2}\sigma^2\right)$
> * **二阶矩 (t=2)**： $E(X^2) = \psi_Y(2) = \exp(2\mu + \frac{1}{2}\sigma^2(2^2)) = \exp(2\mu + 2\sigma^2)$
> * **方差**：
>    $Var(X) = E(X^2) - [E(X)]^2$
>    $= \exp(2\mu + 2\sigma^2) - [\exp(\mu + \frac{1}{2}\sigma^2)]^2$
>    $= \exp(2\mu + 2\sigma^2) - \exp(2\mu + \sigma^2)$
>    $= \exp(2\mu + \sigma^2) [ \exp(\sigma^2) - 1 ]$

> [!example] 例子 5.6.10 - Black-Scholes 期权定价
>
> * **模型**：股票价格 $S_u = S_0 e^{Z_u}$，其中 $Z_u = \mu u + \sigma \sqrt{u} Z$，$Z \sim N(0, 1)$。
> * **风险中性定价**：我们调整 $\mu$，使得股票的预期收益率等于无风险利率 $r$。这需要 $E(S_u) = S_0 e^{ru}$。
>    * $E(S_u) = E(S_0 e^{\mu u + \sigma \sqrt{u} Z}) = S_0 e^{\mu u} E(e^{(\sigma \sqrt{u}) Z})$
>    * $E(e^{(\sigma \sqrt{u}) Z})$ 是 $N(0, 1)$ 的 M.G.F. $\psi_Z(t) = e^{t^2/2}$ 在 $t=\sigma \sqrt{u}$ 处的值。
>    * $E(S_u) = S_0 e^{\mu u} \cdot e^{(\sigma \sqrt{u})^2 / 2} = S_0 e^{\mu u + \sigma^2 u / 2}$。
>    * $S_0 e^{ru} = S_0 e^{\mu u + \sigma^2 u / 2} \implies \mu = r - \sigma^2/2$。
> * **期权价值**：
>    期权价值（以 $q$ 的价格购买）为 $h(S_u) = \max(S_u - q, 0)$。
>    期权的风险中性价格 $C$ 是其未来期望价值的现值： $C = e^{-ru} E[h(S_u)]$。
>    代入 $\mu = r - \sigma^2/2$ 并计算该积分（通过配方法）...
> * **Black-Scholes 公式**：
>    $$C = S_0 \Phi(\sigma u^{1/2} - c) - q e^{-ru} \Phi(-c)$$
>    
>    其中 $c = \frac{\log(q/S_0) - (r - \sigma^2/2)u}{\sigma u^{1/2}}$。

> [!summary] 5.6 总结
> 1.  **正态分布 $N(\mu, \sigma^2)$** 是由其均值 $\mu$ 和方差 $\sigma^2$ 决定的钟形曲线。
> 2.  **标准化**是关键： $Z = (X-\mu)/\sigma$ 服从 $N(0, 1)$。
> 3.  **计算**：$F_X(x) = \Phi([x-\mu]/\sigma)$ 且 $F_X^{-1}(p) = \mu + \sigma \Phi^{-1}(p)$。
> 4.  **线性组合**：独立正态变量的线性组合**仍然是**正态变量。
> 5.  **对数正态分布**：如果 $\log(X)$ 是正态的，则 $X$ 是对数正态的。