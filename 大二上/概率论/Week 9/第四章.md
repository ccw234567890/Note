---
tags: [概率论, 统计学, 期望, 方差, 随机变量]
aliases: [Expectation, Mean, Expected Value, Variance, LOTUS, Jensen's Inequality]
---

# 第四章：期望 (EXPECTATION)

> [!summary] 章节目录
> * 4.1 随机变量的期望 (The Expectation of a Random Variable)
> * 4.2 期望的性质 (Properties of Expectations)
> * 4.3 方差 (Variance)
> * 4.4 矩 (Moments)
> * 4.5 均值与中位数 (The Mean and the Median)
> * 4.6 协方差与相关性 (Covariance and Correlation)
> * 4.7 条件期望 (Conditional Expectation)
> * 4.8 效用 (Utility)
> * 4.9 补充练习 (Supplementary Exercises)

---

## 4.1 随机变量的期望 (The Expectation of a Random Variable)

随机变量 $X$ 的分布包含了所有关于 $X$ 的概率信息。但有时，整个分布信息过于繁杂，我们希望能用一些“摘要”来描述分布。

**期望 (Expectation)**，也称为**均值 (Mean)** 或 **期望值 (Expected Value)**，就是这样一种摘要。它告诉我们 $X$ 的“平均值”或“期望值”在什么位置，而无需描述整个分布。

### 离散分布的期望 (Expectation for a Discrete Distribution)

> [!example] 例子 4.1.1 & 4.1.2：股票的公允价格
>
> * **问题**：一个投资者考虑是否以每股 \$18 的价格投资一支股票一年。一年后，股票的价值将是 $18+X$ 美元，其中 $X$ 是价格变化量。投资者想计算 $X$ 的“平均值”，来和存银行 5% 的利息（即 $18 \times 0.05 = 0.9$ 美元）进行比较。
> * **假设**：$X$ 是一个随机变量，只能取 -2, 0, 1, 4 四个值，其概率分别为：
>     * $Pr(X=-2) = 0.1$
>     * $Pr(X=0) = 0.4$
>     * $Pr(X=1) = 0.3$
>     * $Pr(X=4) = 0.2$
> * **计算**：直观的“平均值”应该是所有可能值 $x$ 的**加权平均**，权重就是它发生的概率 $f(x)$。
>     $$
>     \text{加权平均} = (-2)(0.1) + (0)(0.4) + (1)(0.3) + (4)(0.2) = 0.9
>     $$
> * **结论**：这个期望收益 0.9 美元和存银行的利息 0.9 美元相等，所以 \$18 的价格看起来是“公允”的。

> [!definition] 定义 4.1.1：有界离散随机变量的均值
>
> 设 $X$ 是一个有界的离散随机变量，其概率质量函数 (p.f.) 为 $f$。$X$ 的**期望**，记为 $E(X)$，定义如下：
> $$
> E(X) = \sum_{\text{所有 } x} x f(x)
> $$
>
> > [!note] 注意
> > 期望值 $E(X)$ 不一定是 $X$ 的一个可能取值。在上面的股票例子中，$E(X)=0.9$，但 $X$ 只能取 -2, 0, 1, 4。

> [!example] 例子 4.1.3：伯努利随机变量 (Bernoulli Random Variable)
>
> 设 $X$ 服从参数为 $p$ 的伯努利分布，即 $X$ 只取 0 和 1 两个值：
> * $Pr(X=1) = p$
> * $Pr(X=0) = 1-p$
>
> 那么 $X$ 的均值是：
> $$
> E(X) = 0 \times (1-p) + 1 \times p = p
> $$
> **这是一个非常重要的结论！**

---
#### 处理无穷和无界情况

> [!definition] 定义 4.1.2：一般离散随机变量的均值
>
> 设 $X$ 是一个离散随机变量，其 p.f. 为 $f$。假设下面**至少一个**和是有限的：
> $$
> \sum_{\text{正数 } x} x f(x) \quad , \quad \sum_{\text{负数 } x} x f(x)
> $$
> 那么 $X$ 的均值（期望）**存在**，并定义为：
> $$
> E(X) = \sum_{\text{所有 } x} x f(x)
> $$
>
> > [!warning] 期望不存在的情况
> > 如果上述两个和（正数部分和负数部分）**都是无穷大**，那么 $E(X)$ **不存在**。
> >
> > **通俗解释**：这是为了避免微积分中 "$\infty - \infty$" 这种未定式。如果正数部分的和是 $+\infty$ 而负数部分的和是 $-\infty$，那么 $E(X)$ 的值将取决于你求和的顺序，这是没有意义的。
> >
> > * 如果只有一个和是无穷（例如正数和为 $+\infty$，负数和为 -5），那么 $E(X) = +\infty$。
> > * 如果两个和都是有限的， $E(X)$ 就是一个明确的有限值。

> [!example] 例子 4.1.4：均值不存在
>
> 设 $X$ 的 p.f. 为：
> $f(x) = \frac{1}{2|x|(|x|+1)}$ ，对于 $x = \pm 1, \pm 2, \pm 3, \dots$
>
> 它的正数部分和为：
> $\sum_{x=1}^{\infty} x \frac{1}{2x(x+1)} = \sum_{x=1}^{\infty} \frac{1}{2(x+1)} = \infty$ (这是调和级数的一半)
>
> 它的负数部分和为：
> $\sum_{x=-1}^{-\infty} x \frac{1}{2|x|(|x|+1)} = \sum_{x=-1}^{-\infty} \frac{-1}{2(|x|+1)} = -\infty$
>
> 因为两部分都是无穷，所以 $E(X)$ **不存在**。

> [!example] 例子 4.1.5：无穷均值
>
> 设 $X$ 的 p.f. 为：
> $f(x) = \frac{1}{x(x+1)}$ ，对于 $x = 1, 2, 3, \dots$
>
> 负数部分的和为 0 (因为 $X$ 不取负值)。正数部分的和为：
> $E(X) = \sum_{x=1}^{\infty} x \frac{1}{x(x+1)} = \sum_{x=1}^{\infty} \frac{1}{x+1} = \infty$
>
> 此时 $E(X)$ 存在，但为无穷大。

> [!note] 期望只依赖于分布
> $E(X)$ 被称为 $X$ 的期望，但它**只依赖于 $X$ 的分布**。任何两个具有相同分布的随机变量，即使它们毫不相关，也具有相同的期望。

---

### 连续分布的期望 (Expectation for a Continuous Distribution)

对于连续随机变量，我们用**积分**代替求和，用 **p.d.f. (概率密度函数)** 代替 p.m.f.。

> [!definition] 定义 4.1.3：有界连续随机变量的均值
>
> 设 $X$ 是一个有界的连续随机变量，其 p.d.f. 为 $f$。$X$ 的**期望**定义如下：
> $$
> E(X) = \int_{-\infty}^{\infty} x f(x) dx
> $$

> [!example] 例子 4.1.6：预期故障时间
>
> 一个电器最长寿命为 1 年。其故障时间 $X$ 是一个连续随机变量，p.d.f. 为：
> $f(x) = 2x$ (对于 $0 < x < 1$)
>
> 那么 $X$ 的期望（平均故障时间）为：
> $$
> E(X) = \int_{0}^{1} x (2x) dx = \int_{0}^{1} 2x^2 dx = \left[ \frac{2}{3}x^3 \right]_{0}^{1} = \frac{2}{3}
> $$
> 我们可以说这个分布的期望是 $2/3$ 年。

---
#### 处理无穷积分

> [!definition] 定义 4.1.4：一般连续随机变量的均值
>
> 设 $X$ 是一个连续随机变量，其 p.d.f. 为 $f$。假设下面**至少一个**积分是有限的：
> $$
> \int_{0}^{\infty} x f(x) dx \quad , \quad \int_{-\infty}^{0} x f(x) dx
> $$
> 那么 $X$ 的均值（期望）**存在**，并定义为：
> $$
> E(X) = \int_{-\infty}^{\infty} x f(x) dx
> $$
>
> > [!warning] 期望不存在的情况
> > 如果上述两个积分（正半轴和负半轴）**都是无穷大**，那么 $E(X)$ **不存在**。这和离散情况是完全对应的。

> [!example] 例子 4.1.7：保修期后的故障
>
> 一个产品的保修期为 1 年。其故障时间 $X$ 的 p.d.f. 为：
> $f(x) = \frac{2}{x^3}$ (对于 $x \ge 1$)
>
> 预期故障时间为：
> $$
> E(X) = \int_{1}^{\infty} x \left( \frac{2}{x^3} \right) dx = \int_{1}^{\infty} \frac{2}{x^2} dx = \left[ -\frac{2}{x} \right]_{1}^{\infty} = 0 - (-2) = 2
> $$
> 预期故障时间为 2 年。

> [!danger] 经典陷阱：柯西分布 (Cauchy Distribution)
>
> **例子 4.1.8：一个均值不存在的分布**
>
> 设 $X$ 有一个连续分布，其 p.d.f. 如下（这被称为**柯西分布**）：
> $$
> f(x) = \frac{1}{\pi (1+x^2)} \quad \text{for } -\infty < x < \infty
> $$
> 我们来检查它的均值是否存在：
>
> 1.  **正半轴积分**：
>     $\int_{0}^{\infty} x f(x) dx = \int_{0}^{\infty} \frac{x}{\pi (1+x^2)} dx = \frac{1}{2\pi} \left[ \ln(1+x^2) \right]_{0}^{\infty} = \infty$
>
> 2.  **负半轴积分**：
>     $\int_{-\infty}^{0} x f(x) dx = \int_{-\infty}^{0} \frac{x}{\pi (1+x^2)} dx = \frac{1}{2\pi} \left[ \ln(1+x^2) \right]_{-\infty}^{0} = -\infty$
>
> 因为正负两部分的积分都是无穷大，所以柯西分布的均值 $E(X)$ **不存在**！

---

### 期望的解释 (Interpretation of the Expectation)

> [!info] 核心直觉：期望 = 重心 (Center of Gravity)
>
> 1.  **离散情况**：想象一条没有重量的杆（x轴），在每个 $x_j$ 点挂上 $f(x_j)$ 重量的物体。那么 $E(X)$ 就是这根杆的**平衡点（重心）**。
> 2.  **连续情况**：想象一根杆（x轴），其在 $x$ 点的“线密度”为 $f(x)$。那么 $E(X)$ 也是这根杆的**重心**。
>
> 这个“重心”的直觉非常重要。它意味着，即使在离重心很远的地方只放一点点“概率质量”（$f(x)$），也可能会极大地移动重心（$E(X)$）。

#### 对称性与均值

如果一个分布的 p.f. 或 p.d.f. $f$ 关于某点 $x_0$ **对称**（即 $f(x_0 + \delta) = f(x_0 - \delta)$），**并且**它的均值 $E(X)$ **存在**，那么根据重心的直觉，均值必定是：
$$
E(X) = x_0
$$

> [!warning] 例子 4.1.9：再看柯西分布
>
> 柯西分布 $f(x) = \frac{1}{\pi (1+x^2)}$ 显然是关于 $x=0$ 对称的。
>
> * **错误的想法**：因为它对称于 0，所以 $E(X) = 0$。
> * **正确的想法**：我们必须**先检查均值是否存在**！在例子 4.1.8 中，我们已经证明了柯西分布的均值**不存在**。因此，尽管它对称于 0，但它的均值不是 0（因为它根本没有均值）。

---

### 函数的期望 (The Expectation of a Function)

如果我们知道 $X$ 的分布，我们如何计算 $Y = r(X)$ 的期望 $E(Y)$ 呢？

* **方法一（繁琐的方式）**：
    1.  利用第 3 章的方法，先求出 $Y = r(X)$ 的新 p.d.f. $g(y)$。
    2.  再根据 $g(y)$ 的定义计算 $E(Y) = \int_{-\infty}^{\infty} y g(y) dy$。

* **方法二（简单的方式）**：使用下面的定理。

> [!theorem] 定理 4.1.1：无意识统计师定律 (Law of the Unconscious Statistician - LOTUS)
>
> 设 $X$ 是一个随机变量，$r$ 是一个实值函数。
>
> 1.  **如果 $X$ 是连续的** (p.d.f. 为 $f$)：
>     $$
>     E[r(X)] = \int_{-\infty}^{\infty} r(x) f(x) dx
>     $$
>
> 2.  **如果 $X$ 是离散的** (p.f. 为 $f$)：
>     $$
>     E[r(X)] = \sum_{\text{所有 } x} r(x) f(x)
>     $$
>
> **(前提是期望存在)**
>
> > [!success] 为什么这个定理很重要？
> > 它允许我们**直接使用 $X$ 的分布 $f(x)$** 来计算 $E[r(X)]$，而**不需要**先去计算 $Y=r(X)$ 的新分布 $g(y)$。

> [!example] 例子 4.1.11 & 4.1.12：故障率
>
> * **问题**：在例 4.1.10 中，假设 $X$ 是年故障率，p.d.f. 为 $f(x) = 3x^2$ ($0<x<1$)。我们想知道电器的预期寿命，即 $E[Y] = E[1/X]$。
> * **方法一（繁琐）**：先求 $Y=1/X$ 的 p.d.f. $g(y)$，得到 $g(y) = 3y^{-4}$ ($y>1$)。然后计算 $E(Y) = \int_{1}^{\infty} y (3y^{-4}) dy = \frac{3}{2}$。
> * **方法二（LOTUS）**：直接使用 $f(x)$。
>     $$
>     E[1/X] = \int_{0}^{1} \left( \frac{1}{x} \right) f(x) dx = \int_{0}^{1} \frac{1}{x} (3x^2) dx = \int_{0}^{1} 3x dx = \left[ \frac{3}{2}x^2 \right]_{0}^{1} = \frac{3}{2}
>     $$
>     结果一致，但计算简单得多。

> [!example] 例子 4.1.13：计算 $E[X^{1/2}]$
>
> 设 $X$ 的 p.d.f. 如例 4.1.6 所示，$f(x) = 2x$ ($0<x<1$)。我们想计算 $Y=X^{1/2}$ 的期望。
> 使用 LOTUS：
> $$
> E[Y] = E[X^{1/2}] = \int_{0}^{1} x^{1/2} f(x) dx = \int_{0}^{1} x^{1/2} (2x) dx = 2 \int_{0}^{1} x^{3/2} dx = 2 \left[ \frac{2}{5}x^{5/2} \right]_{0}^{1} = \frac{4}{5}
> $$

> [!danger] 核心陷阱： $E[g(X)] \neq g(E(X))$
>
> **在一般情况下，函数的期望不等于期望的函数！**
>
> 在上一个例子 (4.1.13) 中：
> * 我们计算出 $E[X^{1/2}] = 4/5 = 0.8$。
> * 在例 4.1.6 中，我们知道 $E(X) = 2/3$。
> * $g(E(X)) = (E(X))^{1/2} = (2/3)^{1/2} \approx 0.816$。
>
> 显然，$4/5 \neq (2/3)^{1/2}$。
>
> **唯一的例外**是当 $g(x)$ 是**线性函数**时（例如 $g(X) = aX+b$），此时 $E[aX+b] = aE(X)+b$。我们将在 4.2 节看到。

> [!example] 例子 4.1.14：期权定价 (Option Pricing)
>
> 这是一个在金融中非常重要的应用。
>
> * **背景**：A 公司股票现价 \$200。你获得一个**期权**：一年后，你可以**以 \$200 的价格购买**该股票。
> * **情景**：假设一年后股价 $X$ 只有两种可能：$X=260$ (概率 $p$) 或 $X=180$ (概率 $1-p$)。
> * **期权价值 $Y$**：
>     * 如果 $X=260$：你会行使期权，以 \$200 买入，立刻以 \$260 卖出，获利 $Y = \$60$。
>     * 如果 $X=180$：你不会行使期权（没人会花 \$200 买 \$180 的东西），所以 $Y = \$0$。
> * **期权期望价值**：$Y = h(X)$ 是 $X$ 的函数。
>     $E(Y) = 0 \times (1-p) + 60 \times p = 60p$
> * **公允价格（现值）**：假设无风险利率为 4%。期权的现价 $c$ 应该满足 $1.04 \times c = E(Y)$（即现在花 $c$ 元买期权的期望收益，等于把 $c$ 元存银行的收益）。
>     $c = E(Y) / 1.04 = 60p / 1.04 = 57.69p$
> * **如何确定 $p$？**：金融中的“风险中性定价”假设 $E(X) = \text{现价} \times (1 + \text{利率})$。
>     $E(X) = 260p + 180(1-p)$
>     $200 \times 1.04 = 260p + 180(1-p)$
>     $208 = 80p + 180 \implies 80p = 28 \implies p = 0.35$
> * **最终价格**：该期权的风险中性价格为 $c = 57.69 \times 0.35 = \$20.19$。

---

### 多个随机变量函数的期望

> [!theorem] 定理 4.1.2：LOTUS (多变量版)
>
> 设 $X_1, \dots, X_n$ 是 $n$ 个随机变量， $Y = r(X_1, \dots, X_n)$。
>
> 1.  **连续情况** (联合 p.d.f. 为 $f(x_1, \dots, x_n)$)：
>     $$
>     E(Y) = \int \dots \int_{R^n} r(x_1, \dots, x_n) f(x_1, \dots, x_n) dx_1 \dots dx_n
>     $$
>
> 2.  **离散情况** (联合 p.f. 为 $f(x_1, \dots, x_n)$)：
>     $$
>     E(Y) = \sum_{\text{所有 } x_1, \dots, x_n} r(x_1, \dots, x_n) f(x_1, \dots, x_n)
>     $$

> [!example] 例子 4.1.16：两个变量函数的期望
>
> * **问题**：在一个 $0 \le x \le 1, 0 \le y \le 1$ 的正方形 $S$ 内随机选一点 $(X, Y)$。这意味着 $(X, Y)$ 在 $S$ 上服从均匀分布。
> * **P.d.f.**：由于 $S$ 的面积是 1，联合 p.d.f. 为：
>     $f(x, y) = 1$ (如果 $(x,y)$ 在 $S$ 内)，$f(x, y) = 0$ (其他情况)。
> * **计算 $E(X^2 + Y^2)$**：
>     使用 LOTUS：
>     $$
>     E(X^2 + Y^2) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x^2+y^2) f(x,y) dx dy
>     $$
>     $$
>     = \int_{0}^{1} \int_{0}^{1} (x^2+y^2) (1) dx dy = \int_{0}^{1} \left[ \frac{1}{3}x^3 + y^2 x \right]_{x=0}^{x=1} dy
>     $$
>     $$
>     = \int_{0}^{1} \left( \frac{1}{3} + y^2 \right) dy = \left[ \frac{1}{3}y + \frac{1}{3}y^3 \right]_{y=0}^{y=1} = \frac{1}{3} + \frac{1}{3} = \frac{2}{3}
>     $$

---

## 4.2 期望的性质 (Properties of Expectations)

本节介绍一些简化期望计算的有用定理。

### 核心性质：线性和单调性

> [!theorem] 定理 4.2.1：线性函数 (Linear Function)
>
> 如果 $Y = aX + b$，其中 $a$ 和 $b$ 是有限常数，那么：
> $$
> E(Y) = E(aX + b) = aE(X) + b
> $$
>
> **证明 (连续情况)**：
> $E(aX+b) = \int_{-\infty}^{\infty} (ax+b) f(x) dx$ (根据 LOTUS)
> $= a \int_{-\infty}^{\infty} x f(x) dx + b \int_{-\infty}^{\infty} f(x) dx$
> (第一个积分是 $E(X)$，第二个积分是 p.d.f. 的总积分，等于 1)
> $= aE(X) + b$

> [!example] 例子 4.2.1：计算线性函数的期望
>
> 假设 $E(X) = 5$。
> * $E(3X - 5) = 3E(X) - 5 = 3(5) - 5 = 10$
> * $E(-3X + 15) = -3E(X) + 15 = -3(5) + 15 = 0$

> [!definition] 推论 4.2.1：常数的期望
>
> 如果 $Pr(X = c) = 1$ (即 $X$ 是一个常数 $c$)，那么 $E(X) = c$。
> (这可以看作 $a=0, b=c$ 的特殊情况)

> [!theorem] 定理 4.2.2：期望的单调性
>
> 1.  如果 $Pr(X \ge a) = 1$ (即 $X$ 恒大于等于 $a$)，那么 $E(X) \ge a$。
> 2.  如果 $Pr(X \le b) = 1$ (即 $X$ 恒小于等于 $b$)，那么 $E(X) \le b$。
>
> **推论**：如果 $Pr(a \le X \le b) = 1$，那么 $a \le E(X) \le b$。

> [!theorem] 定理 4.2.3
>
> 如果 $E(X) = a$ 并且 $Pr(X \ge a) = 1$，那么 $Pr(X=a) = 1$。
> (如果 $X$ 总是不小于 $a$，且其平均值刚好是 $a$，那么 $X$ 必须永远等于 $a$)。

---

### 和的期望 (Expectation of a Sum)

> [!theorem] 定理 4.2.4：和的期望
>
> 如果 $X_1, \dots, X_n$ 是 $n$ 个随机变量，且每个 $E(X_i)$ 都有限，那么：
> $$
> E(X_1 + \dots + X_n) = E(X_1) + \dots + E(X_n)
> $$
>
> > [!info] 关键点：期望的线性性 (Linearity of Expectation)
> > **和的期望等于期望的和。**
> >
> > 这个定理**极其强大**，因为它**不要求 $X_i$ 之间相互独立**！无论这些变量之间如何相关，这个等式都成立。

> [!definition] 推论 4.2.2：一般线性组合
>
> 结合 4.2.1 和 4.2.4，我们得到期望的**线性性**：
> $$
> E(a_1 X_1 + \dots + a_n X_n + b) = a_1 E(X_1) + \dots + a_n E(X_n) + b
> $$

> [!example] 例子 4.2.4：无放回抽样 (Sampling without Replacement)
>
> * **问题**：一个盒子里有红球和蓝球，红球比例为 $p$。**无放回**地随机抽取 $n$ 个球。设 $X$ 是抽到的红球总数。求 $E(X)$。
> * **繁琐解法**：$X$ 服从超几何分布。我们需要写出其 p.f. 然后求和，非常复杂。
> * **巧妙解法（利用线性性）**：
>     1.  定义 $n$ 个**指示变量 (Indicator Variables)** $X_1, \dots, X_n$：
>         $X_i = 1$ 如果第 $i$ 次抽到的球是红色
>         $X_i = 0$ 如果第 $i$ 次抽到的球是蓝色
>     2.  红球总数 $X$ 就是这些指示变量的和：$X = X_1 + \dots + X_n$。
>     3.  根据期望的线性性：$E(X) = E(X_1) + \dots + E(X_n)$。
>     4.  计算 $E(X_i)$：
>         $E(X_i) = 1 \times Pr(X_i=1) + 0 \times Pr(X_i=0) = Pr(X_i=1)$
>         由于是随机抽样，第 $i$ 个球是红色的概率（在抽之前看）就是 $p$。
>         所以，$E(X_i) = p$ (对于所有的 $i$)
>     5.  **结论**：
>         $$
>         E(X) = p + p + \dots + p = np
>         $$
> > [!note] 注意
> > 尽管 $X_i$ 之间是**相互依赖**的（因为是无放回抽样），但这完全不影响我们使用期望的线性性。这就是这个方法的巧妙之处。

> [!example] 例子 4.2.5：有放回抽样 (Sampling with Replacement)
>
> 同样的问题，但这次是**有放回**抽样。
> $X$ 是 $n$ 次抽样中的红球数，服从二项分布 $Bin(n, p)$。
> 
> 我们可以用同样的方法：
> 1.  $X = X_1 + \dots + X_n$，其中 $X_i$ 是第 $i$ 次试验是否为红球的指示变量。
> 2.  $E(X_i) = p$ (因为每次抽样都是独立的，红球概率始终为 $p$)。
> 3.  $E(X) = \sum E(X_i) = np$。
>
> **结论**：二项分布 $Bin(n, p)$ 的均值是 $np$。

> [!example] 例子 4.2.6：匹配问题 (Matching Problem)
>
> * **问题**：一个人写了 $n$ 封信，写了 $n$ 个信封。他随机地将信装入信封。求 $X$（放对的信的数量）的期望。
> * **解法（指示变量）**：
>     1.  定义 $n$ 个指示变量 $X_1, \dots, X_n$：
>         $X_i = 1$ 如果第 $i$ 封信放对了信封
>         $X_i = 0$ 其他情况
>     2.  总的放对数量 $X = X_1 + \dots + X_n$。
>     3.  $E(X) = E(X_1) + \dots + E(X_n)$。
>     4.  计算 $E(X_i)$：
>         $E(X_i) = Pr(X_i=1)$
>         第 $i$ 封信放对信封的概率是多少？由于是随机放，它有 $1/n$ 的概率被放入正确的信封。
>         所以，$E(X_i) = 1/n$ (对于所有的 $i$)
>     5.  **结论**：
>         $$
>         E(X) = \frac{1}{n} + \frac{1}{n} + \dots + \frac{1}{n} = n \left( \frac{1}{n} \right) = 1
>         $$
>     无论 $n$ 有多大，平均总能放对 1 封信。

---

### 乘积的期望 (Expectation of a Product)

> [!theorem] 定理 4.2.6：独立随机变量乘积的期望
>
> 如果 $X_1, \dots, X_n$ 是 $n$ 个**相互独立**的随机变量，且每个 $E(X_i)$ 都有限，那么：
> $$
> E( \prod_{i=1}^{n} X_i ) = \prod_{i=1}^{n} E(X_i)
> $$
> (乘积的期望等于期望的乘积)
>
> **证明 (n=2 连续情况)**：
> 因为 $X_1, X_2$ 独立，所以联合 p.d.f. $f(x_1, x_2) = f_1(x_1) f_2(x_2)$。
> $E(X_1 X_2) = \int \int (x_1 x_2) f(x_1, x_2) dx_1 dx_2$
> $= \int \int (x_1 x_2) f_1(x_1) f_2(x_2) dx_1 dx_2$
> $= \left( \int x_1 f_1(x_1) dx_1 \right) \left( \int x_2 f_2(x_2) dx_2 \right)$
> $= E(X_1) E(X_2)$

> [!danger] 重点对比：和的期望 vs 积的期望
>
> 1.  **$E(X+Y) = E(X) + E(Y)$**：**永远成立** (只要期望有限)。
> 2.  **$E(XY) = E(X)E(Y)$**：**仅在 $X, Y$ 独立时** (或不相关时) 成立。

> [!example] 例子 4.2.7：计算组合变量的期望
>
> 设 $X_1, X_2, X_3$ 独立，且 $E(X_i) = 0$, $E(X_i^2) = 1$。
> 计算 $E[X_1^2 (X_2 - 4X_3)^2]$。
>
> 1.  因为 $X_1, X_2, X_3$ 独立，所以 $X_1^2$ 和 $(X_2 - 4X_3)^2$ 也独立。
> 2.  $E[X_1^2 (X_2 - 4X_3)^2] = E(X_1^2) \times E[(X_2 - 4X_3)^2]$
> 3.  $E(X_1^2) = 1$ (已知)。
> 4.  $E[(X_2 - 4X_3)^2] = E[X_2^2 - 8X_2 X_3 + 16X_3^2]$
> 5.  利用期望的线性性：
>     $= E(X_2^2) - 8 E(X_2 X_3) + 16 E(X_3^2)$
> 6.  $E(X_2^2) = 1$, $E(X_3^2) = 1$ (已知)。
> 7.  因为 $X_2, X_3$ 独立，所以 $E(X_2 X_3) = E(X_2) E(X_3) = 0 \times 0 = 0$。
> 8.  $E[(X_2 - 4X_3)^2] = 1 - 8(0) + 16(1) = 17$。
> 9.  **最终答案**：$1 \times 17 = 17$。

---

### 非负随机变量的期望

> [!theorem] 定理 4.2.7：(非负) 整数值随机变量
>
> 设 $X$ 是一个只能取 $0, 1, 2, \dots$ 值的随机变量。那么：
> $$
> E(X) = \sum_{n=1}^{\infty} Pr(X \ge n)
> $$
>
> **通俗解释**：
> $E(X) = 0 \cdot Pr(X=0) + 1 \cdot Pr(X=1) + 2 \cdot Pr(X=2) + 3 \cdot Pr(X=3) + \dots$
> $= (Pr(X=1) + Pr(X=2) + Pr(X=3) + \dots)$  (1 个 $Pr(X=1)$)
> $+ (Pr(X=2) + Pr(X=3) + \dots)$  (1 个 $Pr(X=2)$)
> $+ (Pr(X=3) + \dots)$  (1 个 $Pr(X=3)$)
> $\dots$
>
> 把它们按行相加：
> $Pr(X \ge 1) + Pr(X \ge 2) + Pr(X \ge 3) + \dots = \sum_{n=1}^{\infty} Pr(X \ge n)$

> [!example] 例子 4.2.9：预期试验次数
>
> * **问题**：一个人重复尝试一项任务，每次试验成功的概率为 $p$ (0 < p < 1)，试验间相互独立。求 $X$ (首次成功在第几次试验) 的期望。
> * **解法**：$X$ 是一个非负整数值变量。
>     * $Pr(X \ge 1) = 1$ (至少需要 1 次试验)
>     * $Pr(X \ge 2)$ (至少需要 2 次) = (第 1 次失败) = $1-p$
>     * $Pr(X \ge 3)$ (至少需要 3 次) = (前 2 次都失败) = $(1-p)^2$
>     * $Pr(X \ge n)$ (至少需要 n 次) = (前 n-1 次都失败) = $(1-p)^{n-1}$
> * **应用定理 4.2.7**：
>     $E(X) = \sum_{n=1}^{\infty} Pr(X \ge n) = \sum_{n=1}^{\infty} (1-p)^{n-1}$
>     这是一个公比为 $(1-p)$ 的等比数列求和：
>     $E(X) = 1 + (1-p) + (1-p)^2 + \dots = \frac{1}{1 - (1-p)} = \frac{1}{p}$
>
> > [!success] 重要结论
> > **几何分布 (Geometric Distribution) 的均值是 $1/p$**。
> > (如果成功率是 $1/10$，平均需要 10 次试验才能成功，这很直观。)

> [!theorem] 定理 4.2.8：一般非负随机变量
>
> 设 $X$ 是一个非负随机变量 ($Pr(X \ge 0) = 1$)，其 c.d.f. (累积分布函数) 为 $F$。那么：
> $$
> E(X) = \int_{0}^{\infty} [1 - F(x)] dx
> $$
>
> (注： $1 - F(x) = Pr(X > x)$。这是定理 4.2.7 的连续版本。)

> [!example] 例子 4.2.10：预期等待时间
>
> 设 $X$ 是顾客的等待时间，其 c.d.f. 为：
> $F(x) = 1 - e^{-2x}$ (对于 $x > 0$)  (这叫指数分布)
>
> 那么 $1 - F(x) = e^{-2x}$。
>
> 根据定理 4.2.8：
> $$
> E(X) = \int_{0}^{\infty} [1 - F(x)] dx = \int_{0}^{\infty} e^{-2x} dx = \left[ -\frac{1}{2} e^{-2x} \right]_{0}^{\infty} = 0 - (-\frac{1}{2}) = \frac{1}{2}
> $$

---

### 凸函数与詹森不等式

> [!warning] 再次提醒
> 除非 $g$ 是线性函数，否则 $E[g(X)] \neq g(E(X))$。

> [!definition] 定义 4.2.1：凸函数 (Convex Function)
>
> 一个函数 $g$ 被称为**凸函数**，如果对于任意 $x, y$ 和 $0 < \alpha < 1$：
> $$
> g[\alpha x + (1-\alpha)y] \le \alpha g(x) + (1-\alpha) g(y)
> $$
> (通俗地说，函数上任意两点之间的弦，总是在这两点之间的函数图像的上方。例如 $g(x) = x^2$ 是凸函数。)

> [!theorem] 定理 4.2.5：詹森不等式 (Jensen's Inequality)
>
> 设 $g$ 是一个**凸函数**， $X$ 是一个随机向量。那么：
> $$
> E[g(X)] \ge g(E(X))
> $$
>
> * **例如**：因为 $g(x) = x^2$ 是凸函数，所以 $E[X^2] \ge [E(X)]^2$。
> * **例如**：因为 $g(x) = 1/x$ 在 $x>0$ 时是凸函数，所以 $E[1/X] \ge 1/E(X)$。

---

## 4.3 方差 (Variance)

均值 $E(X)$ 告诉我们分布的“中心”在哪里，但它没有告诉我们分布的**离散程度**（spread out）。

> [!example] 例子 4.3.1：股价变化
>
> 假设两支股票 A 和 B 一个月后的价格：
> * A 服从 [25, 35] 上的均匀分布。
> * B 服从 [15, 45] 上的均匀分布。
>
> 它们有**相同的均值**：$E(A) = (25+35)/2 = 30$，$E(B) = (15+45)/2 = 30$。
>
> 但是它们的风险完全不同：
> * A 的价格保证在 [25, 35] 区间内。
> * B 的价格波动范围 [15, 45] 要大得多。B 有 $1/3$ 的概率 $Pr(B < 25)$，但也有更高的上行潜力。
>
> 我们需要一个度量来描述这种“离散程度”。

### 方差和标准差的定义

> [!definition] 定义 4.3.1：方差 (Variance) / 标准差 (Standard Deviation)
>
> 设 $X$ 是一个均值为 $\mu = E(X)$ 的随机变量。
>
> 1.  **方差 (Variance)**，记为 $Var(X)$ 或 $\sigma^2$，定义为 $X$ 偏离其均值的**平方的期望**：
>     $$
>     Var(X) = E[ (X - \mu)^2 ]
>     $$
>
> 2.  **标准差 (Standard Deviation)**，记为 $\sigma$，定义为方差的非负平方根：
>     $$
>     \sigma = \sqrt{Var(X)}
>     $$
>
> > [!note] 为什么用平方？
> > * 如果我们只计算 $E[X - \mu]$，它永远等于 $E(X) - E(\mu) = \mu - \mu = 0$，这没有意义。
> > * 平方 $(X - \mu)^2$ 使得所有偏离（无论是正还是负）都变成正数。
> > * 标准差 $\sigma$ 的好处是，它的单位和 $X$ 本身（以及均值 $\mu$）的单位相同。

> [!example] 例子 4.3.2：计算股票方差
>
> 对于例 4.3.1 中的两支股票（均值 $\mu = 30$）：
>
> * **股票 A** (均匀分布 p.d.f. $f(a) = 1/10$, $a \in [25, 35]$)：
>     $Var(A) = E[(A - 30)^2] = \int_{25}^{35} (a - 30)^2 \frac{1}{10} da$
>     (令 $x = a-30$)
>     $= \frac{1}{10} \int_{-5}^{5} x^2 dx = \frac{1}{10} \left[ \frac{x^3}{3} \right]_{-5}^{5} = \frac{1}{30} (125 - (-125)) = \frac{250}{30} = \frac{25}{3}$
>     $\sigma_A = \sqrt{25/3} \approx 2.87$
>
> * **股票 B** (均匀分布 p.d.f. $f(b) = 1/30$, $b \in [15, 45]$)：
>     $Var(B) = E[(B - 30)^2] = \int_{15}^{45} (b - 30)^2 \frac{1}{30} db$
>     (令 $y = b-30$)
>     $= \frac{1}{30} \int_{-15}^{15} y^2 dy = \frac{1}{30} \left[ \frac{y^3}{3} \right]_{-15}^{15} = \frac{1}{90} (3375 - (-3375)) = \frac{6750}{90} = 75$
>     $\sigma_B = \sqrt{75} \approx 8.66$
>
> **结论**：$Var(B)$ 是 $Var(A)$ 的 9 倍，标准差 $\sigma_B$ 是 $\sigma_A$ 的 3 倍，这精确地度量了 B 的波动性远大于 A。

---

### 方差的计算

直接使用 $E[(X - \mu)^2]$ 的定义来计算方差通常很麻烦。

> [!success] 定理 4.3.1：方差的常用计算公式
>
> 对于任意随机变量 $X$：
> $$
> Var(X) = E(X^2) - [E(X)]^2
> $$
>
> **通俗地说：“方差 = 平方的期望 - 期望的平方”**
>
> **证明**：
> 设 $\mu = E(X)$。
> $Var(X) = E[(X - \mu)^2]$
> $= E[X^2 - 2\mu X + \mu^2]$
> (利用期望的线性性 $E(A+B+C) = E(A)+E(B)+E(C)$)
> $= E(X^2) - E(2\mu X) + E(\mu^2)$
> (利用 $E(aY) = aE(Y)$，其中 $2\mu$ 和 $\mu^2$ 都是常数)
> $= E(X^2) - 2\mu E(X) + \mu^2$
> (代入 $\mu = E(X)$)
> $= E(X^2) - 2(E(X)) E(X) + (E(X))^2$
> $= E(X^2) - 2[E(X)]^2 + [E(X)]^2$
> $= E(X^2) - [E(X)]^2$

> [!example] 例子 4.3.3 & 4.3.4：计算离散方差
>
> * **问题**：$X$ 等概率取 -2, 0, 1, 3, 4 五个值 (概率 $1/5$)。
> * **均值**：$E(X) = \frac{1}{5}(-2 + 0 + 1 + 3 + 4) = 1.2$
> * **方法一（按定义）**：
>     $Var(X) = E[(X - 1.2)^2]$
>     $= \frac{1}{5}[(-2-1.2)^2 + (0-1.2)^2 + (1-1.2)^2 + (3-1.2)^2 + (4-1.2)^2]$
>     $= \frac{1}{5}[10.24 + 1.44 + 0.04 + 3.24 + 7.84] = \frac{22.8}{5} = 4.56$
> * **方法二（用公式）**：
>     1.  先求 $E(X^2)$ (平方的期望)：
>         $E(X^2) = \frac{1}{5}[(-2)^2 + 0^2 + 1^2 + 3^2 + 4^2]$
>         $= \frac{1}{5}[4 + 0 + 1 + 9 + 16] = \frac{30}{5} = 6$
>     2.  再求 $[E(X)]^2$ (期望的平方)：
>         $[E(X)]^2 = (1.2)^2 = 1.44$
>     3.  计算方差：
>         $Var(X) = E(X^2) - [E(X)]^2 = 6 - 1.44 = 4.56$
>     **结论**：两种方法结果一致，但方法二（计算公式）通常更简单。

> [!warning] 例子 4.3.5：均值和方差对“离群值”的敏感性
>
> * **分布 Y**：伯努利分布，$Pr(Y=0)=0.5, Pr(Y=1)=0.5$
>     * $E(Y) = 0.5$
>     * $E(Y^2) = 0.5 \times 0^2 + 0.5 \times 1^2 = 0.5$
>     * $Var(Y) = E(Y^2) - [E(Y)]^2 = 0.5 - (0.5)^2 = 0.25$
> * **分布 X**：对 Y 做一点点修改：
>     * $f(x=0) = 0.5$
>     * $f(x=1) = 0.499$
>     * $f(x=10000) = 0.001$ (只移动了 0.001 的概率质量)
> * **计算 X 的均值和方差**：
>     * $E(X) = (0.5)(0) + (0.499)(1) + (0.001)(10000) = 0.499 + 10 = 10.499$
>     * $E(X^2) = (0.5)(0^2) + (0.499)(1^2) + (0.001)(10000^2) = 0.499 + 100000 = 100000.499$
>     * $Var(X) = E(X^2) - [E(X)]^2 = 100000.499 - (10.499)^2 \approx 99890.27$
>
> **结论**：仅仅 0.1% 的概率质量被移动到一个很远的值 (10000)，就导致均值和方差发生**巨大**变化。这说明均值和方差**对离群值（outliers）非常敏感**。

---

### 方差的性质

> [!theorem] 定理 4.3.2
>
> 1.  $Var(X) \ge 0$ (方差永远是非负的)。
> 2.  如果 $X$ 是有界的，那么 $Var(X)$ 必定存在且有限。

> [!theorem] 定理 4.3.3
>
> $Var(X) = 0$ **当且仅当** 存在一个常数 $c$ 使得 $Pr(X=c) = 1$。
>
> **解释**：方差为 0 意味着“没有波动”，这当且仅当这个“随机”变量实际上是一个恒定的常数。
>
> **证明**：
> $Var(X) = E[(X-\mu)^2] = 0$。
> $(X-\mu)^2$ 是一个非负变量，它的期望为 0。
> 根据定理 4.2.3，这一定意味着 $Pr((X-\mu)^2 = 0) = 1$。
> 这等价于 $Pr(X - \mu = 0) = 1$，即 $Pr(X = \mu) = 1$。
> (取 $c = \mu$ 即可)

---

### 4.3 方差 (Variance) - 续

#### 方差的性质 (续)

> [!theorem] 定理 4.3.4：线性函数的方差
>
> 设 $Y = aX + b$，其中 $a$ 和 $b$ 是有限常数。那么：
> $$
> Var(Y) = a^2 Var(X)
> $$
> $$
> \sigma_Y = |a| \sigma_X
> $$
>
> **证明**：
> 1.  设 $E(X) = \mu$。根据定理 4.2.1， $E(Y) = E(aX+b) = a\mu + b$。
> 2.  根据方差的定义：
>     $Var(Y) = E[ (Y - E(Y))^2 ]$
>     $= E[ ( (aX+b) - (a\mu+b) )^2 ]$
>     $= E[ (aX - a\mu)^2 ]$
>     $= E[ a^2 (X - \mu)^2 ]$
> 3.  因为 $a^2$ 是常数，可以提出来：
>     $= a^2 E[ (X - \mu)^2 ]$
>     $= a^2 Var(X)$
> 4.  两边开根号，得到 $\sigma_Y = \sqrt{a^2 Var(X)} = |a| \sqrt{Var(X)} = |a| \sigma_X$。
>
> > [!info] 重要推论
> >
> > 1.  **$Var(X + b) = Var(X)$** (取 $a=1$)
> >     **直觉**：将整个分布平移 $b$ 个单位，均值也会平移 $b$，但分布的“离散程度”或“宽度”**不变**。
> >
> > 2.  **$Var(-X) = Var(X)$** (取 $a=-1, b=0$)
> >     **直觉**：将分布关于原点进行镜像反射，均值变为 $-\mu$，但分布的“离散程度”**不变**。

> [!example] 例子 4.3.6：计算线性函数的方差
>
> * **问题**：$X$ 等概率取 -2, 0, 1, 3, 4 五个值。在例 4.3.3 中我们算得 $Var(X) = 4.56$。求 $Y = 4X - 7$ 的方差和标准差。
> * **解法**：
>     * $Var(Y) = Var(4X - 7)$
>     * 根据定理 4.3.4 (a=4, b=-7)：
>     * $Var(Y) = 4^2 Var(X) = 16 \times Var(X) = 16 \times (4.56) = 72.96$
>     * $\sigma_Y = |4| \sigma_X = 4 \times \sqrt{4.56} \approx 4 \times 2.135 = 8.54$

---

### 和的方差

> [!theorem] 定理 4.3.5：独立随机变量的和的方差
>
> 如果 $X_1, \dots, X_n$ 是 $n$ 个**相互独立**的随机变量，且方差都有限，那么：
> $$
> Var(X_1 + \dots + X_n) = Var(X_1) + \dots + Var(X_n)
> $$
>
> > [!warning] 警告：独立性至关重要
> > $E(X+Y) = E(X) + E(Y)$ **总是**成立。
> > $Var(X+Y) = Var(X) + Var(Y)$ **仅在 $X, Y$ 独立时** (或不相关时) 成立。
>
> **证明 (n=2 的情况)**：
> 1.  设 $E(X_1) = \mu_1$，$E(X_2) = \mu_2$。
> 2.  $E(X_1 + X_2) = \mu_1 + \mu_2$。
> 3.  根据方差定义：
>     $Var(X_1 + X_2) = E[ ( (X_1 + X_2) - (\mu_1 + \mu_2) )^2 ]$
>     $= E[ ( (X_1 - \mu_1) + (X_2 - \mu_2) )^2 ]$
> 4.  展开平方项：
>     $= E[ (X_1 - \mu_1)^2 + (X_2 - \mu_2)^2 + 2(X_1 - \mu_1)(X_2 - \mu_2) ]$
> 5.  利用期望的线性性：
>     $= E[(X_1 - \mu_1)^2] + E[(X_2 - \mu_2)^2] + 2 E[(X_1 - \mu_1)(X_2 - \mu_2)]$
>     $= Var(X_1) + Var(X_2) + 2 E[(X_1 - \mu_1)(X_2 - \mu_2)]$
> 6.  **关键步骤**：因为 $X_1$ 和 $X_2$ **独立**，所以 $(X_1 - \mu_1)$ 和 $(X_2 - \mu_2)$ 也独立。
>     根据定理 4.2.6 (独立变量乘积的期望等于期望的乘积)：
>     $E[(X_1 - \mu_1)(X_2 - \mu_2)] = E(X_1 - \mu_1) \times E(X_2 - \mu_2)$
>     $= (\mu_1 - \mu_1) \times (\mu_2 - \mu_2) = 0 \times 0 = 0$
> 7.  因此，交叉项消失了，只剩下：
>     $Var(X_1 + X_2) = Var(X_1) + Var(X_2)$

> [!definition] 推论 4.3.1：独立变量线性组合的方差
>
> 结合 4.3.4 和 4.3.5：
> 如果 $X_1, \dots, X_n$ **相互独立**，那么：
> $$
> Var(a_1 X_1 + \dots + a_n X_n + b) = a_1^2 Var(X_1) + \dots + a_n^2 Var(X_n)
> $$
>
> **注意**：
> 8.  常数 $b$ 消失了 (因为它只影响平移)。
> 9.  系数 $a_i$ 变成了**平方** $a_i^2$。

> [!example] 例子 4.3.7：投资组合 (Investment Portfolio)
>
> * **问题**：一个投资者有 \$100,000。
>     * 股票 1 ($R_1$)：\$60/股，均值 $E(R_1)=6$，方差 $Var(R_1)=55$。
>     * 股票 2 ($R_2$)：\$48/股，均值 $E(R_2)=4$，方差 $Var(R_2)=28$。
>     * 固投 ($s_3$)：固定利率 3.6%。
> * **假设**：$R_1$ 和 $R_2$ **相互独立**。
> * **组合**：$s_1$ 股股票 1，$s_2$ 股股票 2，以及 $s_3$ 美元的固投。
>     * 约束条件：$60s_1 + 48s_2 + s_3 = 100,000$。
> * **组合总收益 (Return)**：$Y = s_1 R_1 + s_2 R_2 + 0.036 s_3$。
> * **组合的均值** (利用期望的线性性)：
>     $E(Y) = s_1 E(R_1) + s_2 E(R_2) + 0.036 s_3 = 6s_1 + 4s_2 + 0.036 s_3$
> * **组合的方差** (利用推论 4.3.1，注意 $0.036 s_3$ 是常数，方差为0)：
>     $Var(Y) = s_1^2 Var(R_1) + s_2^2 Var(R_2) = 55 s_1^2 + 28 s_2^2$
> * **有效投资组合 (Efficient Portfolios)**：
>     在金融学中，投资者希望**均值 $E(Y)$ 越大越好**，同时**方差 $Var(Y)$ (风险) 越小越好**。
>     * **有效组合**：对于给定的均值，方差最小的组合；或者对于给定的方差，均值最大的组合。
>     * 如图 4.7 所示，投资者会在“有效前沿”（图中的下边界线）上选择一个组合。

---

### 二项分布的方差

> [!success] 二项分布 $Bin(n, p)$ 的方差
>
> 我们再次使用指示变量的技巧。
> $X$ 是 $n$ 次独立试验中成功的次数。
> $X = X_1 + \dots + X_n$
> 其中 $X_i$ 是第 $i$ 次试验的指示变量（成功=1，失败=0），服从伯努利分布。
>
> 1.  **$X_i$ 之间相互独立** (因为是有放回抽样或独立试验)。
> 2.  根据定理 4.3.5：$Var(X) = \sum_{i=1}^{n} Var(X_i)$。
> 3.  **计算 $Var(X_i)$**：
>     * $E(X_i) = p$ (在例 4.1.3 中已算)
>     * $E(X_i^2)$：因为 $X_i$ 只能取 0 或 1，所以 $X_i^2 = X_i$。
>         $E(X_i^2) = E(X_i) = p$
>     * 使用方差公式 $Var(Y) = E(Y^2) - [E(Y)]^2$：
>         $Var(X_i) = p - p^2 = p(1-p)$
> 4.  **计算 $Var(X)$**：
>     $Var(X) = Var(X_1) + \dots + Var(X_n)$
>     $= p(1-p) + \dots + p(1-p)$ (共 n 项)
>     $$
>     Var(X) = np(1-p)
>     $$
>
> > [!info] 总结：二项分布 $Bin(n, p)$
> > * **均值**：$E(X) = np$
> > * **方差**：$Var(X) = np(1-p)$

---

### 四分位距 (Interquartile Range - IQR)

> [!example] 例子 4.3.8：柯西分布的挑战
>
> 柯西分布的均值不存在 (例 4.1.8)，因此其**方差也不存在**。
>
> 但我们仍然希望有一个度量来描述它的“离散程度”。例如，$Y=2X$ 应该比 $X$ “离散两倍”。方差无法做到这一点。

> [!definition] 定义 4.3.2：四分位距 (IQR)
>
> 设 $X$ 是一个随机变量，其分位数函数为 $F^{-1}(p)$。
> **IQR** 定义为：
> $$
> \text{IQR} = F^{-1}(0.75) - F^{-1}(0.25)
> $$
>
> **通俗解释**：IQR 是包含分布“中间 50%”概率的区间的长度。
>
> **优点**：IQR 对**所有分布**都存在，并且对离群值不敏感 (稳健，Robust)。

> [!example] 例子 4.3.9：柯西分布的 IQR
>
> * **c.d.f.**：$F(x) = \frac{1}{2} + \frac{\tan^{-1}(x)}{\pi}$
> * **分位数函数**：$F^{-1}(p) = \tan[\pi(p - 1/2)]$
> * **计算 IQR**：
>     * $F^{-1}(0.75) = \tan[\pi(0.75 - 0.5)] = \tan(\pi/4) = 1$
>     * $F^{-1}(0.25) = \tan[\pi(0.25 - 0.5)] = \tan(-\pi/4) = -1$
>     * $\text{IQR} = 1 - (-1) = 2$
> * 如果 $Y = 2X$，可以证明 $Y$ 的 IQR 是 4。

---

## 4.4 矩 (Moments)

### 矩的存在性

> [!definition] 矩 (Moments)
>
> 1.  **$k$ 阶矩** (kth moment)：$E(X^k)$。
>     * (1 阶矩就是均值 $E(X)$)
> 2.  **$k$ 阶中心矩** (kth central moment)：$E[ (X - \mu)^k ]$，其中 $\mu = E(X)$。
>     * (1 阶中心矩 $E[X-\mu] = 0$)
>     * (2 阶中心矩 $E[(X-\mu)^2]$ 就是方差 $Var(X)$)
>
> 我们说 $k$ 阶矩**存在**，当且仅当 $E(|X|^k) < \infty$。

> [!theorem] 定理 4.4.1：高阶矩的存在性
>
> 如果 $k$ 阶矩存在 ($E(|X|^k) < \infty$)，那么所有低于 $k$ 阶的 $j$ 阶矩 ( $j < k$ ) 也必定存在 ($E(|X|^j) < \infty$)。
>
> **证明 (连续情况)**：
> $E(|X|^j) = \int_{-\infty}^{\infty} |x|^j f(x) dx$
> $= \int_{|x| \le 1} |x|^j f(x) dx + \int_{|x| > 1} |x|^j f(x) dx$
>
> 1.  在 $|x| \le 1$ 的积分域上，$|x|^j \le 1$。
> 2.  在 $|x| > 1$ 的积分域上，因为 $j < k$，所以 $|x|^j < |x|^k$。
>
> $\le \int_{|x| \le 1} 1 \cdot f(x) dx + \int_{|x| > 1} |x|^k f(x) dx$
> $\le \int_{-\infty}^{\infty} f(x) dx + \int_{-\infty}^{\infty} |x|^k f(x) dx$
> $\le 1 + E(|X|^k)$
>
> 因为 $E(|X|^k)$ 是有限的，所以 $E(|X|^j)$ 也是有限的。
>
> **推论**：如果 2 阶矩 $E(X^2)$ 存在，那么均值 $E(X)$ 必定存在 (因此方差也存在)。

#### 中心矩与对称性

* **1 阶中心矩**：$E(X-\mu) = E(X) - \mu = \mu - \mu = 0$。
* **对称分布**：如果一个分布关于其均值 $\mu$ 对称，并且其 $k$ 阶中心矩存在（$k$ 为奇数），那么这个 $k$ 阶中心矩必定为 0。
    * $E[(X-\mu)^k] = 0$ (对于奇数 $k$)
    * 这是因为在积分/求和中， $(x-\mu)^k$ 的正负项会完全抵消。

### 偏度 (Skewness)

> [!definition] 定义 4.4.1：偏度 (Skewness)
>
> 设 $X$ 的均值为 $\mu$，标准差为 $\sigma$，且 3 阶矩存在。
> **偏度**定义为标准化的 3 阶中心矩：
> $$
> \text{Skewness} = \frac{E[(X-\mu)^3]}{\sigma^3}
> $$
>
> * **目的**：偏度是衡量分布**不对称性**的指标。
> * **标准化** (除以 $\sigma^3$) 是为了消除单位和离散程度的影响，只测量“形状”。
> * **对称分布** (如正态分布、柯西分布) 的偏度为 0。
> * **右偏/正偏** (long tail to the right) $\implies$ 偏度 > 0。
> * **左偏/负偏** (long tail to the left) $\implies$ 偏度 < 0。

> [!example] 例子 4.4.2：二项分布的偏度
>
> $X \sim Bin(10, 0.25)$。
> * $\mu = np = 10 \times 0.25 = 2.5$
> * $\sigma = \sqrt{np(1-p)} = \sqrt{10 \times 0.25 \times 0.75} \approx 1.369$
> * 计算 $E[(X-2.5)^3] = \sum_{x=0}^{10} (x-2.5)^3 \binom{10}{x} (0.25)^x (0.75)^{10-x} \approx 0.9375$
> * $\text{Skewness} = \frac{0.9375}{1.369^3} \approx 0.3652$
>
> 这是一个正值，说明该分布是**右偏**的（如图 4.8 所示）。
> * 如果 $p=0.5$，分布对称，偏度为 0。
> * $Bin(n, p)$ 的偏度 与 $Bin(n, 1-p)$ 的偏度互为相反数。

---

### 矩生成函数 (Moment Generating Function - MGF)

MGF 是一种描述分布的替代方法，它与矩密切相关，并且在处理**独立变量之和**时非常有用。

> [!definition] 定义 4.4.2：矩生成函数 (MGF)
>
> 设 $X$ 是一个随机变量。对任意实数 $t$，定义：
> $$
> \psi(t) = E[e^{tX}]
> $$
>
> * **离散**：$\psi(t) = \sum_x e^{tx} f(x)$
> * **连续**：$\psi(t) = \int_{-\infty}^{\infty} e^{tx} f(x) dx$
>
> 这个函数 $\psi(t)$ 被称为 $X$ 的**矩生成函数 (m.g.f.)**。
>
> > [!note] MGF 的基本性质
> > * MGF 只依赖于 $X$ 的分布。
> > * MGF **不一定**对所有的 $t$ 都存在 (即期望可能是无穷大)。
> > * 但是，它**总是在 $t=0$ 时存在**：$\psi(0) = E[e^{0 \cdot X}] = E[1] = 1$。
> > * 我们只关心 MGF 在 $t=0$ 附近的一个小开区间内 $( -h, h )$ 是否存在。

#### MGF 如何“生成”矩？

> [!theorem] 定理 4.4.2：MGF 与矩
>
> 如果 $X$ 的 MGF $\psi(t)$ 在 $t=0$ 附近的一个开区间内存在（有限），那么 $X$ 的所有阶矩 ($E(X^n)$) 都存在。
> 并且， $X$ 的 $n$ 阶矩可以通过对 $\psi(t)$ **求导 $n$ 次**，然后令 $t=0$ 得到：
> $$
> E(X^n) = \psi^{(n)}(0)
> $$
>
> **为什么？(非严格证明)**
> 1.  对 $\psi(t) = E[e^{tX}]$ 关于 $t$ 求导 (假设求导和期望可以互换)：
>     $\psi'(t) = \frac{d}{dt} E[e^{tX}] = E[\frac{d}{dt} e^{tX}] = E[X e^{tX}]$
> 2.  令 $t=0$：
>     $\psi'(0) = E[X e^0] = E[X]$ (1 阶矩)
> 3.  再求导一次：
>     $\psi''(t) = \frac{d}{dt} E[X e^{tX}] = E[\frac{d}{dt} X e^{tX}] = E[X^2 e^{tX}]$
> 4.  令 $t=0$：
>     $\psi''(0) = E[X^2 e^0] = E[X^2]$ (2 阶矩)
> 5.  以此类推... $\psi^{(n)}(0) = E[X^n]$ ($n$ 阶矩)

> [!example] 例子 4.4.3：指数分布 (Exponential Distribution)
>
> * **问题**：设 $X$ 的 p.d.f. 为 $f(x) = e^{-x}$ (对于 $x > 0$)。求 $X$ 的 MGF，并用它计算 $E(X)$ 和 $Var(X)$。
> * **1. 求 MGF**：
>     $\psi(t) = E[e^{tX}] = \int_{0}^{\infty} e^{tx} (e^{-x}) dx = \int_{0}^{\infty} e^{(t-1)x} dx$
>     这个积分只在 $(t-1) < 0$ (即 $t < 1$) 时收敛。
>     $\psi(t) = \left[ \frac{e^{(t-1)x}}{t-1} \right]_{x=0}^{x=\infty} = \frac{0}{t-1} - \frac{1}{t-1} = \frac{-1}{t-1} = \frac{1}{1-t}$ (对于 $t < 1$)
> * **2. 求矩** (MGF 在 $t=0$ 附近存在，例如在 $(-1, 1)$ 内)：
>     * $\psi(t) = (1-t)^{-1}$
>     * $\psi'(t) = (-1)(1-t)^{-2}(-1) = (1-t)^{-2}$
>     * $\psi''(t) = (-2)(1-t)^{-3}(-1) = 2(1-t)^{-3}$
> * **3. 令 t=0**：
>     * $E(X) = \psi'(0) = (1-0)^{-2} = 1$
>     * $E(X^2) = \psi''(0) = 2(1-0)^{-3} = 2$
> * **4. 计算方差**：
>     $Var(X) = E(X^2) - [E(X)]^2 = 2 - 1^2 = 1$

---

### MGF 的性质

> [!theorem] 定理 4.4.3：线性函数的 MGF
>
> 设 $X$ 的 MGF 为 $\psi_X(t)$。
> 设 $Y = aX + b$。
> 那么 $Y$ 的 MGF $\psi_Y(t)$ 是：
> $$
> \psi_Y(t) = e^{bt} \psi_X(at)
> $$
>
> **证明**：
> $\psi_Y(t) = E[e^{tY}] = E[e^{t(aX+b)}] = E[e^{atX + bt}]$
> $= E[e^{atX} \cdot e^{bt}]$ (因为 $e^{bt}$ 是常数，可以提出期望)
> $= e^{bt} E[e^{atX}]$ ( $E[e^{atX}]$ 就是 $\psi_X$ 在 $at$ 处的值)
> $= e^{bt} \psi_X(at)$

> [!theorem] 定理 4.4.4：独立变量之和的 MGF
>
> 设 $X_1, \dots, X_n$ 是 $n$ 个**相互独立**的随机变量。
> 设 $Y = X_1 + \dots + X_n$。
> 那么 $Y$ 的 MGF 是 $X_i$ 的 MGF 的**乘积**：
> $$
> \psi_Y(t) = \psi_{X_1}(t) \cdot \psi_{X_2}(t) \cdot \dots \cdot \psi_{X_n}(t)
> $$
>
> **证明**：
> $\psi_Y(t) = E[e^{tY}] = E[e^{t(X_1 + \dots + X_n)}] = E[e^{tX_1} \cdot \dots \cdot e^{tX_n}]$
> 因为 $X_i$ 相互独立，所以 $e^{tX_i}$ 也相互独立。
> 根据定理 4.2.6 (独立变量乘积的期望 = 期望的乘积)：
> $= E[e^{tX_1}] \cdot \dots \cdot E[e^{tX_n}]$
> $= \psi_{X_1}(t) \cdot \dots \cdot \psi_{X_n}(t)$
>
> > [!success] MGF 的威力
> > **独立变量之和的 MGF = MGF 的乘积**。
> > 这比“卷积公式” (第3章) 简单得多！

---

#### MGF 的应用

> [!example] 二项分布的 MGF
>
> 设 $X \sim Bin(n, p)$。我们知道 $X = X_1 + \dots + X_n$，其中 $X_i$ 是独立的伯努利($p$)变量。
> 1.  **求 $X_i$ 的 MGF $\psi_i(t)$**：
>     $\psi_i(t) = E[e^{tX_i}]$
>     $= e^{t \cdot 1} Pr(X_i=1) + e^{t \cdot 0} Pr(X_i=0)$
>     $= e^t (p) + 1 \cdot (1-p) = (pe^t + 1 - p)$
> 2.  **求 $X$ 的 MGF $\psi(t)$** (根据定理 4.4.4)：
>     $\psi(t) = \psi_1(t) \cdot \dots \cdot \psi_n(t)$
>     $= (pe^t + 1 - p) \cdot \dots \cdot (pe^t + 1 - p)$
>     $$
>     \psi_{Bin(n,p)}(t) = (pe^t + 1 - p)^n
>     $$

> [!theorem] 定理 4.4.5：MGF 的唯一性
>
> 如果两个随机变量 $X_1$ 和 $X_2$ 的 MGF $\psi_1(t)$ 和 $\psi_2(t)$ 在 $t=0$ 附近的一个开区间内**相等**，那么 $X_1$ 和 $X_2$ **具有完全相同的分布**。
>
> **意义**：MGF 是分布的“指纹”。如果两个分布的 MGF 相同，它们就是同一个分布。

> [!theorem] 定理 4.4.6：二项分布的可加性
>
> * 设 $X_1 \sim Bin(n_1, p)$
> * 设 $X_2 \sim Bin(n_2, p)$
> * 且 $X_1, X_2$ **相互独立**。
>
> 那么 $Y = X_1 + X_2 \sim Bin(n_1 + n_2, p)$。
>
> **使用 MGF 的证明**：
> 1.  $X_1$ 的 MGF：$\psi_1(t) = (pe^t + 1 - p)^{n_1}$
> 2.  $X_2$ 的 MGF：$\psi_2(t) = (pe^t + 1 - p)^{n_2}$
> 3.  $Y$ 的 MGF (根据定理 4.4.4)：
>     $\psi_Y(t) = \psi_1(t) \psi_2(t) = (pe^t + 1 - p)^{n_1} (pe^t + 1 - p)^{n_2}$
>     $\psi_Y(t) = (pe^t + 1 - p)^{n_1 + n_2}$
> 4.  我们发现 $\psi_Y(t)$ 正是 $Bin(n_1 + n_2, p)$ 分布的 MGF。
> 5.  根据**唯一性定理 (4.4.5)**，$Y$ 必须服从该二项分布。

---

## 4.5 均值 (Mean) 与 中位数 (Median)

均值和中位数都是衡量分布“中心位置”的指标。

### 中位数 (Median)

> [!definition] 定义 4.5.1：中位数 (Median)
>
> 设 $X$ 是一个随机变量。任何满足以下两个条件的数 $m$，都称为 $X$ 分布的**中位数**：
>
> 1.  $Pr(X \le m) \ge 1/2$
> 2.  $Pr(X \ge m) \ge 1/2$
>
> **通俗解释**：中位数 $m$ 是一个“分割点”，它左边的概率和右边的概率都*至少*是 50%。
> (注：第 3 章定义的 $1/2$ 分位数 $F^{-1}(0.5)$ 总是一个中位数。)

> [!example] 例子 4.5.1：唯一中位数 (离散)
>
> * $Pr(X=1)=0.1$, $Pr(X=2)=0.2$, $Pr(X=3)=0.3$, $Pr(X=4)=0.4$
> * $Pr(X \le 2) = 0.3 < 0.5$
> * $Pr(X \le 3) = 0.6 \ge 0.5$  (满足条件 1)
> * $Pr(X \ge 3) = 0.3 + 0.4 = 0.7 \ge 0.5$  (满足条件 2)
> * 因此，$m=3$ 是一个中位数。

> [!example] 例子 4.5.2 & 4.5.4：不唯一的中位数
>
> **离散情况**：
> * $Pr(X=1)=0.1$, $Pr(X=2)=0.4$, $Pr(X=3)=0.3$, $Pr(X=4)=0.2$
> * $Pr(X \le 2) = 0.5$
> * $Pr(X \ge 3) = 0.5$
> * 对于**任意** $m$ 满足 $2 \le m \le 3$：
>     * $Pr(X \le m) = 0.5 \ge 0.5$
>     * $Pr(X \ge m) = 0.5 \ge 0.5$
> * 所以， $[2, 3]$ 闭区间内的**所有数**都是中位数。
>
> **连续情况**：
> * $f(x) = 1/2$ ( $0 \le x \le 1$ )，$f(x) = 1$ ( $2.5 \le x \le 3$ )
> * $Pr(X \le 1) = 1/2 \times (1-0) = 1/2$
> * $Pr(X \ge 2.5) = 1 \times (3-2.5) = 1/2$
> * 对于**任意** $m$ 满足 $1 \le m \le 2.5$， $Pr(X \le m) = 1/2$ 且 $Pr(X \ge m) = 1/2$。
> * 所以， $[1, 2.5]$ 闭区间内的**所有数**都是中位数。

---

### 均值 vs. 中位数

| 特性 | 均值 (Mean) $E(X)$ | 中位数 (Median) $m$ |
| --- | --- | --- |
| **存在性** | **不一定** (例如柯西分布) | **总是存在** |
| **唯一性** | 如果存在，**总是唯一** | **不一定唯一** (可能是一个区间) |
| **稳健性** | 对离群值**非常敏感** | 对离群值**非常稳健 (Robust)** |

> [!example] 例子 4.5.6：年收入
>
> 100 个家庭：99 户年收入 \$1,000，1 户年收入 \$2,901,000。
> * **均值**：$\frac{99(1000) + 1(2901000)}{100} = \$30,000$
>     * 这个均值具有误导性，99% 的家庭都远低于这个数。
> * **中位数**：\$1,000
>     * 这个中位数更能反映“典型”家庭的情况。

---

### 预测与损失函数

假设我们要用一个常数 $d$ 来“预测” $X$ 的值。哪个 $d$ 是“最好”的？这取决于我们如何定义“最好”。

#### 1. 均方误差 (Mean Squared Error - MSE)

> [!definition] 定义 4.5.2：均方误差 (M.S.E.)
>
> M.S.E. 是**误差平方的期望**：
> $$
> \text{M.S.E.}(d) = E[ (X - d)^2 ]
> $$

> [!theorem] 定理 4.5.2：均值最小化 MSE
>
> 要使 $E[ (X - d)^2 ]$ 最小， $d$ 的**唯一最优**选择是：
> $$
> d = E(X) = \mu
> $$
> 此时，最小的 MSE 值就是**方差** $Var(X) = \sigma^2$。
>
> **证明**：
> $E[(X-d)^2] = E[ ( (X - \mu) + (\mu - d) )^2 ]$  (其中 $\mu - d$ 是常数)
> $= E[ (X - \mu)^2 + 2(X - \mu)(\mu - d) + (\mu - d)^2 ]$
> $= E[(X - \mu)^2] + 2(\mu - d) E[X - \mu] + E[(\mu - d)^2]$
> $= Var(X) + 2(\mu - d) (0) + (\mu - d)^2$
> $$
> E[(X-d)^2] = Var(X) + (\mu - d)^2
> $$
>
> * $Var(X)$ 是一个固定的非负常数。
> * $(\mu - d)^2$ 是一个非负项，它取决于我们的选择 $d$。
> * 要使总和最小，我们必须让 $(\mu - d)^2 = 0$，这当且仅当 $d = \mu$ 时成立。

> [!example] 例子 4.5.5 & 4.5.7：彩票问题 (几何分布)
>
> * **问题**：每天抽 1000 个号。$X$ 是最后一个号码被抽中所需的天数。
> * **分布**：$X$ 服从几何分布，$p = 0.001$。
>     $f(x) = (0.001)(0.999)^{x-1}$ (对于 $x=1, 2, \dots$)
> * **最小化 MSE 的预测**：$d = E(X)$。
> * **计算 $E(X)$**：
>     我们知道几何分布的均值是 $1/p$ (在例 4.2.9 中已证明)。
>     $E(X) = \frac{1}{0.001} = 1000$ 天。
> * **结论**：最佳的 MSE 预测是 1000 天。

#### 2. 平均绝对误差 (Mean Absolute Error - MAE)

> [!definition] 定义 4.5.3：平均绝对误差 (M.A.E.)
>
> M.A.E. 是**误差绝对值的期望**：
> $$
> \text{M.A.E.}(d) = E[ |X - d| ]
> $$

> [!theorem] 定理 4.5.3：中位数最小化 MAE
>
> 要使 $E[ |X - d| ]$ 最小， $d$ 的最优选择是 $X$ 的**任意一个中位数 $m$**。
> $$
> d = \text{Median}(X)
> $$
>
> **证明 (简述 $d > m$ 的情况)**：
> 设 $m$ 是一个中位数 ($Pr(X \le m) \ge 0.5$)， $f(x)$ 是 p.d.f.。
> $\text{M.A.E.}(d) - \text{M.A.E.}(m) = E[|X-d|] - E[|X-m|]$
> $= \int_{-\infty}^{\infty} (|x-d| - |x-m|) f(x) dx$
>
> 我们把积分分成三段 $(-\infty, m], [m, d], [d, \infty)$：
> $= \int_{-\infty}^{m} ((d-x) - (m-x)) f(x) dx + \int_{m}^{d} ((d-x) - (x-m)) f(x) dx + \int_{d}^{\infty} ((x-d) - (x-m)) f(x) dx$
> $= \int_{-\infty}^{m} (d-m) f(x) dx + \int_{m}^{d} (d+m-2x) f(x) dx + \int_{d}^{\infty} (m-d) f(x) dx$
>
> 在 $[m, d]$ 区间，$2x \le 2d$，所以 $d+m-2x \ge d+m-2d = m-d$。
> $\ge \int_{-\infty}^{m} (d-m) f(x) dx + \int_{m}^{d} (m-d) f(x) dx + \int_{d}^{\infty} (m-d) f(x) dx$
> $= (d-m) \int_{-\infty}^{m} f(x) dx + (m-d) \int_{m}^{\infty} f(x) dx$
> $= (d-m) Pr(X \le m) - (d-m) Pr(X > m)$
> $$
> = (d-m) [Pr(X \le m) - Pr(X > m)]
> $$
> * 因为 $d > m$，所以 $(d-m) > 0$。
> * 因为 $m$ 是中位数，所以 $Pr(X \le m) \ge 0.5$， $Pr(X > m) \le 0.5$。
> * 所以 $[Pr(X \le m) - Pr(X > m)] \ge 0$。
>
> 因此，总差值 $\ge 0$，即 $E[|X-d|] \ge E[|X-m|]$。

> [!example] 例子 4.5.8：彩票问题 (续)
>
> * **问题**：求 $X \sim Geom(0.001)$ 的中位数。
> * **最小化 MAE 的预测**：$d = \text{Median}(X)$。
> * **计算中位数 $m$**：
>     c.d.f. $F(x) = Pr(X \le x) = \sum_{k=1}^{x} (0.001)(0.999)^{k-1}$
>     这是一个等比数列求和： $F(x) = \frac{0.001 (1 - 0.999^x)}{1 - 0.999} = 1 - 0.999^x$
>     我们要找最小的 $x$ 使得 $F(x) \ge 0.5$：
>     $1 - 0.999^x \ge 0.5$
>     $0.5 \ge 0.999^x$
>     $\ln(0.5) \ge x \ln(0.999)$
>     $x \ge \frac{\ln(0.5)}{\ln(0.999)} \approx \frac{-0.6931}{-0.0010005} \approx 692.8$
>     由于 $x$ 必须是整数，所以最小的整数 $x$ 是 $m = 693$。
> * **结论**：最佳的 MAE 预测是 693 天。
> * **对比**：
>     * 均值 (MSE) = 1000
>     * 中位数 (MAE) = 693
>     * 均值远大于中位数，因为这个分布是高度**右偏**的 (有很小的概率花 5000 天、10000 天...)，这些离群值拉高了均值，但对中位数影响很小。

---

## 4.6 协方差与相关性 (Covariance and Correlation)

均值和方差只描述**单个**随机变量。当我们有两个变量 $X, Y$ 时，我们想知道它们之间的**关系**。

**协方差**和**相关性**是衡量**线性依赖**程度的指标。

> [!example] 例子 4.6.1：考试分数
>
> * $X$ = 语言测试分数
> * $Y$ = 数学测试分数
> * 我们直觉上认为 $X, Y$ 是有关系的。 $X$ 高的学生，$Y$ 往往也比较高。
> * 协方差和相关性就是用来量化这种“往往”的程度。

### 协方差 (Covariance)

> [!definition] 定义 4.6.1：协方差 (Covariance)
>
> 设 $E(X) = \mu_X$，$E(Y) = \mu_Y$ (均值都有限)。
> $X$ 和 $Y$ 的**协方差**，记为 $Cov(X, Y)$，定义为：
> $$
> Cov(X, Y) = E[ (X - \mu_X) (Y - \mu_Y) ]
> $$
>
> > [!info] 协方差的直觉
> >
> > 协方差是 $(X-\mu_X)$ 和 $(Y-\mu_Y)$ 乘积的期望。
> > * **$Cov(X, Y) > 0$ (正协方差)**：
> >     这说明 $(X, Y)$ 倾向于**同向**偏离均值。
> >     即： $X > \mu_X$ (X 偏高) 和 $Y > \mu_Y$ (Y 偏高) **同时发生**；
> >     并且/或者 $X < \mu_X$ (X 偏低) 和 $Y < \mu_Y$ (Y 偏低) **同时发生**。
> >
> > * **$Cov(X, Y) < 0$ (负协方差)**：
> >     这说明 $(X, Y)$ 倾向于**反向**偏离均值。
> >     即： $X > \mu_X$ (X 偏高) 和 $Y < \mu_Y$ (Y 偏低) **同时发生**。
> >
> > * **$Cov(X, Y) = 0$ (零协方差)**：
> >     同向偏离和反向偏离的趋势相互抵消。我们称 $X, Y$ **不相关 (Uncorrelated)**。

> [!success] 定理 4.6.1：协方差的计算公式
>
> $$
> Cov(X, Y) = E(XY) - E(X)E(Y)
> $$
>
> **通俗地说：“协方差 = 乘积的期望 - 期望的乘积”**
>
> **证明**：
> $Cov(X, Y) = E[ (X - \mu_X) (Y - \mu_Y) ]$
> $= E[ XY - \mu_X Y - \mu_Y X + \mu_X \mu_Y ]$ (展开)
> (利用期望的线性性)
> $= E(XY) - E(\mu_X Y) - E(\mu_Y X) + E(\mu_X \mu_Y)$
> (提出常数 $\mu_X, \mu_Y$)
> $= E(XY) - \mu_X E(Y) - \mu_Y E(X) + \mu_X \mu_Y$
> (代入 $E(Y)=\mu_Y, E(X)=\mu_X$)
> $= E(XY) - \mu_X \mu_Y - \mu_Y \mu_X + \mu_X \mu_Y$
> $= E(XY) - \mu_X \mu_Y$
> $= E(XY) - E(X)E(Y)$

---

### 相关性 (Correlation)

**协方差的缺点**：$Cov(X, Y)$ 的大小会受 $X, Y$ 自身尺度的影响。
例如： $Cov(2X, Y) = E(2XY) - E(2X)E(Y) = 2 E(XY) - 2 E(X)E(Y) = 2 Cov(X, Y)$。
仅仅把 $X$ 的单位乘以 2，协方差就变大了 2 倍，但这并没有改变 $X, Y$ 之间的“关系强度”。

我们需要一个**标准化**的、不受单位影响的度量。

> [!definition] 定义 4.6.2：相关性 (Correlation)
>
> 设 $X, Y$ 的方差 $\sigma_X^2, \sigma_Y^2$ 都为正且有限。
> $X$ 和 $Y$ 的**相关性** (或**相关系数**)，记为 $\rho(X, Y)$，定义为：
> $$
> \rho(X, Y) = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}
> $$

#### 相关性的范围

> [!theorem] 定理 4.6.2：施瓦茨不等式 (Schwarz Inequality)
>
> 对于任意随机变量 $U, V$：
> $$
> [E(UV)]^2 \le E(U^2) E(V^2)
> $$
>
> 当 $E(U^2), E(V^2)$ 有限时，等号成立的**当且仅当**存在非零常数 $a, b$ 使得 $aU + bV = 0$ (以概率 1 成立)。

> [!theorem] 定理 4.6.3：柯西-施瓦茨不等式 (Cauchy-Schwarz Inequality)
>
> 1.  $[Cov(X, Y)]^2 \le \sigma_X^2 \sigma_Y^2$
>
> 2.  (相关系数的范围)：
>     $$
>     -1 \le \rho(X, Y) \le 1
>     $$
>
> 3.  **等号成立条件**：
>     * $\rho(X, Y) = 1$ 或 $-1$ (即 $|\rho| = 1$)
>     * **当且仅当** $X, Y$ 之间存在**完美的线性关系**。
>     * 即： $Y = aX + b$ (以概率 1 成立)，其中 $a \ne 0$。
>     * 如果 $a > 0$， $\rho = +1$ (完全正相关)
>     * 如果 $a < 0$， $\rho = -1$ (完全负相关)
>
> **证明**：
> 4.  在定理 4.6.2 中，令 $U = X - \mu_X$， $V = Y - \mu_Y$。
> 5.  $E(U^2) = E[(X-\mu_X)^2] = \sigma_X^2$
> 6.  $E(V^2) = E[(Y-\mu_Y)^2] = \sigma_Y^2$
> 7.  $E(UV) = E[(X - \mu_X) (Y - \mu_Y)] = Cov(X, Y)$
> 8.  代入施瓦茨不等式：$[Cov(X, Y)]^2 \le \sigma_X^2 \sigma_Y^2$。
> 9.  两边同时除以 $\sigma_X^2 \sigma_Y^2$：
>     $\frac{[Cov(X, Y)]^2}{\sigma_X^2 \sigma_Y^2} \le 1$
>     $[\rho(X, Y)]^2 \le 1$
> 10.  两边开根号：$-1 \le \rho(X, Y) \le 1$。
> 11.  等号成立条件 $aU + bV = 0$ 变为 $a(X-\mu_X) + b(Y-\mu_Y) = 0$，整理即可得 $Y$ 是 $X$ 的线性函数。

---

### 协方差与相关性的性质

> [!definition] 定义 4.6.3：相关性术语
>
> * $\rho(X, Y) > 0 \implies X, Y$ **正相关**
> * $\rho(X, Y) < 0 \implies X, Y$ **负相关**
> * $\rho(X, Y) = 0 \implies X, Y$ **不相关**

> [!theorem] 定理 4.6.4：独立 $\implies$ 不相关
>
> 如果 $X, Y$ **相互独立** (且方差有限)，那么它们必然**不相关**。
> $$
> \text{独立 (Independent)} \implies \text{不相关 (Uncorrelated, } \rho=0)
> $$
>
> **证明**：
> 1.  $X, Y$ 独立 $\implies E(XY) = E(X)E(Y)$ (定理 4.2.6)
> 2.  $Cov(X, Y) = E(XY) - E(X)E(Y) = 0$
> 3.  $\rho(X, Y) = \frac{Cov(X, Y)}{\sigma_X \sigma_Y} = \frac{0}{\sigma_X \sigma_Y} = 0$

> [!danger] 核心陷阱：不相关 $\not\implies$ 独立
>
> **反之不成立！** 两个**相关** (Dependent) 的变量，也可能是**不相关** (Uncorrelated) 的。
>
> 原因是：$\rho = 0$ 只说明了**没有线性关系**。但它们可能存在**非线性关系**。

> [!example] 例子 4.6.4：相关但又不相关
>
> * **问题**：设 $X$ 等概率取 -1, 0, 1 三个值 ($Pr(X=x) = 1/3$)。
> * **定义 $Y = X^2$**。
> * **1. 它们是否相关？**
>     **是**。 $Y$ 的值完全由 $X$ 决定。 $Y$ 和 $X$ 显然是相关的 (dependent)。
> * **2. 它们是否不相关？(计算 $\rho(X, Y)$)**
>     * $E(X) = \frac{1}{3}(-1 + 0 + 1) = 0$
>     * $E(Y) = E(X^2) = \frac{1}{3}[(-1)^2 + 0^2 + 1^2] = \frac{2}{3}$
>     * $E(XY) = E(X \cdot X^2) = E(X^3)$
>         $E(X^3) = \frac{1}{3}[(-1)^3 + 0^3 + 1^3] = \frac{1}{3}[-1 + 0 + 1] = 0$
>     * $Cov(X, Y) = E(XY) - E(X)E(Y) = 0 - (0)(\frac{2}{3}) = 0$
> * **结论**：$Cov(X, Y) = 0$，所以 $\rho(X, Y) = 0$。
> $X$ 和 $Y=X^2$ 是**相关**的，但它们是**不相关**的。
>
> **直觉**：$X$ 和 $Y=X^2$ 之间存在完美的**二次 (非线性)** 关系，但没有任何**线性**关系。


---

> [!example] 例子 4.6.5：圆形上的均匀分布
>
> * **问题**：设 $(X, Y)$ 在 $x^2 + y^2 < 1$ 的单位圆内部服从均匀分布。其 p.d.f. $f(x, y)$ 在圆内是一个常数，即 $1/(\text{面积}) = 1/\pi$。
> * **分析**：
>     1.  **相关性 (Dependence)**：$X$ 和 $Y$ 显然是**相关**的。因为变量的取值范围不是一个矩形。例如， $Y$ 的边际取值范围是 $(-1, 1)$，但如果我们观测到 $X=0.5$， $Y$ 的可能取值范围就缩小到了 $(-0.866, 0.866)$ 之间。
>     2.  **不相关性 (Uncorrelation)**：
>         * 由于分布关于原点对称， $E(X) = 0$ 且 $E(Y) = 0$。
>         * $E(XY) = \int\int xy f(x,y) dx dy$。由于 $xy$ 在第一、三象限为正，在第二、四象限为负，并且分布区域是对等的，这个积分的结果为 0。
>         * $Cov(X, Y) = E(XY) - E(X)E(Y) = 0 - 0 \cdot 0 = 0$。
> * **结论**：$X$ 和 $Y$ 是相关的，但它们是**不相关**的 ($\rho = 0$)。

> [!warning] 核心要点：相关性只衡量线性关系
>
> 例子 4.6.4 和 4.6.5 完美地展示了：
> * $\rho(X, Y) = 0$ (不相关) **不** 意味着 $X, Y$ 独立。
> * $\rho(X, Y)$ 只衡量**线性**关系的强度。
> * $X, Y$ 可以有很强的**非线性**关系（如 $Y=X^2$），但相关系数 $\rho$ 仍然可以为 0。

---

> [!theorem] 定理 4.6.5：完美线性相关
>
> 假设 $X$ 的方差 $0 < \sigma_X^2 < \infty$。
> 设 $Y = aX + b$，其中 $a \ne 0$。
>
> 1.  如果 $a > 0$ (正斜率)，则 $\rho(X, Y) = 1$ (完全正相关)。
> 2.  如果 $a < 0$ (负斜率)，则 $\rho(X, Y) = -1$ (完全负相关)。
>
> **证明**：
> 1.  $\mu_Y = a\mu_X + b$，所以 $Y - \mu_Y = a(X - \mu_X)$。
> 2.  $Cov(X, Y) = E[ (X - \mu_X) (Y - \mu_Y) ] = E[ (X - \mu_X) (a(X - \mu_X)) ]$
>     $= a E[ (X - \mu_X)^2 ] = a \sigma_X^2$。
> 3.  根据定理 4.3.4， $\sigma_Y = |a| \sigma_X$。
> 4.  $\rho(X, Y) = \frac{Cov(X, Y)}{\sigma_X \sigma_Y} = \frac{a \sigma_X^2}{\sigma_X (|a| \sigma_X)} = \frac{a \sigma_X^2}{|a| \sigma_X^2} = \frac{a}{|a|}$。
> 5.  如果 $a > 0$，$a/|a| = 1$。如果 $a < 0$，$a/|a| = -1$。
>
> **反之亦然**：如果 $|\rho(X, Y)| = 1$，那么 $X$ 和 $Y$ 之间必然存在完美的线性关系 $Y = aX + b$。

---

### 和的方差 (续)

> [!theorem] 定理 4.6.6：任意两个变量和的方差
>
> 如果 $X, Y$ 的方差有限，那么：
> $$
> Var(X + Y) = Var(X) + Var(Y) + 2 Cov(X, Y)
> $$
>
> **证明**：
> $Var(X+Y) = E[ ( (X+Y) - (\mu_X+\mu_Y) )^2 ]$
> $= E[ ( (X - \mu_X) + (Y - \mu_Y) )^2 ]$
> $= E[ (X - \mu_X)^2 + (Y - \mu_Y)^2 + 2(X - \mu_X)(Y - \mu_Y) ]$
> (利用期望的线性性)
> $= E[(X - \mu_X)^2] + E[(Y - \mu_Y)^2] + 2 E[(X - \mu_X)(Y - \mu_Y)]$
> $= Var(X) + Var(Y) + 2 Cov(X, Y)$

> [!definition] 推论 4.6.1：任意线性组合的方差
>
> $$
> Var(aX + bY + c) = a^2 Var(X) + b^2 Var(Y) + 2ab Cov(X, Y)
> $$
>
> **一个有用的特例**：
> $$
> Var(X - Y) = Var(X) + Var(Y) - 2 Cov(X, Y)
> $$

> [!example] 例子 4.6.6：投资组合 (负相关)
>
> * **问题**：回到例 4.3.7 的投资组合，但这次假设 $R_1, R_2$ **不独立**，而是 $\rho(R_1, R_2) = -0.3$。
> * **方差计算**：$Y = s_1 R_1 + s_2 R_2 + 0.036 s_3$。
>     $Var(Y) = Var(s_1 R_1 + s_2 R_2)$
>     $= Var(s_1 R_1) + Var(s_2 R_2) + 2 Cov(s_1 R_1, s_2 R_2)$
>     $= s_1^2 Var(R_1) + s_2^2 Var(R_2) + 2 s_1 s_2 Cov(R_1, R_2)$
>     (其中 $Cov(R_1, R_2) = \rho \sigma_1 \sigma_2 = -0.3 \sqrt{55 \times 28}$)
>     $= 55 s_1^2 + 28 s_2^2 - (2 \times 0.3 \sqrt{55 \times 28}) s_1 s_2$
> * **结论**：由于 $\rho < 0$，协方差项为负，这会**降低**整个投资组合的方差 (风险)。如图 4.11 所示，负相关的“有效前沿”比 $\rho=0$ 时的风险更低（曲线在下方）。

> [!theorem] 定理 4.6.7：n 个变量和的方差
>
> $$
> Var\left(\sum_{i=1}^{n} X_i\right) = \sum_{i=1}^{n} Var(X_i) + 2 \sum_{i<j} Cov(X_i, X_j)
> $$
>
> **通俗解释**：总方差 = 所有**方差**之和 (对角线项) + 所有**协方差**的两倍 (非对角线项)。
>
> **证明**：
> $Var(\sum X_i) = Cov(\sum X_i, \sum X_j) = \sum_i \sum_j Cov(X_i, X_j)$
> 把这个 $n \times n$ 的协方差矩阵求和，分成“对角线” (当 $i=j$) 和“非对角线” (当 $i \ne j$)：
> $= \sum_{i=1}^{n} Cov(X_i, X_i) + \sum_{i \ne j} Cov(X_i, X_j)$
> (因为 $Cov(X_i, X_i) = Var(X_i)$ 且 $Cov(X_i, X_j) = Cov(X_j, X_i)$)
> $= \sum_{i=1}^{n} Var(X_i) + 2 \sum_{i < j} Cov(X_i, X_j)$

> [!definition] 推论 4.6.2：不相关变量和的方差
>
> 如果 $X_1, \dots, X_n$ **两两不相关** (即 $Cov(X_i, X_j) = 0$ 对所有 $i \ne j$)，那么：
> $$
> Var\left(\sum_{i=1}^{n} X_i\right) = \sum_{i=1}^{n} Var(X_i)
> $$
>
> **注意**：这比定理 4.3.5 (要求**独立**) 的条件更弱。**只要不相关，方差就可以相加**。

---

> [!summary] 4.6 总结
> * **协方差** $Cov(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] = E(XY) - E(X)E(Y)$。
> * **相关性** $\rho(X,Y) = Cov(X,Y) / (\sigma_X \sigma_Y)$，是标准化的协方差。
> * $\rho$ 衡量**线性**关系：$-1 \le \rho \le 1$。
> * $|\rho| = 1$ 当且仅当 $Y = aX + b$。
> * **独立 $\implies$ 不相关 ($\rho=0$)**。
> * **不相关 ($\rho=0$) $\not\implies$ 独立** (可能存在非线性关系)。
> * **方差和**： $Var(aX+bY+c) = a^2 Var(X) + b^2 Var(Y) + 2ab Cov(X,Y)$。
> * **不相关**变量的方差**可以**相加；**相关**变量的方差**不可以**简单相加。

---

## 4.7 条件期望 (Conditional Expectation)

既然期望、方差、协方差都是分布的属性，我们自然可以定义它们在**条件分布**下的版本。

本节的核心思想是：如果我们知道了 $X$ 的值，我们如何更新对 $Y$ 的最佳预测？

### 定义与基本性质

> [!definition] 定义 4.7.1：条件期望 (Conditional Expectation)
>
> 设 $X, Y$ 为随机变量。 $Y$ **给定 $X=x$ 的条件期望**，记为 $E(Y|x)$，被定义为 $Y$ 的**条件分布 $g_2(y|x)$ 的期望**。
>
> * **连续情况**：$E(Y|x) = \int_{-\infty}^{\infty} y g_2(y|x) dy$
> * **离散情况**：$E(Y|x) = \sum_{\text{所有 } y} y g_2(y|x)$

$E(Y|x)$ 的计算结果是一个**关于 $x$ 的函数**，我们记这个函数为 $h(x)$。

> [!definition] 定义 4.7.2：条件期望 (作为随机变量)
>
> 设 $h(x) = E(Y|x)$。我们定义 $E(Y|X)$ 为 $h(X)$。
> $E(Y|X)$ 是一个**随机变量**，因为它是一个关于随机变量 $X$ 的函数。
>
> * **$E(Y|x)$** 是一个**数值** (或一个关于 $x$ 的函数)。
> * **$E(Y|X)$** 是一个**随机变量** (其取值取决于 $X$ 的取值)。

> [!example] 例子 4.7.1 & 4.7.2：家庭调查
>
> * **问题**：随机抽取一个家庭， $X$ 是成员数， $Y$ 是汽车数。数据见表 4.1 & 4.2。
> * **计算 $E(Y|X=4)$** (即：已知一个家庭有4个成员，预期他们家有几辆车？)：
>     1.  找到 $X=4$ 的条件 p.f. $g_2(y|4) = f(4, y) / f_1(4)$。
>         $f_1(4) = 0.208$ (来自表 4.2 的 $x=4$ 列的总和)。
>         $g_2(0|4) = f(4,0)/f_1(4) = 0.008 / 0.208 \approx 0.0385$
>         $g_2(1|4) = f(4,1)/f_1(4) = 0.120 / 0.208 \approx 0.5769$
>         $g_2(2|4) = f(4,2)/f_1(4) = 0.060 / 0.208 \approx 0.2885$
>         $g_2(3|4) = f(4,3)/f_1(4) = 0.020 / 0.208 \approx 0.0962$
>     2.  计算该条件分布的期望：
>         $E(Y|4) = 0(0.0385) + 1(0.5769) + 2(0.2885) + 3(0.0962) \approx 1.442$
> * **$E(Y|X)$**：
>     $E(Y|X)$ 是一个随机变量，它取 $E(Y|x)$ 表中的值 (如 0.609, 1.057, ..., 2)，其概率由 $f_1(x)$ ( $X$ 的边际 p.f.) 给出。

> [!example] 例子 4.7.3：临床试验
>
> * **背景**：$P$ 是病人成功的真实（未知）概率。 $X_i$ 是第 $i$ 个病人的结果 (1=成功, 0=失败)。
> * **假设**：给定 $P=p$， $X_i$ 之间条件独立，且 $Pr(X_i=1|P=p) = p$。
> * **问题**：$X = X_1 + \dots + X_n$ 是 $n$ 个病人中成功的总数。求 $E(X|P)$。
> * **解**：
>     * 给定 $P=p$，$X$ 服从 $Bin(n, p)$ 分布。
>     * $Bin(n, p)$ 分布的均值是 $np$。
>     * 因此，$E(X|p) = np$。
>     * 作为随机变量， $E(X|P) = nP$。

---

> [!theorem] 定理 4.7.1：期望的全概率定律 (Law of Total Probability for Expectations)
>
> (也称为：**Iterated Expectations** 或 **Tower Property**)
> $$
> E[ E(Y|X) ] = E(Y)
> $$
>
> **通俗解释**：“对条件期望的期望，就是期望本身”。
>
> **证明 (连续情况)**：
> 1.  $E[E(Y|X)]$ 是 $E(Y|X)$ 这个随机变量的期望。根据 LOTUS：
>     $E[E(Y|X)] = \int_{-\infty}^{\infty} E(Y|x) f_1(x) dx$
> 2.  代入 $E(Y|x)$ 的定义 (式 4.7.1)：
>     $= \int_{-\infty}^{\infty} \left[ \int_{-\infty}^{\infty} y g_2(y|x) dy \right] f_1(x) dx$
> 3.  因为 $g_2(y|x) f_1(x) = f(x, y)$ (条件概率定义)：
>     $= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y f(x, y) dy dx$
> 4.  这正是 $E(Y)$ 的定义 (根据多变量 LOTUS)。

> [!example] 例子 4.7.6：均匀分布的期望
>
> 1.  $X \sim U[0, 1]$。
> 2.  给定 $X=x$ 后，$Y \sim U[x, 1]$。
> 3.  求 $E(Y)$。
>
> **解**：
> * 我们需要 $E(Y|x)$。 $Y$ 在 $[x, 1]$ 上均匀分布，其均值是中点：
>   $E(Y|x) = \frac{x+1}{2}$。
> * $E(Y|X)$ 是随机变量 $\frac{X+1}{2}$。
> * 根据定理 4.7.1： $E(Y) = E[E(Y|X)] = E\left[ \frac{X+1}{2} \right]$。
> * $= \frac{1}{2} E(X) + \frac{1}{2}$。
> * 因为 $X \sim U[0, 1]$， $E(X) = 1/2$。
> * $E(Y) = \frac{1}{2}(\frac{1}{2}) + \frac{1}{2} = \frac{3}{4}$。

---

> [!theorem] 定理 4.7.2
>
> 设 $Z = r(X, Y)$。
> $Z$ 给定 $X=x$ 的条件分布，与 $r(x, Y)$ 给定 $X=x$ 的条件分布相同。
>
> **推论** (把 $X$ 当作常数)：
> $E[r(X, Y) | X] = E[r(X, Y) | X=X]$。在 $X$ 已知的条件下，$X$ 就好像是一个常数。
>
> > [!example] 例子 4.7.7：线性条件期望
> >
> > * **问题**：假设 $E(Y|X) = aX + b$。求 $E(XY)$。
> > * **解**：
> >     1.  $E(XY) = E[ E(XY | X) ]$ (期望的全概率定律)
> >     2.  计算 $E(XY | X)$。在 $X$ 给定的条件下，$X$ 可以被当作一个常数提出来：
> >         $E(XY | X) = X \cdot E(Y | X)$
> >     3.  代入假设 $E(Y|X) = aX+b$：
> >         $E(XY | X) = X (aX + b) = aX^2 + bX$
> >     4.  代回第 1 步：
> >         $E(XY) = E[ aX^2 + bX ] = a E(X^2) + b E(X)$

---

### 条件方差 (Conditional Variance)

> [!definition] 定义 4.7.3：条件方差 (Conditional Variance)
>
> $Y$ **给定 $X=x$ 的条件方差**，记为 $Var(Y|x)$，是 $Y$ 的**条件分布**的方差。
> $$
> Var(Y|x) = E\left[ (Y - E(Y|x))^2 \mid x \right]
> $$
>
> **$Var(Y|X)$** 是一个**随机变量**，它是一个关于 $X$ 的函数 $v(X)$，其中 $v(x) = Var(Y|x)$。

---

### 预测 (Prediction)

> [!question] 问题
> 假设我们要观测 $X$，然后预测 $Y$。我们想找一个预测函数 $d(X)$，使得**均方误差 M.S.E.** $E[ (Y - d(X))^2 ]$ 最小。

> [!theorem] 定理 4.7.3：最佳预测器
>
> 最小化 M.S.E. $E[ (Y - d(X))^2 ]$ 的最优预测函数 $d(X)$ 是**条件期望**：
> $$
> d(X) = E(Y|X)
> $$
>
> **证明思路**：
> 1.  利用期望的全概率定律：$E[ (Y - d(X))^2 ] = E\left[ E\left( (Y - d(X))^2 \mid X \right) \right]$。
> 2.  我们想让 $E[ E( (Y - d(x))^2 | x ) ]$ (即 $\int E[ (Y - d(x))^2 | x ] f_1(x) dx$) 最小。
> 3.  这等价于让被积函数 $E[ (Y - d(x))^2 | x ]$ 对**每一个 $x$** 都最小。
> 4.  $E[ (Y - d(x))^2 | x ]$ 是 $Y$ 在**给定 $X=x$ 的条件分布**下的均方误差。
> 5.  根据定理 4.5.2 (均值最小化 MSE)，使均方误差最小的预测值 $d(x)$ 必须是该**条件分布**的**均值**。
> 6.  这个均值正是 $E(Y|x)$。
> 7.  因此，最优的 $d(X)$ 必须是 $E(Y|X)$。

#### M.S.E. 与方差分解

* 如果我们**不**观测 $X$，最优预测是 $E(Y)$，最小 M.S.E. 是 $Var(Y)$。
* 如果我们**观测 $X$**，最优预测是 $E(Y|X)$。
    * **在观测到 $X=x$ 之后**，该预测的 M.S.E. 是 $Var(Y|x)$。
    * **在观测 $X$ 之前**，该预测的**总体** M.S.E. (在所有 $X$ 上的平均) 是 $E[Var(Y|X)]$。
* **观测 $X$ 所减少的 M.S.E.** = $Var(Y) - E[Var(Y|X)]$。

> [!theorem] 定理 4.7.4：方差的全概率定律 (Law of Total Probability for Variances)
>
> $$
> Var(Y) = E[Var(Y|X)] + Var[E(Y|X)]
> $$
>
> **通俗解释**：
> * **总方差 (Total Variance)** = $Var(Y)$
> * **预期条件方差 (Expected Conditional Variance)** = $E[Var(Y|X)]$
>     (这是 $Y$ 在 $X$ 已知时的“平均”剩余方差，即“未解释的方差”)
> * **条件均值的方差 (Variance of Conditional Mean)** = $Var[E(Y|X)]$
>     (这是 $X$ 能“解释” $Y$ 的多少方差，即“已解释的方差”)
>
> **推论**：观测 $X$ 所减少的 M.S.E. $Var(Y) - E[Var(Y|X)]$ 正好等于 $Var[E(Y|X)]$。

> [!example] 例子 4.7.8：临床试验 (贝叶斯预测)
>
> 这是一个非常重要的贝叶斯推理例子。
> * **设置**：
>     * $P$ = 病人成功的真实（未知）概率。
>     * **先验分布 (Prior)**：我们对 $P$ 一无所知，假设 $P \sim U[0, 1]$。 $f_2(p) = 1$。
>     * **数据 (Likelihood)**：我们观测到 $n=40$ 个病人。 $X$ = 成功的病人数。
>     * **模型**：给定 $P=p$， $X \sim Bin(40, p)$。 $g_1(x|p) = \binom{40}{x} p^x (1-p)^{40-x}$。
> * **目标**：观测 $X=x$ 后，预测 $P$。
> * **1. 无 $X$ 时的预测 (先验)**：
>     * 预测 $d = E(P) = 1/2$。
>     * M.S.E. = $Var(P) = E(P^2) - [E(P)]^2 = \int_0^1 p^2 dp - (1/2)^2 = 1/3 - 1/4 = 1/12$。
> * **2. 有 $X$ 时的预测 (后验)**：
>     * 最优预测 $d(x) = E(P|x)$。
>     * **后验 p.d.f. $g_2(p|x)$**：根据贝叶斯定理 $g_2(p|x) \propto g_1(x|p) f_2(p)$。
>         $g_2(p|x) \propto \binom{40}{x} p^x (1-p)^{40-x} \cdot 1$。
>         $g_2(p|x) = C \cdot p^x (1-p)^{40-x}$。
>         使用Beta积分公式 $\int_0^1 p^k (1-p)^l dp = \frac{k!l!}{(k+l+1)!}$，我们可以标准化它。
>         (这其实是一个 Beta($x+1, 40-x+1$) 分布)
>         $g_2(p|x) = \frac{(41)!}{x!(40-x)!} p^x (1-p)^{40-x}$。
>     * **后验均值 $E(P|x)$** (最优预测)：
>         $E(P|x) = \int_0^1 p \cdot g_2(p|x) dp = \int_0^1 p \cdot C \cdot p^x (1-p)^{40-x} dp$
>         $= C \cdot \int_0^1 p^{x+1} (1-p)^{40-x} dp$
>         $= \frac{(41)!}{x!(40-x)!} \cdot \frac{(x+1)!(40-x)!}{(42)!} = \frac{41! (x+1)!}{x! (42)!} = \frac{x+1}{42}$。
>     * **结论**：观测到 $x$ 次成功后，对 $P$ 的最佳预测是 $(x+1)/42$。
> * **3. M.S.E. 评估**：
>     * **后验 M.S.E. (给定 $X=x$)**： $Var(P|x) = E(P^2|x) - [E(P|x)]^2$。
>         $E(P^2|x) = \int p^2 g_2(p|x) dp = \frac{(x+1)(x+2)}{42 \times 43}$。
>         $Var(P|x) = \frac{(x+1)(x+2)}{42 \times 43} - \left(\frac{x+1}{42}\right)^2 = \frac{(x+1)(41-x)}{42^2 \times 43}$。
>         (这是观测到 $x$ 后的 M.S.E.)。
>     * **总体 M.S.E. (使用 $X$)**： $E[Var(P|X)]$。
>         首先 $X$ 的边际 p.f. $f_1(x) = \int_0^1 g_1(x|p) f_2(p) dp = \int_0^1 \binom{40}{x} p^x (1-p)^{40-x} dp = \frac{40!}{x!(40-x)!} \frac{x!(40-x)!}{41!} = \frac{1}{41}$。
>         $X$ 在 $x=0, \dots, 40$ 上服从离散均匀分布。
>         $E[Var(P|X)] = \sum_{x=0}^{40} Var(P|x) f_1(x) = \sum_{x=0}^{40} \frac{(x+1)(41-x)}{75852} \cdot \frac{1}{41} = \dots = 0.003968$。
>     * **对比**：观测 $X$ 使 M.S.E. 从 $1/12 \approx 0.08333$ 降到了 0.003968，这是一个巨大的提升。

---

> [!summary] 4.7 总结
> * **$E(Y|x)$** 是一个**数值** (或 $x$ 的函数)，代表给定 $X=x$ 时 $Y$ 的均值。
> * **$E(Y|X)$** 是一个**随机变量**，它是 $E(Y|x)$ 这个函数代入 $X$ 后的结果。
> * **期望的全概率定律**：$E[E(Y|X)] = E(Y)$。
> * **最佳预测器 (最小 M.S.E.)**：预测 $Y$ 的最佳函数 $d(X)$ 是 $E(Y|X)$。
> * **方差的全概率定律**：$Var(Y) = E[Var(Y|X)] + Var[E(Y|X)]$。

---

## 4.8 效用 (Utility)

在统计决策中，我们总是在不确定的情况下选择行动。**效用 (Utility)** 是一种工具，它为我们选择的各种结果（收益和损失）分配一个“价值”。然后，我们选择能使**期望效用 (Expected Utility)** 最大化的行动。

### 效用函数

> [!example] 例子 4.8.1：赌局选择
>
> * **赌局 X**：$Pr(X=500) = 0.5$, $Pr(X=-350) = 0.5$。
>     * $E(X) = 0.5(500) + 0.5(-350) = 75$。
> * **赌局 Y**：$Pr(Y=40)=1/3$, $Pr(Y=50)=1/3$, $Pr(Y=60)=1/3$。
>     * $E(Y) = (40+50+60)/3 = 50$。
>
> $E(X) > E(Y)$，但一个赌徒可能更喜欢 Y，因为 Y 保证赚钱，而 X 有 50% 的几率损失 \$350。
>
> **效用理论**解释了这一点：人们最大化的不是 $E(X)$，而是 $E[U(X)]$，其中 $U$ 是他们的“效用函数”。

> [!definition] 定义 4.8.1：效用函数 (Utility Function)
>
> 效用函数 $U(x)$ 将一个**金额 $x$** (收益或损失) 映射为**一个数值 $U(x)$**，这个数值代表了该金额对决策者的**真实价值**或**满意度**。

> [!definition] 定义 4.8.2：最大化期望效用
>
> 一个理性的决策者会选择一个赌局（或行动） $X$，使其**期望效用 $E[U(X)]$ 最大**。
>
> * $E[U(X)] = \sum_x U(x) f(x)$

> [!example] 例子 4.8.2：计算期望效用
>
> 对于例 4.8.1 的赌局：
> * $E[U(X)] = \frac{1}{2} U(500) + \frac{1}{2} U(-350)$
> * $E[U(Y)] = \frac{1}{3} U(60) + \frac{1}{3} U(50) + \frac{1}{3} U(40)$
>
> 假设一个**风险规避 (risk-averse)** 的人，他对损失的“痛苦”远大于对同等收益的“快乐”。
> 比如书中的 $U(x) = x$ (损失时, $x<0$) 但 $U(x) = 100 \log(x+100) - 461$ (收益时, $x \ge 0$)。
> * $U(500) = 100 \log(600) - 461 \approx 216.9$
> * $U(-350) = -350$
> * $E[U(X)] = 0.5 (216.9) + 0.5 (-350) \approx -85.4$
> * $U(60) \approx 49.6$, $U(50) \approx 46.2$, $U(40) \approx 42.5$
> * $E[U(Y)] = \frac{1}{3}[49.6 + 46.2 + 42.5] \approx 46.1$
>
> **结论**：$E[U(Y)] (46.1) > E[U(X)] (-85.4)$，这个人会选择赌局 Y。

### 效用函数的形状

我们假设 $U(x)$ 是增函数（钱越多越好）。但它的形状（凹凸性）决定了人的风险偏好。

> [!example] 三种效用函数
>
> 设赌局 X：$Pr(X=-3)=0.5$, $Pr(X=2.5)=0.4$, $Pr(X=6)=0.1$。
> 设赌局 Y：$Pr(Y=-2)=0.3$, $Pr(Y=1)=0.4$, $Pr(Y=3)=0.3$。
> 选项 Z (不赌)：$Pr(Z=0)=1$。
>
> 1.  **例子 4.8.3：线性效用 (风险中性, $U(x)=ax+b$)**
>     * $E[U(X)] = a E(X) + b$。
>     * 决策等价于**最大化 $E(X)$**。
>     * $E(X) = 0.1$, $E(Y) = 0.7$, $E(Z) = 0$。
>     * $E(Y) > E(X) > E(Z)$。**选择 Y**。
>
> 2.  **例子 4.8.4：凸函数 (风险偏好, $U(x)=x^3$)**
>     * $E[U(X)] = 0.5(-3)^3 + 0.4(2.5)^3 + 0.1(6)^3 = 14.35$。
>     * $E[U(Y)] = 0.3(-2)^3 + 0.4(1)^3 + 0.3(3)^3 = 6.1$。
>     * $E[U(Z)] = U(0) = 0$。
>     * $E[U(X)] > E[U(Y)] > E[Z]$。**选择 X** (尽管 $E(X)$ 很低)。
>
> 3.  **例子 4.8.5：凹函数 (风险规避, $U(x)=\log(x+4)$)**
>     * $E[U(X)] = 0.5(\log 1) + 0.4(\log 6.5) + 0.1(\log 10) \approx 0.979$。
>     * $E[U(Y)] = 0.3(\log 2) + 0.4(\log 5) + 0.3(\log 7) \approx 1.436$。
>     * $E[U(Z)] = U(0) = \log(4) \approx 1.386$。
>     * $E[U(Y)] > E[U(Z)] > E[U(X)]$。**选择 Y**。如果 Y 不可选，会选择**不赌 (Z)** 而不是 X。

---
### 效用理论的应用

> [!example] 例子 4.8.6 & 4.8.7：出售彩票
>
> * **问题**：你有一张彩票 $X$， $Pr(X=36) = 1/4$, $Pr(X=0) = 3/4$。
>     $E(X) = (1/4)(36) = 9$ 美元。
> * **出售**：有人出价 $x_0$ 美元买你的彩票 (这是一个确定的收益)。
> * **决策**：你会出售，当且仅当：
>     $\text{确定收益的效用} > \text{赌局的期望效用}$
>     $U(x_0) > E[U(X)]$
>
> 1.  **例子 4.8.6 (风险偏好, $U(x)=x^2$)**
>     * $E[U(X)] = (1/4) U(36) + (3/4) U(0) = (1/4)(36^2) + 0 = 324$。
>     * $U(x_0) = x_0^2 > 324 \implies x_0 > 18$。
>     * **结论**：此人**不会**以低于 \$18 的价格出售 (尽管期望值只有 \$9)。
>
> 2.  **例子 4.8.7 (风险规避, $U(x)=\sqrt{x}$)**
>     * $E[U(X)] = (1/4) U(36) + (3/4) U(0) = (1/4)(\sqrt{36}) + 0 = 1.5$。
>     * $U(x_0) = \sqrt{x_0} > 1.5 \implies x_0 > 2.25$。
>     * **结论**：此人**愿意**以任何高于 \$2.25 的价格出售 (远低于 \$9 的期望值)。

> [!example] 例子 4.8.10：投资决策
>
> * **问题**：回到例 4.2.2，选择股票 1 还是 2？
> * **效用**：$U(x) = x^{0.8}$ (收益时)，$U(x) = x$ (损失时)。
> * **期望效用**：$E[U(X)] = \int_{-\infty}^0 x f(x) dx + \int_0^\infty (x)^{0.8} f(x) dx$。
>
> 1.  **股票 1**：$s_1=120$ 股，$R_1 \sim U[-10, 20]$ (p.d.f. $f(r) = 1/30$)。
>     $E[U(120 R_1)] = \int_{-10}^0 (120r) \frac{1}{30} dr + \int_0^{20} (120r)^{0.8} \frac{1}{30} dr = -12.6$
>
> 2.  **股票 2**：$s_2=200$ 股，$R_2 \sim U[-4.5, 10]$ (p.d.f. $f(r) = 1/14.5$)。
>     $E[U(200 R_2)] = \int_{-4.5}^0 (200r) \frac{1}{14.5} dr + \int_0^{10} (200r)^{0.8} \frac{1}{14.5} dr = 27.9$
>
> **结论**：股票 1 的期望效用是负的，股票 2 是正的。具有这种效用函数的人会**选择股票 2**。

---

> [!summary] 4.8 总结
> * **效用** $U(x)$ 是对收益 $x$ 的主观价值评估。
> * 理性决策者选择行动（赌局）$X$，以**最大化期望效用** $E[U(X)]$。
> * **线性效用** $U(x) = ax+b$ $\implies$ 风险中性，等价于最大化 $E(X)$。
> * **凹函数效用** $U(x) = \sqrt{x}$ 或 $\log(x)$ $\implies$ **风险规避** (Risk Averse)。
> * **凸函数效用** $U(x) = x^2$ $\implies$ **风险偏好** (Risk Seeking)。

---

## 4.9 补充练习 (Supplementary Exercises)

1.  证明：若 $E(X)$ 存在， $lim_{x\to\infty} x[1-F(x)] = 0$。
2.  证明：若 $Pr(X \ge 0) = 1$ 且 $E(X)$ 存在 (连续情况)， $E(X) = \int_0^\infty [1-F(x)] dx$。
3.  证明：练习 2 的结论在离散情况下也成立。
4.  $X, Y, Z \ge 0$ 且 $Pr(X+Y+Z \le 1.3) = 1$。证明 $X, Y, Z$ 的边际分布不可能都是 $U[0, 1]$。
5.  $Y = aX+b$。求 $a, b$ 使得 $E(Y)=0$ 且 $Var(Y)=1$。
6.  求 $U[0, 1]$ 样本量为 $n$ 的**极差** (Range) 的期望。
7.  $X, Y$ 的联合 p.d.f. $f(x, y) = x/36$ ( $0 < x < y < 6$ )。求 $E(Y-X)$ (预期收益)。
8.  $X_i \sim i.i.d.$ $f(x)=2x$ ( $0<x<1$ )。 $Y_n = \max\{X_1, \dots, X_n\}$。求 $E(Y_n)$。
9.  $m$ 是 $X$ 的中位数，$Y=r(X)$ 是单调函数。证明 $r(m)$ 是 $Y$ 的中位数。
10. $X_i \sim i.i.d.$ 连续分布，中位数为 $m$。 $Y_n = \max\{X_1, \dots, X_n\}$。求 $Pr(Y_n > m)$。
11. 卖可乐问题：需求 $f(x)$，利润 $g$，损失 $c$。求最大化期望净收益的最优订购量。
12. 机器故障问题：故障时间 $f(x)$，检查成本 $b$，故障后每小时成本 $c$。求最小化期望成本的最优检查时间。
13. $E(X)=3, E(Y)=1, Var(X)=4, Var(Y)=9$。$Z = 5X - Y + 15$。求 $E(Z)$ 和 $Var(Z)$ 在 (a) 独立, (b) 不相关, (c) $\rho = 0.25$ 时的值。
14. $X_0, \dots, X_n$ 独立同方差 $\sigma^2$。 $Y_j = X_j - X_{j-1}$。 $\bar{Y}_n = \frac{1}{n} \sum Y_j$。求 $Var(\bar{Y}_n)$。
15. $Var(X_i) = \sigma^2$，$ \rho(X_i, X_j) = \rho$ ( $i \ne j$ )。证明 $\rho \ge -1/(n-1)$。
16. $(X, Y)$ 在矩形上均匀分布。求 $\rho(X, Y)$。
17. 匹配问题： $n$ 封信随机放入 $n$ 个信封。求 $X$ (匹配正确数) 的方差。
18. 证明：3 阶中心矩 $E[(X-\mu)^3] = E(X^3) - 3\mu\sigma^2 - \mu^3$。
19. $c(t) = \log[\psi(t)]$ (累积量生成函数)。证明 $c'(0) = \mu$ 且 $c''(0) = \sigma^2$。
20. $E(Y|X)$ 是 $X$ 的线性函数。证明 $E(Y|X) = \mu_Y + \rho \frac{\sigma_Y}{\sigma_X} (X - \mu_X)$。
21. $E(Y|X) = 7 - (1/4)X$ 且 $E(X|Y) = 10 - Y$。求 $\rho(X, Y)$。
22. 3 英尺长的木棍在 $f(x)$ 处折断。求长短两段长度的相关性。
23. $Var(X)=Var(Y)=1, \rho > 1/2$。证明 $b = -1/(2\rho)$ 是唯一的 $b$ 使得 $\rho(X, X+bY) = \rho$。
24. 员工住A,B,C,D (0, 1, 3, 5)。(a) 最小化总距离 (MAE) 的办公室位置？ (b) 最小化总平方距离 (MSE) 的办公室位置？
25. $f(x, y) = 8xy$ ($0 < y < x < 1$)。观测到 $X=0.2$。(a) 求 $Y$ 的最小 M.S.E. 预测值。(b) 求 $Y$ 的最小 M.A.E. 预测值。
26. 证明： $Cov(X, Y) = E[Cov(X, Y | Z)] + Cov[E(X|Z), E(Y|Z)]$。
27. 抽样 $n$ 次， $X$ (有放回) $Y$ (无放回)。证明 $Pr(X=n) > Pr(Y=n)$。
28. $U(x) = x^2$ ($x \ge 0$)。证明该决策者总是宁愿赌 $X$ 也不愿接受确定的 $E(X)$。
29. $m$ 美元分配给 A (概率 $p$) 和 $A^c$。 $U(x) = \log x$。求最大化期望效用的分配比例 $a$。

