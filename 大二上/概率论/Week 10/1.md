# 概率论笔记：期望值 (Expectation)

> [!NOTE] 核心纲要
> 这一章我们开始讨论随机变量的**数字特征 (Numerical Characteristics)**。
>
> * **为什么需要数字特征？**
>     * 完整的分布（如 PMF/PDF）包含了全部信息，但太复杂了。
>     * 数字特征（如期望、方差）是用来描述分布的**关键摘要信息**。
> * **计算 vs. 理解：**
>     * 在今天，**计算**这些特征值已经不重要了。计算机、软件、甚至智能手机都能帮你算。
>     * **<font color="#F39C12">真正重要的是理解</font>**：
>         1.  这些数字**告诉了我们什么 (Interpretation)？**
>         2.  它们**不能告诉我们什么 (Limitations)？**
>     * 否则，你很容易被数据欺骗。

---

## 1. 数据的陷阱：相关性 vs. 因果性

在讨论期望之前，我们必须先理解一个最关键的统计陷阱：**<font color="#E74C3C">相关性不等于因果性 (Correlation does not imply causation)</font>**。

* **相关性 (Correlation)：** 两个变量在数据上呈现出一起变化（同增同减，或一增一减）的趋势。
* **因果性 (Causation)：** 一个变量的变化是**<font color="#E74C3C">导致</font>**另一个变量变化的原因。

> [!WARNING] 数据的误导
> 很多研究（甚至论文）的致命缺陷就是从相关性直接推出了因果性。很多时候，人们是**先有了结论，再用数据去解释这个结论**，而不是从数据中得到结论。
>
> 讲师：“我可以用同样的数据告诉你一个相反的结果，我根本不需要去造假，但是我可以选择你想听到的东西，然后告诉你。”

### <Note> 经典案例</Note>

1.  **<font color="#3498DB">巧克力与诺贝尔奖</font>**
    * **数据：** 有研究论文显示，一个国家的人均巧克力年消耗量 (X轴) 与该国诺贝尔奖获得者数量 (Y轴) 呈**<font color="#2ECC71">正相关</font>**。
    * **错误结论：** “多吃巧克力能让你得诺贝尔奖。”
    * **真相：** 这可能是由一个**<font color="#F39C12">隐藏的混淆变量</font>**（Confounding Variable）——如“国家富裕程度”——同时引起的。富裕国家有钱消费更多巧克力，也有钱投入科研。

2.  **<font color="#3498DB">冰淇淋销量与暴力犯罪率</font>**
    * **数据：** 冰淇淋的销量 (X轴) 与暴力犯罪率 (Y轴) 呈**<font color="#2ECC71">正相关</font>**。
    * **错误结论：** “吃冰淇淋导致暴力。”
    * **真相：** 混淆变量是“**<font color="#F39C12">天气</font>**”。天气热，冰淇淋卖得多；同时天气热，人也更烦躁，更容易发生冲突。

3.  **<font color="#3498DB">辛普森悖论 (Simpson's Paradox)</font>**
    * **数据：** 当把所有数据点放在一起看时，X 和 Y 呈**<font color="#2ECC71">正相关</font>** (X 增加, Y 增加)。
    * **真相：** 当把数据按某个变量（如不同组群）拆开看时，**<font color="#E74C3C">每一个</font>**单独的组群内部都呈**<font color="#E74C3C">负相关</font>** (X 增加, Y 减少)。
    * **结论：** 汇总数据得出的结论可能和分组数据得出的结论**<font color="#E74C3C">完全相反</font>**。

4.  **<font color="#3498DB">医院规模与死亡率</font>**
    * **数据：** 如果把所有医院数据放一起，会发现医院规模越大，死亡率越高。
    * **错误结论：** “大医院更危险，更容易死人。”
    * **真相：** 混淆变量是“**<font color="#F39C12">病人病情的严重程度</font>**”。
        * **校医院：** 死亡率几乎为 0，因为他们只看感冒发烧。
        * **三甲医院：** 死亡率高，因为**<font color="#E74C3C">只有会死的疾病你才会跑过去</font>**。

---

## 2. 期望 (Expectation)

**<font color="#3498DB">期望值 (Expected Value)</font>**，也称为**<font color="#3498DB">均值 (Mean)</font>**，是描述随机变量“中心趋势”或“平均值”的数字特征。

### 2.1 引入案例：投资决策

你现在有 $18，有两种选择：
1.  **存银行：** 年利率 5%，一年后价值 $18 \times (1 + 0.05) = 18.90$。你的**<font color="#2ECC71">确定收益</font>**是 $0.90。
2.  **买股票：** 一年后价值 $18 + X$，$X$ 是一个随机变量。

假设 $X$ 的概率分布 (PMF) 如下（根据讲座内容重构）：

| $x$ (收益) | -2 (亏2块) | 0 (不赚不亏) | 1 (赚1块) | 4 (赚4块) |
| :--- | :---: | :---: | :---: | :---: |
| $f(x)$ (概率) | 0.1 | 0.4 | 0.3 | 0.2 |

* **问题：** 你应该选哪个？
* **分析：** 我们需要计算股票收益 $X$ 的“平均预期收益”，即**<font color="#F39C12">期望值 $E(X)$</font>**。
* **计算：**
    $E(X) = \sum x \cdot f(x)$
    $E(X) = (-2)(0.1) + (0)(0.4) + (1)(0.3) + (4)(0.2)$
    $E(X) = -0.2 + 0 + 0.3 + 0.8 = 0.90$
* **结论：** 股票的**<font color="#F39C12">期望</font>**收益是 $0.90，和银行的**<font color="#2ECC71">确定</font>**收益完全一样。在这种情况下，两者（在数学期望上）没有区别。

---

## 3. 期望的正式定义与计算

### 3.1 <font color="#8E44AD">离散型 (Discrete Case)</font>

* **定义：**
    $$E(X) = \sum_{\text{all } x} x \cdot f(x)$$
    其中 $f(x)$ 是 $X$ 的 PMF。

* **期望的含义 (均值)：**
    为什么叫“均值”？想象一个实验重复 $n$ 次（$n$ 很大）。
    * $x_1$ 出现了 $n_1$ 次
    * $x_2$ 出现了 $n_2$ 次
    * ...
    * 所有结果的**<font color="#F39C12">算术平均值</font>**为： $\frac{x_1 n_1 + x_2 n_2 + \dots}{n} = \sum x_i \left(\frac{n_i}{n}\right)$
    * 根据大数定律（频率观），当 $n \to \infty$ 时，事件的**频率 $\frac{n_i}{n}$** 趋近于其**概率 $f(x_i)$**。
    * 因此，长期的算术平均值 $\to \sum x_i f(x_i) = E(X)$。

* **<font color="#E74C3C">存在性条件 (Condition for Existence)</font>：**
    * **有限情况：** 如果 $X$ 只能取有限个值，期望一定存在。
    * **无限情况：** 如果 $X$ 可以取可数无限个值（$x_1, x_2, \dots$），$E(X)$ 是一个**<font color="#E74C3C">无穷级数</font>**。
    * 该级数必须**<font color="#E74C3C">绝对收敛 (Absolutely Convergent)</font>**，即 $\sum |x_i| f(x_i) < \infty$。
    * **为什么？** 如果只是“条件收敛”，那么**<font color="#F39C12">改变求和的顺序</font>**（比如先加正的再加负的，或反之）**<font color="#F39C12">会导致完全不同的结果</font>**。期望值必须是一个唯一确定的数，不能依赖于你计算的顺序。
    * 如果不满足绝对收敛，我们说 $E(X)$ **不存在 (does not exist)**。

#### <font color="#1E8449">离散型示例</font>

1.  **伯努利 (Bernoulli)：** $P(X=1)=p, P(X=0)=1-p$
    $E(X) = (1)(p) + (0)(1-p) = p$

2.  **几何 (Geometric)：** (讲师版本：$x$ 为首次成功前的失败次数)
    $f(x) = p(1-p)^x$, for $x=0, 1, 2, \dots$
    $E(X) = \sum_{k=0}^\infty k \cdot p(1-p)^k = p(1-p) \sum_{k=1}^\infty k \cdot (1-p)^{k-1}$
    *使用微积分技巧：*
    已知 $\sum_{k=0}^\infty t^k = \frac{1}{1-t}$
    两边求导： $\sum_{k=1}^\infty k \cdot t^{k-1} = \frac{1}{(1-t)^2}$
    *代入 $t = 1-p$*
    $E(X) = p(1-p) \left[ \frac{1}{(1-(1-p))^2} \right] = p(1-p) \left[ \frac{1}{p^2} \right] = \frac{1-p}{p}$

3.  **泊松 (Poisson)：** $f(x) = \frac{e^{-\lambda} \lambda^x}{x!}$, for $x=0, 1, 2, \dots$
    $E(X) = \sum_{x=0}^\infty x \cdot \frac{e^{-\lambda} \lambda^x}{x!}$
    *第 $x=0$ 项为 0，从 $x=1$ 开始：*
    $E(X) = \sum_{x=1}^\infty x \cdot \frac{e^{-\lambda} \lambda^x}{x(x-1)!} = \sum_{x=1}^\infty \frac{e^{-\lambda} \lambda \cdot \lambda^{x-1}}{(x-1)!}$
    *提出常数 $\lambda e^{-\lambda}$，并令 $k = x-1$：*
    $E(X) = \lambda e^{-\lambda} \sum_{k=0}^\infty \frac{\lambda^k}{k!}$
    *根据泰勒级数， $\sum_{k=0}^\infty \frac{\lambda^k}{k!} = e^\lambda$：*
    $E(X) = \lambda e^{-\lambda} (e^\lambda) = \lambda$

---

### 3.2 <font color="#2E86C1">连续型 (Continuous Case)</font>

* **定义：**
    将求和 $\sum$ 替换为积分 $\int$：
    $$E(X) = \int_{-\infty}^{\infty} x \cdot f(x) \,dx$$
* **含义 (均值)：**
    $E(X)$ 可以被理解为 $X$ 的概率密度函数 $f(x)$ 下的“重心”或“加权平均值”。
    * $E(X) = \lim \sum x_i P(x_i \le X \le x_i + \Delta x)$
    * $\approx \sum x_i (f(x_i) \Delta x) \to \int x f(x) \,dx$

* **<font color="#E74C3C">存在性条件 (Condition for Existence)</font>：**
    * 积分必须**<font color="#E74C3C">绝对收敛</font>**。
    * $\int_{-\infty}^{\infty} |x| f(x) \,dx < \infty$
    * 如果不满足，则 $E(X)$ **不存在**。

#### <font color="#1E8449">连续型示例</font>

1.  **均匀分布 (Uniform)：** $X \sim U(a, b)$
    $f(x) = \frac{1}{b-a}$, for $x \in [a, b]$
    $E(X) = \int_a^b x \cdot \left(\frac{1}{b-a}\right) \,dx = \frac{1}{b-a} \left[ \frac{x^2}{2} \right]_a^b$
    $E(X) = \frac{1}{b-a} \left( \frac{b^2 - a^2}{2} \right) = \frac{(b-a)(b+a)}{2(b-a)} = \frac{a+b}{2}$
    *(期望值是区间的中点)*

2.  **指数分布 (Exponential)：** $f(x) = \lambda e^{-\lambda x}$, for $x \ge 0$
    $E(X) = \int_0^\infty x (\lambda e^{-\lambda x}) \,dx$
    *使用分部积分法 (Integration by Parts): $\int u \,dv = uv - \int v \,du$*
    * 令 $u = x$  $dv = \lambda e^{-\lambda x} \,dx$
    * 则 $du = dx$  $v = -e^{-\lambda x}$
    $E(X) = \left[ -x e^{-\lambda x} \right]_0^\infty - \int_0^\infty (-e^{-\lambda x}) \,dx$
    * $\left[ -x e^{-\lambda x} \right]_0^\infty = (0) - (0) = 0$ (使用洛必达法则求 $x \to \infty$ 的极限)
    $E(X) = 0 + \int_0^\infty e^{-\lambda x} \,dx = \left[ -\frac{1}{\lambda} e^{-\lambda x} \right]_0^\infty$
    $E(X) = (0) - \left( -\frac{1}{\lambda} e^0 \right) = \frac{1}{\lambda}$

3.  **正态分布 (Normal)：** $X \sim N(\mu, \sigma^2)$
    $E(X) = \int_{-\infty}^\infty x \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \,dx$
    *证明 $E(X) = \mu$：*
    *做代换 $t = \frac{x-\mu}{\sigma}$，则 $x = \mu + \sigma t$ 且 $dx = \sigma \,dt$*
    $E(X) = \int_{-\infty}^\infty (\mu + \sigma t) \cdot \frac{1}{\sigma\sqrt{2\pi}} e^{-t^2/2} \cdot (\sigma \,dt)$
    $E(X) = \int_{-\infty}^\infty (\mu + \sigma t) \cdot \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \,dt$
    *拆成两项：*
    $$E(X) = \mu \underbrace{\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \,dt}_{\text{= 1 (标准正态 PDF 积分为1)}} + \sigma \underbrace{\int_{-\infty}^\infty t \cdot \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \,dt}_{\text{= 0 (奇函数在对称区间积分为0)}}$$
    $E(X) = \mu \cdot 1 + \sigma \cdot 0 = \mu$

4.  **柯西分布 (Cauchy)：** (期望不存在的例子)
    $f(x) = \frac{1}{\pi(1+x^2)}$
    $E(X) = \int_{-\infty}^\infty x \cdot \frac{1}{\pi(1+x^2)} \,dx$
    *检查绝对收敛性：*
    $\int_{-\infty}^\infty \frac{|x|}{\pi(1+x^2)} \,dx = \frac{2}{\pi} \int_0^\infty \frac{x}{1+x^2} \,dx$
    *当 $x \to \infty$ 时，被积函数 $\approx \frac{x}{x^2} = \frac{1}{x}$。*
    *$\int \frac{1}{x} \,dx = \ln(x)$，在 $x \to \infty$ 时发散。*
    *由于该积分不绝对收敛，柯西分布的**<font color="#E74C3C">期望不存在</font>**。*

---

## 4. 随机变量函数的期望 (LOTUS)

> [!IMPORTANT] 核心问题
> 如果我们知道 $X$ 的分布 $f(x)$，我们想求 $Y = g(X)$ 的期望 $E(Y)$。

* **<font color="#F39C12">“清醒的”统计师 (Hard Way)</font>：**
    1.  找到 $Y$ 的 PDF/PMF，记为 $g_Y(y)$。 (这步通常很难)
    2.  使用 $Y$ 的定义计算：$E(Y) = \int y \cdot g_Y(y) \,dy$。

* **<font color="#2ECC71">“无意识的”统计师 (Easy Way / LOTUS)</font>：**
    * **L**aw **O**f **T**he **U**nconscious **S**tatistician
    * 你**<font color="#E74C3C">不需要</font>**求出 $Y$ 的分布！
    * 你可以直接在 $X$ 的分布上进行计算。

> [!NOTE] LOTUS 定理
> **<font color="#8E44AD">Discrete:</font>**
> $$E(g(X)) = \sum_{\text{all } x} g(x) \cdot f_X(x)$$
>
> **<font color="#2E86C1">Continuous:</font>**
> $$E(g(X)) = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \,dx$$
>
> **<font color="#D35400">Multivariate:</font>**
> $$E(g(X_1, X_2)) = \iint g(x_1, x_2) \cdot f(x_1, x_2) \,dx_1 \,dx_2$$

* **LOTUS 为什么是对的？(离散型证明)**
    $E(Y) = \sum_y y \cdot f_Y(y)$ (按 $Y$ 的定义)
    $\qquad = \sum_y y \cdot \left( \sum_{x \text{ s.t. } g(x)=y} f_X(x) \right)$ (把 $f_Y(y)$ 展开)
    $\qquad = \sum_y \sum_{x \text{ s.t. } g(x)=y} g(x) \cdot f_X(x)$ (因为 $y=g(x)$)
    $\qquad = \sum_{\text{all } x} g(x) \cdot f_X(x)$ (把按 $y$ 分组的 $x$ 重新组合)
* 这个定理告诉我们，即使我们“无意识地”直接把公式 $\int x f(x) dx$ 里的 $x$ 替换成 $g(x)$，得到的结果 $E(g(X)) = \int g(x) f(x) dx$ 居然是对的。

---

## 5. 期望的性质 (Properties of Expectation)

### <font color="#F39C12">1. 线性性质 (Linearity) - 最重要的性质</font>

* $E(aX + b) = aE(X) + b$
* **证明 (使用 LOTUS):**
    $E(aX+b) = \int (ax+b) f(x) \,dx$
    $\qquad = \int ax f(x) \,dx + \int b f(x) \,dx$
    $\qquad = a \int x f(x) \,dx + b \int f(x) \,dx$
    $\qquad = aE(X) + b(1)$
    $\qquad = aE(X) + b$

### <font color="#F39C12">2. 和的期望 (Expectation of a Sum)</font>

* $E(X_1 + X_2 + \dots + X_n) = E(X_1) + E(X_2) + \dots + E(X_n)$
* **<font color="#E74C3C">警告：</font>** 这个性质**<font color="#E74C3C">永远成立</font>**！它**<font color="#E74C3C">不需要</font>** $X_1$ 和 $X_2$ 相互独立！
* **证明 (n=2, 连续型):**
    $E(X_1 + X_2) = \iint (x_1 + x_2) f(x_1, x_2) \,dx_1 \,dx_2$ (by LOTUS)
    $\qquad = \iint x_1 f(x_1, x_2) \,dx_1 \,dx_2 + \iint x_2 f(x_1, x_2) \,dx_1 \,dx_2$
    $\qquad = \int x_1 \left( \int f(x_1, x_2) \,dx_2 \right) \,dx_1 + \int x_2 \left( \int f(x_1, x_2) \,dx_1 \right) \,dx_2$
    $\qquad = \int x_1 f_1(x_1) \,dx_1 + \int x_2 f_2(x_2) \,dx_2$ (括号内是边际 PDF)
    $\qquad = E(X_1) + E(X_2)$

### <font color="#F39C12">3. 一般线性组合 (General Linear Combination)</font>

* $E(a_1 X_1 + \dots + a_n X_n) = a_1 E(X_1) + \dots + a_n E(X_n)$

### <font color="#F39C12">4. 常数的期望 (Constant)</font>

* 如果 $P(X=c) = 1$，那么 $E(X) = c$。

### <font color="#F39C12">5. 单调性 (Monotonicity)</font>

* 如果 $P(X \ge a) = 1$ (即 $X$ 永远不会小于 $a$)，那么 $E(X) \ge a$。
* **证明：**
    $E(X) = \int_{-\infty}^\infty x f(x) \,dx = \int_a^\infty x f(x) \,dx$ (因为 $x < a$ 时 $f(x)=0$)
    *在积分区间 $[a, \infty)$ 内，$x \ge a$，所以：*
    $E(X) \ge \int_a^\infty a f(x) \,dx = a \int_a^\infty f(x) \,dx$
    *$\int_a^\infty f(x) \,dx = 1$ (总概率)*
    $E(X) \ge a$
