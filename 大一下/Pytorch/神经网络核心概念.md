# 🧠 神经网络核心概念学习笔记

> [!abstract] 学习导览
> 这份笔记将带你从最基础的“零件”开始，一步步组装并理解一个完整的、能够学习的神经网络。我们将遵循以下路径：
> **核心计算单元 -> 数学基础 -> 网络结构 -> “大脑”设定 -> 学习过程**

---

## 🧱 第一部分：最核心的计算单元

> [!info] ### 1. 神经元 (Neuron)：一个只负责一件事的“初级专家”
> **神经元 (Neuron)** 是神经网络中最基本的信息处理单元。你可以把它想象成一个只专注于一个微小特征的检测器。
>
> #### 它的工作流程：
> 1.  **接收信息 (Inputs)**: 从上一层获取所有输入信号（比如图片的像素值）。
> 2.  **加权处理 (Processing)**: 对接收到的信息进行一次核心计算：**`y = Wx + b`**，这里的 `W` (权重) 和 `b` (偏置) 是它自己独有的一套“评判标准”。
> 3.  **激活判断 (Activation)**: 将计算结果通过一个**激活函数**（如 `ReLU`），来决定自己是否要“发声”以及“声音多大”。
> 4.  **输出信号 (Output)**: 将最终的判断结果（一个数值）传递给下一层的所有神经元。

---

## ⚙️ 第二部分：神经网络的数学基础

> [!tip] ### 1. 核心运算：矩阵乘法 (`@`)
> - **`@` 是什么？**
>   `@` 是 Python 中用于**矩阵乘法 (Matrix Multiplication)** 的专用运算符。它遵循严格的线性代数规则，与 `*`（按元素相乘）完全不同。**看到 `@`，就理解为矩阵乘法**。

> [!question] ### 2. 输入数据的形状：`X` 的维度 `[1, 784]`
> 这描述了输入数据 `X` 是一个**有 1 行 和 784 列的矩阵**。
>
> - **`784` (列数) 代表什么？**
>   - 代表**一个样本的全部特征**。对于 MNIST 图片，就是将 `28x28` 的像素网格**展平 (Flatten)** 后得到的 **784** 个像素点。
>
> - **`1` (行数) 代表什么？**
>   - 代表**样本的数量**。`1` 表示我们这一次只处理 **1** 个样本（即 1 张图片）。
>   - **延伸 - 批处理 (Batch)**: 在实际训练中，我们通常一次处理一批数据以提高效率。如果一次处理 32 张图片，`X` 的维度就会是 **`[32, 784]`**。

> [!question] ### 3. 权重的维度：为什么是 `W1: [784, d1]`？
> > [!lightbulb] 矩阵乘法的黄金法则
> > 要计算 `A @ B`，A 的 **列数** 必须等于 B 的 **行数**。
>
> - **应用法则**:
>   - 计算过程是 `X @ W1`。
>   - `X` 的维度是 `[1, 784]`，有 **784 列**。
>   - 因此，`W1` 的维度**必须**以 **784 行**开始，即 `[784, ?]`。
> - **决定输出**:
>   - 输出的维度由 `A` 的行数和 `B` 的列数决定。
>   - `X` 的行数是 `1`，我们自定义 `W1` 的列数为 `d1`。
>   - 所以，输出 `H1` 的维度就是 `[1, d1]`。
>
> > [!example] 可视化过程
> > ```
> >        X         @        W1         =         H1
> >     [1, 784]           [784, d1]            [1, d1]
> >       ↑    |             |    ↑               ↑    ↑
> >       |    |_____________|    |               |    |
> >       |                      |               |    |
> >  1. 内侧维度必须匹配 (784 == 784)              |    |
> >                                              |    |
> >  2. 外侧维度决定了输出的维度 --------------------------
> > ```
>
> > [!success] 结论
> > `W1` 的维度 `[784, d1]` 是为了**将一个有 784 个原始特征的输入，转换成一个包含 `d1` 个新特征的输出**。

---

## 🏗️ 第三部分：神经网络的结构

> [!info] ### 1. 隐藏层 (Hidden Layer)：一个各司其职的“专家组”
> **隐藏层 (Hidden Layer)** 是由许多个神经元并排组成的一层。你可以把它想象成一个“专家委员会小组”，里面的每个“专家”（神经元）同时在看同一个输入，但各司其职，分别负责寻找不同的特征。
>
> > [!faq]- 为什么叫“隐藏”层？
> > 之所以叫“隐藏”，是因为我们只设计它的结构（比如有多少个神经元），但**不会指定每个神经元具体学什么特征**。它们的分工是在训练中**自动学习**的，其内部工作对我们是不可见的“黑盒”。
>
> > [!faq]- 为什么需要多个隐藏层？
> > 为了构建**层级化的特征**。第一层学习基础特征（边、角），第二层将基础特征组合成复杂特征（眼睛、鼻子），更深的层再进一步组合。**“深度学习”的“深度”就来源于此**。

> [!example] ### 2. 整体结构：一个信息加工流水线
> 1.  **输入层 (Input Layer)**: 公司的“前台”，只负责接收原始数据并传递。
> 2.  **隐藏层 (Hidden Layers)**: 公司的核心“分析部门”，负责层层分析和提炼信息。
> 3.  **输出层 (Output Layer)**: 公司的“决策CEO”，根据所有分析结果，做出最终的判断和预测。

---

## 🧑‍🏫 第四部分：神经网络的“大脑”设定

> [!abstract] ### 1. 参数 (Parameters) vs. 超参数 (Hyperparameters)
> 这是理解模型如何构建和学习的关键区别。
>
> | 特性     | **参数 (Parameters)** | **超参数 (Hyperparameters)** |
> | :------- | :----------------------------------------------- | :--------------------------------------------------------- |
> | **谁来设定？** | **模型自己**，在训练中自动学习和优化。           | **人类设计师**，在训练前手动设定。                         |
> | **作用？** | 是模型的“知识”，直接用来进行预测。             | 是模型的“配置”，用来控制其结构和训练过程。                 |
> | **例子？** | 权重 `W` 和 偏置 `b`。                           | 学习率、神经元数量、层数等。                               |

> [!warning] ### 2. `d1` (自定义神经元数量) 作为超参数
> `d1`，即第一个隐藏层的神经元数量，是一个典型的超参数。
>
> - **它是什么？** 它决定了网络第一层有多“宽”，以及模型在第一阶段能提取出的**基础模式的数量**。
> - **为什么是超参数？** 因为这个数值需要我们**在训练开始前就设计好**。模型不能在训练中途自己改变自己的结构。
> - **如何选择？(重要权衡)**
>     - **`d1` 大**: 学习能力强，但计算量大，且有**过拟合 (Overfitting)** 风险。
>     - **`d1` 小**: 计算速度快，但可能能力不足，导致**欠拟合 (Underfitting)**。
>     - 选择合适的 `d1` 是一个需要在模型容量和性能间权衡的**设计决策**。

---

## 🔄 第五部分：神经网络如何“学习”

> [!note] 学习的四步循环
> 网络结构和超参数只是一个空架子，让它变得“智能”的是**训练 (Training)** 过程。这个过程是一个不断循环的四步曲：
>
> 1.  **✅ 预测 (Guess)**: 模型根据当前的 `W` 和 `b`，对输入数据进行一次计算，做出一个猜测。
> 2.  **📉 比较 (Compare)**: 将猜测结果与“标准答案”比较，计算出**误差 (Loss)**。
> 3.  **🧠 学习 (Learn)**: 使用**反向传播 (Backpropagation)** 算法，追溯网络中的每一个 `W` 和 `b` 对这次错误应该“负多大的责任”。
> 4.  **🛠️ 调整 (Adjust)**: 使用**梯度下降 (Gradient Descent)** 算法，朝着能让误差减小的方向，对全网所有的 `W` 和 `b` 进行一次微小的调整。
>
> 这个 **“预测 → 比较 → 学习 → 调整”** 的循环会重复数百万次，每一次模型都会变得更精确一点，最终学会如何解决问题。