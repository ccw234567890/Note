---
tags:
  - PyTorch
  - AI
  - Tensor
creation-date: 2025-07-24
---

# 🧩 PyTorch 核心操作：张量的合并与分割

在 PyTorch 中，高效地组合与拆分数据是日常操作的基础。这篇笔记总结了四种最核心的函数：`torch.cat`、`torch.stack`、`torch.split` 和 `torch.chunk`。

---

## Part 1: 🧲 合并张量 (Merge Tensors)

合并操作用于将多个小张量组合成一个大张量。

### ➤ `torch.cat()` - 拼接

> [!NOTE] 功能：沿一个**已存在**的维度进行拼接（Concatenate）。
> 可以想象成把几叠文件首尾相连，文件本身（其他维度）的尺寸需要对齐。

> [!success] ✅ 核心规则
> - **不创建新维度**：输出张量的维度数与输入张量相同。
> - **维度匹配**：除了被拼接的维度 `dim` 之外，其他所有维度的大小必须完全一致。

> [!EXAMPLE] 示例：合并班级数据
> ```python
> # 假设 a 是 1-4 班数据, b 是 5-8 班数据
> # Shape: [班级数, 学生数, 成绩数]
> a = torch.rand(4, 32, 8)
> b = torch.rand(5, 32, 8)
> 
> # 沿第 0 维（班级维度）拼接
> c = torch.cat([a, b], dim=0)
> 
> # c.shape -> torch.Size([9, 32, 8])
> # 4 个班 + 5 个班 = 9 个班
> ```
> 在这个例子中，`dim=0` 是班级维度，我们成功地将其扩展了。而 `dim=1` (32) 和 `dim=2` (8) 保持不变，因为它们是匹配的。

> [!fail] ❌ 常见错误
> 如果非拼接维度的尺寸不匹配，操作会失败。
> ```python
> a1 = torch.rand(4, 3, 32, 32)
> a2 = torch.rand(4, 1, 32, 32) # 第 1 维尺寸不同
> 
> # torch.cat([a1, a2], dim=0) -> ❌ RuntimeError!
> # 除了 dim=0，其他维度也必须匹配，但这里 dim=1 的 3 和 1 不匹配。
> ```

### ➤ `torch.stack()` - 堆叠

> [!NOTE] 功能：**创建一个新维度**，并沿这个新维度堆叠张量。
> 可以想象成把几本书（2D）摞在一起，形成一叠书（3D）。

> [!success] ✅ 核心规则
> - **创建新维度**：输出张量的维度数会比输入张量**多一**。
> - **形状严格一致**：所有输入张量的形状必须**完全相同**。

> [!EXAMPLE] 示例：创建批次维度
> ```python
> # a 和 b 是两个独立的样本，形状完全相同
> a = torch.rand(32, 8)
> b = torch.rand(32, 8)
> 
> # 沿新的第 0 维堆叠，形成一个 batch
> c = torch.stack([a, b], dim=0)
> 
> # c.shape -> torch.Size([2, 32, 8])
> # 新增的 dim=0 的大小为 2，代表堆叠了 2 个张量
> ```

### `cat` vs `stack` 对比速查表

> [!TIP] `cat` v.s. `stack` 核心区别
> 假设有两个形状为 `(32, 8)` 的张量 `a` 和 `b`。
> 
> | 操作 | `torch.cat([a, b], dim=0)` | `torch.stack([a, b], dim=0)` |
> | :--- | :--- | :--- |
> | **维度变化** | 维度不变 | 维度 +1 |
> | **工作方式** | 扩展现有维度 | 创建新维度 |
> | **结果形状** | `(64, 8)` | `(2, 32, 8)` |

---

## Part 2: 🔪 分割张量 (Split Tensors)

分割操作用于将一个大张量拆分成多个小张量。

### ➤ `torch.split()` - 按长度分割

> [!NOTE] 功能：根据指定的**块长度 (`split_size_or_sections`)** 来分割张量。
> 你可以精确控制每一块的大小。

> [!success] ✅ 核心规则
> - **按长度定义**：第二个参数可以是**整数**（每块的固定长度）或**列表**（每块的具体长度）。
> - **返回元组**：函数返回一个包含所有分割后张量的元组。

> [!EXAMPLE] 示例：精确分割
> ```python
> c = torch.rand(2, 32, 8)
> 
> # --- 使用列表精确分割 ---
> # 沿 dim=0 (长度为2)，分割成长度为 [1, 1] 的两块
> aa, bb = c.split([1, 1], dim=0)
> # aa.shape -> torch.Size([1, 32, 8])
> # bb.shape -> torch.Size([1, 32, 8])
> 
> # --- 使用整数固定长度分割 ---
> # 沿 dim=0，每块长度为 1
> result_tuple = c.split(1, dim=0)
> # len(result_tuple) -> 2
> # result_tuple[0].shape -> torch.Size([1, 32, 8])
> ```

> [!fail] ❌ 常见错误
> `ValueError: not enough values to unpack`
> ```python
> c = torch.rand(2, 32, 8)
> 
> # 期望得到两个变量，但操作只产生一个结果
> aa, bb = c.split(2, dim=0) # ❌ ValueError!
> ```
> **原因**：`c.split(2, dim=0)` 的意思是“每块长度为2”。因为 `dim=0` 的总长度就是2，所以只会产生**一个**张量，即 `c` 本身。但代码试图将这一个返回值解包赋给两个变量 `aa, bb`，因此出错。

### ➤ `torch.chunk()` - 按数量分割

> [!NOTE] 功能：根据指定的**块数量 (`chunks`)** 来分割张量。
> 你告诉 PyTorch 想要几块，它会自动帮你计算每块的大小，并尽量均分。

> [!success] ✅ 核心规则
> - **按数量定义**：第二个参数是**整数**，代表你希望得到的块的总数。
> - **自动计算**：PyTorch 会自动处理长度，即使不能整除，最后一块会变小。

> [!EXAMPLE] 示例：均等分割
> ```python
> c = torch.rand(2, 32, 8)
> 
> # 沿 dim=0，分割成 2 块
> aa, bb = c.chunk(2, dim=0)
> # PyTorch 自动计算每块长度为 2 / 2 = 1
> # aa.shape -> torch.Size([1, 32, 8])
> # bb.shape -> torch.Size([1, 32, 8])
> ```

### `split` vs `chunk` 对比速查表

> [!TIP] `split` v.s. `chunk` 核心区别
> 假设有一个形状为 `(10, 8)` 的张量 `a`。
> 
> | 操作 | `a.split(3, dim=0)` | `a.chunk(3, dim=0)` |
> | :--- | :--- | :--- |
> | **参数含义** | 每块长度为 **3** | 分割成 **3** 块 |
> | **结果块数** | 4 块 (3, 3, 3, 1) | 3 块 (4, 3, 3) |
> | **关注点** | **每块多长** | **总共几块** |