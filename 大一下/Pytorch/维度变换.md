# PyTorch 核心张量操作学习笔记

## 概览：四种核心操作

> [!abstract] 课程目录
> - **View / reshape**: 改变张量的形状
> - **Squeeze / unsqueeze**: 压缩或增加维度
> - **Transpose / t / permute**: 交换维度
> - **Expand / repeat**: 扩展或重复张量

---

## 第一部分：张量形状变换

### 主题：张量变形 `View` / `Reshape`

> [!note] 功能说明
> 使用 `.view()` 或 `.reshape()` 可以快速改变张量的形状，但不会改变其中元素的总数。

> [!example] 代码示例
> ```python
> # a 的原始形状: (4, 1, 28, 28)
> a = torch.rand(4, 1, 28, 28)
> 
> # 将每张 1x28x28 的图像"压平"成一个 784 维的向量
> # a 的新形状: (4, 784)
> b = a.view(4, 28*28) 
> ```

> [!warning] 注意：潜在的逻辑问题
> - **丢失维度信息 (Lost dim information)**: 将 `(4, 1, 28, 28)` 变形为 `(4, 784)` 后，原有的通道、高度、宽度等结构信息就丢失了。
> - **逻辑错误 (Logic Bug)**: 例子中，`b.view(4, 28, 28, 1)` 虽然元素数量正确，但其维度顺序 `(4, 28, 28, 1)` 和原始的 `a` 张量 `(4, 1, 28, 28)` 不同，这可能导致后续运算出错。

> [!danger] 核心限制：元素总数必须严格匹配
> `view()` 操作必须遵守 **元素总数不变** 的原则。
> ```python
> # 原始张量 a 的元素总数是 4 * 1 * 28 * 28 = 3136
> 
> # 尝试 view 成 4 * 783 = 3132 个元素，会直接报错
> a.view(4, 783) 
> # RuntimeError: shape '[4, 783]' is invalid for input of size 3136
> ```

### 主题：`Squeeze` vs `Unsqueeze` (挤压与解压)

> [!quote] 形象比喻
> - 🍋 **Squeeze (挤压)**: 像挤柠檬一样，**移除** 张量中大小为 1 的维度。
> - 🎁 **Unsqueeze (解压)**: 像打开礼物盒一样，**增加** 一个大小为 1 的新维度。

### 操作：`unsqueeze()` 详解 (增加维度)

> [!tip] 功能与索引
> - **功能**: `unsqueeze(dim)` 在指定的位置 `dim` 增加一个大小为 1 的维度。
> - **索引**: 对于一个 N 维张量，`dim` 的有效范围是 `[-N-1, N]`。正索引从前数，负索引从后数。

> [!example] 代码示例
> ```python
> # a 的形状: (4, 1, 28, 28)
> 
> # 在最前面 (索引0) 增加维度
> a.unsqueeze(0).shape # -> torch.Size([1, 4, 1, 28, 28])
> 
> # 在最后面 (索引-1) 增加维度
> a.unsqueeze(-1).shape # -> torch.Size([4, 1, 28, 28, 1])
> 
> # 链式操作：(32) -> (1, 32, 1, 1)
> b = torch.rand(32)
> b.unsqueeze(1).unsqueeze(2).unsqueeze(0).shape # -> torch.Size([1, 32, 1, 1])
> ```

### 操作：`squeeze()` 详解 (移除维度)

> [!info] 两种用法
> 1. `squeeze()`: 不带参数，移除 **所有** 大小为 1 的维度。
> 2. `squeeze(dim)`: 带参数，只在 **指定位置 `dim`** 的维度大小为 1 时才移除它。

> [!example] 代码示例
> ```python
> # b 的形状: (1, 32, 1, 1)
> b = torch.rand(1, 32, 1, 1)
> 
> # 移除所有大小为1的维度
> b.squeeze().shape # -> torch.Size([32])
> 
> # 只移除索引0的维度
> b.squeeze(0).shape # -> torch.Size([32, 1, 1])
> 
> # 尝试移除索引1的维度（大小为32），形状不变
> b.squeeze(1).shape # -> torch.Size([1, 32, 1, 1])
> ```

---
## 第二部分：张量扩展与维度交换

### 主题：`Expand` vs `Repeat` 的核心区别

> [!summary] 一句话总结
> `expand` 是高效的“视图”广播，不复制数据；`repeat` 是“物理”上的复制，会创建新的数据副本。

> [!note] `expand()` - 广播机制
> - **原理**: 基于广播 (Broadcasting)，不实际复制数据，非常节省内存。
> - **规则**: 只能扩展大小为 1 的维度。
> - **参数**: 目标形状。使用 `-1` 表示该维度大小不变。

> [!caution] `repeat()` - 内存拷贝
> - **原理**: 在内存中 **真正地复制数据** 来填充，会消耗更多内存。
> - **规则**: 将张量沿各个维度 **重复指定的次数**。
> - **参数**: 每个维度的 **重复倍数**。

### 操作：`expand()` 和 `repeat()` 示例

> [!example] `expand()` 代码示例
> ```python
> # b 的形状: (1, 32, 1, 1)
> b = torch.rand(1, 32, 1, 1)
> 
> # 扩展大小为1的维度，大小为32的维度保持不变
> b.expand(4, 32, 14, 14).shape # -> torch.Size([4, 32, 14, 14])
> ```

> [!example] `repeat()` 代码示例
> ```python
> # b 的形状: (1, 32, 1, 1)
> b = torch.rand(1, 32, 1, 1)
> 
> # 将第0维重复4次，第1维重复32次
> # 新形状: (1*4, 32*32, 1*1, 1*1)
> b.repeat(4, 32, 1, 1).shape # -> torch.Size([4, 1024, 1, 1])
> ```

### 主题：维度交换 `transpose` & `permute`

> [!note] 功能对比
> - **`.t()`**: `transpose(0, 1)` 的简写，**仅适用于2D张量**。
> - **`transpose(dimA, dimB)`**: 交换 **任意两个** 指定的维度。
> - **`permute(dims)`**: 最强大的方法，可以 **一次性重排所有** 维度。

> [!example] 代码示例
> ```python
> # b 的形状: (4, 3, 28, 32)
> b = torch.rand(4, 3, 28, 32)
> 
> # 交换索引 1 和 3 的维度
> b.transpose(1, 3).shape # -> torch.Size([4, 32, 28, 3])
> 
> # 将维度 (0,1,2,3) 重排为 (0,2,3,1)
> b.permute(0, 2, 3, 1).shape # -> torch.Size([4, 28, 32, 3])
> ```

### 核心重点：内存连续性 (`contiguous`)

> [!bug] 问题根源：`transpose` / `permute` 导致内存不连续
> `transpose` 或 `permute` 操作通常只修改张量的元数据（步长 stride），而不会移动内存中的数据。这会导致张量在内存中的布局变得 **不连续 (non-contiguous)**。
> 而 `.view()` 等操作要求张量必须是内存连续的，因此直接调用会报错。

> [!danger] 错误演示
> ```python
> a = torch.rand(4, 3, 32, 32)
> 
> # transpose 使 a 内存不连续，再调用 view 会失败
> a.transpose(1, 3).view(4, 3*32*32) 
> # RuntimeError: view size is not compatible with input tensor's size and stride...
> ```

> [!success] 正确的解决方案
> 在进行 `view` 操作前，调用 **`.contiguous()`** 方法，它会返回一个内存连续的新张量。
> ```python
> a = torch.rand(4, 3, 32, 32)
> 
> # 调用 .contiguous() 解决问题
> a.transpose(1, 3).contiguous().view(4, 3*32*32) # -> OK
> ```

---
# 深入理解PyTorch内存布局：`contiguous`, `transpose`与`view`的核心

> [!abstract] 内容提要
> 这篇笔记将深入探讨一个在 PyTorch 中非常常见但又容易混淆的问题：为什么 `transpose()` 之后调用 `.view()` 会报错？我们将从最基本的内存概念出发，层层递进，彻底搞懂其背后的工作原理。
> 1.  **基本概念**：内存、元数据 (Metadata)
> 2.  **核心机制**：步长 (Stride) 与 `transpose` 的工作原理
> 3.  **问题根源**：为什么 `.view()` 会失败以及如何用 `.contiguous()` 解决

---

## 第1部分：基本概念：内存与元数据

> [!note] 🧠 什么是内存 (Memory)？
> 你可以把计算机的内存（RAM）想象成一条非常长的、一维的街道。
> -   街道上的每个房子都有一个唯一的门牌号，这就是 **内存地址**。
> -   每个房子里可以存放东西，这就是 **数据**。
>
> 
>无论你的数据结构（如二维矩阵）多么复杂，最终都必须被“压平”成一维序列，存放在这条街道上。

> [!info] 📝 什么是张量 (Tensor) 与元数据 (Metadata)？
> 当你创建一个张量时，PyTorch 会做两件事：
> 1.  **存储数据**：在内存“街道”上申请一块连续的空间，把张量的所有元素（如 `[1, 2, 3, 4, 5, 6]`）按顺序放进去。
> 2.  **创建元数据**：创建一份“说明书”来描述这块数据。这份说明书就是**元数据 (Metadata)**，即“关于数据的数据”。
> 
> 元数据主要包含以下信息：
> -   **Data Pointer**: 指向数据在内存中的起始地址（第一个元素的“门牌号”）。
> -   **Shape (形状)**: 描述张量的逻辑维度，例如 `(2, 3)`。
> -   **Stride (步长)**: **理解本问题的关键**，定义了在内存中如何移动来寻找下一个元素。
> -   **Dtype (数据类型)**: 例如 `torch.float32`。

---

## 第2部分：核心机制：步长(Stride)与`transpose`

> [!tip] 👣 深入理解步长 (Stride)
> **定义**：在某个维度上，从一个元素移动到下一个元素，需要在物理内存中“跳”过多少个位置。
> 
> **示例**：对于张量 `a = torch.tensor([[1, 2, 3], [4, 5, 6]])`，其物理内存为 `[1, 2, 3, 4, 5, 6]`。
> -   **维度0 (行)**: 从 `a[0,0]`(1) 到 `a[1,0]`(4)，需要跳 **3** 个位置。所以维度0的步长是 **3**。
> -   **维度1 (列)**: 从 `a[0,0]`(1) 到 `a[0,1]`(2)，需要跳 **1** 个位置。所以维度1的步长是 **1**。
> 
> 因此，张量 `a` 的步长是 `(3, 1)`。
> 当一个张量的物理内存布局和其步长满足这种标准关系时，我们称之为 **内存连续的 (Contiguous)**。

> [!example] `transpose` 操作的真相
> 当我们执行 `b = a.transpose(0, 1)` 时，**物理内存中的数据 `[1, 2, 3, 4, 5, 6]` 根本没有动！**
> `transpose` 仅仅是交换了元数据中的 `shape` 和 `stride`：
> 
> | 属性 | 原始 `a` | `transpose` 后的 `b` |
> | :--- | :--- | :--- |
> | **Shape** | `(2, 3)` | `(3, 2)` |
> | **Stride** | `(3, 1)` | `(1, 3)` |
> 
> `b` 的逻辑视图是 `[[1, 4], [2, 5], [3, 6]]`，它的新步长 `(1, 3)` 完美地描述了如何在**未改变的旧内存**上找到这些元素。

> [!warning] 警告：内存变得不连续 (Non-Contiguous)
> `transpose` 之后，张量 `b` 虽然逻辑上有效，但其内存布局已经不再是标准的“连续”状态。
> -   一个新创建的、连续的 `(3, 2)` 张量，其步长应该是 `(2, 1)`。
> -   而 `b` 的步长是 `(1, 3)`。
> 
> 两者不匹配，所以 `b` 是 **不连续的**。

---

## 第3部分：问题根源与解决方案

> [!question] ❓ 为什么 `.view()` 会在这种情况下失败？
> `.view()` 是一个纯元数据操作，为了保证极高的执行效率，它也有一个前提假设：**被操作的张量必须是内存连续的**。
> 
> 当你对不连续的张量 `b` 调用 `.view()` 时，`.view()` 会发现张量的物理内存顺序 (`1, 2, 3, 4, 5, 6`) 与其逻辑顺序 (`1, 4, 2, 5, 3, 6`) 不一致。如果强行操作，会产生错误的数据。

> [!danger] 错误演示与根源
> 为了防止数据被破坏，PyTorch 在执行 `.view()` 前会检查内存是否连续。当检查到不连续时，它会立即报错。
> ```python
> # a 是连续的
> a = torch.rand(4, 3, 32, 32)
> 
> # b = a.transpose(1, 3) 之后，b 变得不连续
> b = a.transpose(1, 3)
> 
> # 对不连续的 b 调用 view 会失败！
> b.view(4, 3*32*32) 
> # RuntimeError: view size is not compatible with input tensor's size and stride...
> ```

> [!success] ✅ 解决方案：`.contiguous()`
> `.contiguous()` 的作用就是解决这个问题。它会：
> 1.  **开辟一块全新的内存空间**。
> 2.  **按照原张量的逻辑顺序，将数据一个个地复制到新内存中**。
> 3.  **返回一个内存连续的、全新的张量副本**。
> 
> 这个新副本就可以安全地交给 `.view()` 使用了。
> ```python
> # 在调用 view 之前，先调用 .contiguous()
> # contiguous() 会返回一个内存连续的新张量
> contiguous_b = b.contiguous()
> 
> # 现在可以安全地 view 了
> contiguous_b.view(4, 3*32*32) # OK!
> 
> # 链式调用
> a.transpose(1, 3).contiguous().view(4, 3*32*32) # 同样 OK!
> ```