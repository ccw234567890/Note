![](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202510051101044.png)

这张 **Table 2** 在回答两个方法学问题：  
1）**用什么注意力结构最适合“读网页 HTML”？**  
2）**预训练时用哪种“span 去噪(denoising)”混合更有效？**

我把它分成左右两半来讲。

---

# 左半：不同注意力结构的效果（MiniWoB++，12K 示范，平均 56 个任务）

**怎么看表头**

- **Architectures / Attention Type**：对同一模型规模（Base）尝试不同注意力：
    
    - **Dense**（致密全连接注意力，Flan-T5-Base）
        
    - **Local**（局部窗口注意力，Long-T5-Base）
        
    - **Local & Global**（局部 + 少量全局锚点，Long-T5-Base）
        
- **L = 2048 / 4096**：输入长度上限（上下文窗口）。
    
- 单元格数值：平均**成功率(%)**。
    

**读数要点**

- **Dense → Local**：把注意力改成**局部窗口**，成功率从 **34% → 43～44%**，已经明显涨。
    
- **Local → Local & Global**：再加入**全局锚点**（比如标题、表单/按钮等关键 token 拿到“全球视野”），成功率继续涨到 **53%+**。
    
- **L 从 2048 到 4096**：窗口加大，收益很小（都只+0.x%）。说明**结构化注意力**比单纯拉长上下文更关键。
    
- 作者在文中总结：**Local & Global** 相比 **Dense**，**提升超 18 个点**；这与 HTML 的**层级/邻域性**相吻合：局部窗口擅长看父子/兄弟节点，全局锚点负责长距离引用（如 label-for、aria 关系、导航到内容）。
    

> 结论（左半）：**“局部 + 全局”的 Long-T5 风格注意力**最适合做 HTML 编码器；单纯堆上下文窗口不是解法。

---

# 右半：HTML 去噪预训练（Span Length μ 的混合）

**怎么看表头**

- **Span Length μ**：掩蔽/删除的连续片段平均长度（可以混合多种 μ），让模型“填空”。
    
- 两列指标：
    
    - **real-estate**：真实房产域的**任务成功率**；
        
    - **MiniWoB++**：离线基准上的**平均成功率**。
        
- 顶部 baseline **(no HTML-denoising)** 是**不做 HTML 去噪**的预训练结果。
    

**读数要点**（抓规律，不死背每个数）

- 做了 **HTML 去噪** 的所有配置都**比不做**要好，说明**面向 HTML 的预训练**是有效的。
    
- **更长的 span** 配置更强：
    
    - 例如仅用 **μ={8,64}** 往往是**最优/近最优**；
        
    - 带有很多**短 span（μ=3）** 的混合（哪怕再加 Prefix-LM 目标）**略逊**，因为太短容易退化成 n-gram 预测，难以学到**结构偏置**。
        
- **Prefix 目标**（在“3,8,64,Prefix”等行）能一定程度抑制短 span 的副作用，但整体仍不如“以**中长 span 为主**”的方案。
    

> 结论（右半）：**HTML-denoising 要“掩长不掩短”**——以 **μ≈8～64** 的长跨度去噪为主，必要时再少量搭配其他 μ；这能逼着模型利用 DOM 结构和远距线索去“补全”，学到**网页的层级/对齐关系**。

---

# 实操启示（把表格落到可复现的选型）

1. **编码器**：用 **Long-T5 式 Local & Global 注意力**（而不是 Dense），上下文够用即可，不必盲目加窗。
    
2. **预训练**：做 **HTML 专项去噪**，推荐以 **μ ∈ {8,64}** 为核心的混合；若要加短 span 或 Prefix-LM，比例要低，防止学成“词缝补丁”。
    
3. **为什么这套更适配 HTML**：
    
    - **局部**捕捉父/子/兄弟节点与局部表单结构；
        
    - **全局**连接 label↔input、导航↔内容、弹窗↔触发器等长距离依赖；
        
    - **长跨度去噪**迫使模型依靠结构而非近邻上下文，形成**HTML 结构偏置**。
        

一句话总括：  
**结构化注意力 + 长跨度 HTML 去噪** = 把“网页是棵树”的 inductive bias 学进模型里，远比“全连接注意力 + 拓大窗口”更有效。