# 算法：分形网络 (FractalNets)

**标签**: #DeepLearning #NetworkArchitecture #ResNetAlternative #Historical

> [!info] 核心思想
> **分形网络（FractalNets）** 是一种不依赖[[残差连接]]，但依然能够有效训练极深神经网络的架构。它的核心思想是通过**递归（recursion）**和**自相似（self-similarity）**的分形结构来组织网络层。
>
> 这种分形结构天然地创造了一个包含**指数级数量、不同长度路径**的网络。
>
> 想象一下**一颗树**的结构：
> - 从树根到树梢最高处的叶子，是一条**很长很深**的路径。
> - 从树根到主干上分叉出的一片叶子，是一条**很短很浅**的路径。
>
> 一颗树天然地包含了从短到长的所有路径。FractalNet 的设计理念与此类似，它构建的不是一条“单行道”，而是一个“茂密的丛林”，信息和梯度可以在其中通过无数长短不一的路径进行传播。

---

## 1. 核心哲学：结构隐喻而非显式添加

FractalNet 与 [[残差网络 (ResNet)]] 在解决深度网络训练难题上走了两条不同的路：

- **[[残差网络 (ResNet)]] 的方法**: **显式添加 (Explicit Addition)**。
    - 它从一个标准的顺序网络出发，然后**“手动”添加**一条[[快捷连接 (Shortcut Connection)|快捷连接]]，来创建一个明确的、无损的信息通路。这像是在一条拥堵的普通街道旁，专门修了一条“高速公路”。

- **FractalNet 的方法**: **结构隐喻 (Implicit Structure)**。
    - 它不添加任何额外的连接，而是将网络本身设计成一个**“宽厚”的、内在就包含多种路径**的结构。短路径是其分形设计的一个**自然涌现的属性 (emergent property)**，而不是后加的。

---

## 2. 分形块的构建

FractalNet 是通过递归规则来构建的。我们定义一个基础的分形块 $F_C(x)$，其中 $C$ 代表分形的“深度”或“列数”。

- **基础情况 ($C=1$)**:
    - $F_1(x)$ 是一个最简单的计算单元，例如一个**卷积层**（后面可接 BN 和 ReLU）。
    - `F_1(x) = ConvBlock(x)`

- **递归规则 ($C > 1$)**:
    - 一个深度为 $C$ 的分形块 $F_C(x)$，由**两个**深度为 $C-1$ 的分形块 $F_{C-1}(x)$ 串联，并与一个“连接”（Join）操作组成。
    - `F_C(x) = join(x, F_{C-1}(F_{C-1}(x)))`

**连接操作 (Join Operation)**:
- 连接操作负责将不同路径的输出进行融合。在 FractalNet 中，这通常是**逐元素的均值 (element-wise mean)**。

**一个深度为 F_3 的分形块的结构**:
1.  它由两个 F_2 块串联和一个连接操作组成。
2.  每个 F_2 块又由两个 F_1 块串联和一个连接操作组成。
3.  F_1 块就是一个卷积层。

![FractalNet Block Structure](https://i.stack.imgur.com/G5lO5.png)
*上图：一个深度为 F_4 的分形块的递归展开结构。可以看到，从输入到输出存在多条长短不一的路径。*

---

## 3. 工作机制与训练方法

### A. 隐式的网络集合 (Implicit Ensemble)
- 由于其分形结构，一个 FractalNet 内部隐式地包含了 $2^{(C-1)}$ 条不同的子路径。
- 在前向传播时，输入数据会流经所有这些路径，最终的输出是所有路径结果的平均。这相当于同时运行了**一个由多个不同深度网络构成的集合（ensemble）**，从而获得了强大的性能和泛化能力。

### B. “丢弃路径” (Drop-Path) 训练
- 为了进一步增强正则化效果并模拟 [[Stochastic Depth]] 的思想，FractalNet 在训练时引入了**“丢弃路径”（Drop-Path）**。
- 在处理每个训练样本时，随机地从网络中选择**一部分路径**来激活，而禁用其他路径。主要有两种丢弃方式：
    1.  **局部丢弃 (Local Drop-Path)**: 在每个连接（Join）点，随机选择一条输入路径（例如，只选择两个子路径中的一个，或只选择跳跃连接），并禁用其他的。
    2.  **全局丢弃 (Global Drop-Path)**: 从整个网络的所有可能路径中，**随机采样一条完整的路径**来进行前向和反向传播。这意味着在某次迭代中，网络可能退化成一个非常浅的普通网络。

### C. 推理 (Inference)
- 在测试和推理阶段，**所有的路径都会被激活**，不再有任何随机丢弃。
- 整个网络作为一个确定的、庞大的集合模型进行预测，从而获得最佳性能。

---

## 4. 历史意义与遗产

- **提供了新视角**: FractalNet 雄辩地证明了，ResNet 的显式残差连接**并非**解决深度网络训练难题的**唯一途径**。核心在于**保证短路径的存在**，以实现有效的梯度传播。
- **强化了集合思想**: 与 [[Stochastic Depth]] 一样，它进一步验证了“训练一个由不同深度子网络构成的隐式集合”是一种非常有效的正则化策略。
- **为什么没有 ResNet 流行？**
    - **复杂性**: FractalNet 的递归结构比 ResNet 简单的线性叠加要**复杂得多**，无论是理解还是代码实现都更困难。
    - **效率**: ResNet 的结构更适合现代 GPU 的并行计算，通常在训练和推理时**效率更高**。
    - **灵活性**: ResNet 的“即插即用”特性使其可以非常方便地与其他模块（如 Inception）结合。

尽管 ResNet 最终成为了主流，但 FractalNet 作为同期的一个强有力的竞争者，其独特的设计哲学为后来的网络结构研究提供了宝贵的思路和启发。