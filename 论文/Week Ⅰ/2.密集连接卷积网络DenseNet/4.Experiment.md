好的，遵照您的指令，我将严格依据您提供的PDF内容，开始对**第四章 Experiments**进行逐句深度解析。

---
### **4.0:1**

*   **原文 (Original):**
    *   We empirically demonstrate DenseNet’s effectiveness on several benchmark datasets and compare with state-of-the-art architectures, especially with ResNet and its variants.

*   **总结 (Summary):**
    *   作者通过实验来证明DenseNet在多个基准数据集上的有效性，并将其与最先进的架构，特别是ResNet及其变体，进行比较。

*   **句子结构 (Sentence Structure):**
    *   这是一个典型的实验章节开篇句，用于概括本章的核心内容和目的。结构为：We empirically demonstrate [Our Model]'s effectiveness on several benchmark datasets and compare with [competitors], especially with [main competitor].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Experiments`: 明确了本章的主要任务：实验验证和对比。
    *   `[[ResNet]]`: 被指定为本文最主要的比较对象。 #AI/DeepLearning/Models

---
### **4.1:1**

*   **原文 (Original):**
    *   CIFAR.

*   **总结 (Summary):**
    *   小节标题：CIFAR数据集。

*   **句子结构 (Sentence Structure):**
    *   这是一个标准的子章节标题。结构为：[Dataset Name].

*   **知识点 (Knowledge Points):**
    *   `[[CIFAR]]`: 包括CIFAR-10和CIFAR-100。 #AI/Datasets/ImageRecognition

---
### **4.1:2**

*   **原文 (Original):**
    *   The two CIFAR datasets consist of colored natural images with 32×32 pixels.

*   **总结 (Summary):**
    *   这两个CIFAR数据集都包含32x32像素的彩色自然图像。

*   **句子结构 (Sentence Structure):**
    *   这是一个对数据集基本属性进行描述的句子。结构为：The two [Dataset Family] datasets consist of [property A] images with [property B] pixels.

*   **知识点 (Knowledge Points):**
    *   `[[CIFAR]]`: 描述了其图像尺寸和类型。 #AI/Datasets/ImageRecognition

---
### **4.1:3**

*   **原文 (Original):**
    *   CIFAR-10 (C10) consists of images drawn from 10 and CIFAR-100 (C100) from 100 classes.

*   **总结 (Summary):**
    *   CIFAR-10（简称C10）包含10个类别，而CIFAR-100（简称C100）包含100个类别。

*   **句子结构 (Sentence Structure):**
    *   这是一个对数据集具体版本进行区分说明的句子。结构为：[Dataset A] consists of images drawn from [N] and [Dataset B] from [M] classes.

*   **知识点 (Knowledge Points):**
    *   `[[CIFAR-10]]`: 10分类数据集。 #AI/Datasets/ImageRecognition
    *   `[[CIFAR-100]]`: 100分类数据集。 #AI/Datasets/ImageRecognition

---
### **4.1:4**

*   **原文 (Original):**
    *   The training and test sets contain 50,000 and 10,000 images respectively, and we hold out 5,000 training images as a validation set.

*   **总结 (Summary):**
    *   这两个数据集的训练集和测试集分别包含5万和1万张图片，作者还从训练集中划分出5千张作为验证集。

*   **句子结构 (Sentence Structure):**
    *   这是一个对数据集划分和规模进行说明的句子。结构为：The training and test sets contain [N] and [M] images respectively, and we hold out [K] training images as a validation set.

*   **知识点 (Knowledge Points):**
    *   `[[Training Set]]`: 用于训练模型。 #AI/Methodology/Experiments
    *   `[[Test Set]]`: 用于最终评估。 #AI/Methodology/Experiments
    *   `[[Validation Set]]`: 用于模型选择和超参数调整。 #AI/Methodology/Experiments

---
### **4.1:5**

*   **原文 (Original):**
    *   We adopt a standard data augmentation scheme (mirroring/shifting) that is widely used for these two datasets.

*   **总结 (Summary):**
    *   作者采用了在这两个数据集上广泛使用的标准数据增强方案（水平翻转和随机平移）。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明实验中使用了何种数据增强技术的句子。结构为：We adopt a standard data augmentation scheme ([examples]) that is widely used for these two datasets.

*   **知识点 (Knowledge Points):**
    *   [[Data Augmentation]]: 一种通过对训练数据进行变换（如翻转、裁剪、变色）来扩充数据集，以提高模型泛化能力的技术。 #AI/DeepLearning/Techniques

---
### **4.1:6**

*   **原文 (Original):**
    *   We denote this data augmentation scheme by a “+” mark at the end of the dataset name (e.g., C10+).

*   **总结 (Summary):**
    *   作者用一个“+”号标记在数据集名称末尾来表示使用了数据增强（例如C10+）。

*   **句子结构 (Sentence Structure):**
    *   这是一个对实验记号进行定义的句子。结构为：We denote this [scheme] by a "[mark]" at the end of the dataset name (e.g., [example]).

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Notation`: 解释了C10+等记号的含义。

---
### **4.1:7**

*   **原文 (Original):**
    *   For preprocessing, we normalize the data using the channel means and standard deviations.

*   **总结 (Summary):**
    *   在预处理阶段，作者使用通道均值和标准差对数据进行归一化。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明数据预处理步骤的句子。结构为：For preprocessing, we normalize the data using the [method].

*   **知识点 (Knowledge Points):**
    *   `[[Data Normalization]]`: 一种将数据缩放到特定范围（如均值为0，方差为1）的预处理技术，有助于加速模型训练。 #AI/DataProcessing/Techniques

---
### **4.1:8**

*   **原文 (Original):**
    *   For the final run we use all 50,000 training images and report the final test error at the end of training.

*   **总结 (Summary):**
    *   在最终运行时，作者使用全部5万张训练图片进行训练，并报告训练结束时的最终测试误差。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明最终评估流程的句子。结构为：For the final run we use all [number] training images and report the final test error at the end of training.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Methodology`: 描述了标准的实验评估流程，即用包含验证集在内的全部训练数据重新训练模型，然后在测试集上报告一次最终性能。

---
### **4.1:9**

*   **原文 (Original):**
    *   SVHN.

*   **总结 (Summary):**
    *   小节标题：SVHN数据集。

*   **句子结构 (Sentence Structure):**
    *   这是一个标准的子章节标题。结构为：[Dataset Name].

*   **知识点 (Knowledge Points):**
    *   `[[SVHN (Street View House Numbers)]]`: 谷歌街景门牌号数据集。 #AI/Datasets/ImageRecognition

---
### **4.1:10**

*   **原文 (Original):**
    *   The Street View House Numbers (SVHN) dataset contains 32×32 colored digit images.

*   **总结 (Summary):**
    *   SVHN数据集包含32x32像素的彩色数字图像。

*   **句子结构 (Sentence Structure):**
    *   这是一个对数据集基本属性进行描述的句子。结构为：The [Dataset Name] contains [property A] [property B] images.

*   **知识点 (Knowledge Points):**
    *   `[[SVHN (Street View House Numbers)]]`: 描述了其图像尺寸和类型。 #AI/Datasets/ImageRecognition

---
### **4.1:11**

*   **原文 (Original):**
    *   There are 73,257 images in the training set, 26,032 images in the test set, and 531,131 images for additional training.

*   **总结 (Summary):**
    *   该数据集包含73,257张训练图片，26,032张测试图片，以及531,131张额外的训练图片。

*   **句子结构 (Sentence Structure):**
    *   这是一个对数据集规模进行详细说明的句子。结构为：There are [N] images in the training set, [M] images in the test set, and [K] images for additional training.

*   **知识点 (Knowledge Points):**
    *   `[[SVHN (Street View House Numbers)]]`: 描述了其详细的数据集划分和规模。 #AI/Datasets/ImageRecognition

---
### **4.1:12**

*   **原文 (Original):**
    *   Following common practice we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set.

*   **总结 (Summary):**
    *   遵循通用做法，作者使用了所有的训练数据（包括额外的），并且没有进行数据增强；同时从训练集中划分出6000张图片作为验证集。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明SVHN实验设置的句子，点明了其与CIFAR实验的不同之处。结构为：Following common practice we use all the training data without any [technique], and a validation set with [number] images is split from the training set.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Methodology`: 描述了SVHN数据集的具体实验设置。

---
### **4.1:13**

*   **原文 (Original):**
    *   We select the model with the lowest validation error during training and report the test error.

*   **总结 (Summary):**
    *   作者选择在训练过程中验证误差最低的模型，并报告其在测试集上的误差。

*   **句子结构 (Sentence Structure):**
    *   这是一个描述模型选择策略的句子。结构为：We select the model with the lowest validation error during training and report the test error.

*   **知识点 (Knowledge Points):**
    *   `[[Model Selection]]`: 一种基于验证集性能来选择最佳模型（或最佳训练轮次）的常用策略。 #AI/Methodology/Experiments

---
### **4.1:14**

*   **原文 (Original):**
    *   We follow and divide the pixel values by 255 so they are in the range.

*   **总结 (Summary):**
    *   遵循Wide ResNet论文的做法，作者将像素值除以255，将其归一化到的范围内。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明数据预处理步骤的句子。结构为：We follow [citation] and [action] so they are in the [range].

*   **知识点 (Knowledge Points):**
    *   `[[Data Normalization]]`: 描述了在SVHN上使用的具体归一化方法。 #AI/DataProcessing/Techniques

---
### **4.1:15**

*   **原文 (Original):**
    *   ImageNet.

*   **总结 (Summary):**
    *   小节标题：ImageNet数据集。

*   **句子结构 (Sentence Structure):**
    *   这是一个标准的子章节标题。结构为：[Dataset Name].

*   **知识点 (Knowledge Points):**
    *   `[[ImageNet数据集]]`: #AI/Datasets/ImageRecognition

---
### **4.1:16**

*   **原文 (Original):**
    *   The ILSVRC 2012 classification dataset consists 1.2 million images for training, and 50,000 for validation, from 1,000 classes.

*   **总结 (Summary):**
    *   ILSVRC 2012分类数据集包含120万张训练图片和5万张验证图片，共1000个类别。

*   **句子结构 (Sentence Structure):**
    *   这是一个对数据集规模和属性进行描述的句子。结构为：The [Dataset Name] consists [N] images for training, and [M] for validation, from [K] classes.

*   **知识点 (Knowledge Points):**
    *   `[[ImageNet数据集]]`: 描述了其规模。 #AI/Datasets/ImageRecognition

---
### **4.1:17**

*   **原文 (Original):**
    *   We adopt the same data augmentation scheme for training images as in, and apply a single-crop or 10-crop with size 224×224 at test time.

*   **总结 (Summary):**
    *   作者采用了与ResNet等工作相同的训练数据增强方案，并在测试时使用224x224尺寸的单中心裁剪或十裁剪方法。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明ImageNet实验数据增强和测试策略的句子。结构为：We adopt the same [scheme] for training images as in [citations], and apply a [test method A] or [test method B] with [size] at test time.

*   **知识点 (Knowledge Points):**
    *   `[[Test-Time Augmentation (TTA)]]`: 在测试时对单张图片进行多次变换（如多位置裁剪、翻转）并综合预测结果，以提高准确率的一种技术。10-crop就是一种TTA。 #AI/DeepLearning/Techniques

---
### **4.1:18**

*   **原文 (Original):**
    *   Following, we report classification errors on the validation set.

*   **总结 (Summary):**
    *   遵循ResNet等工作的惯例，作者在验证集上报告分类误差。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明报告结果所用数据集的句子。结构为：Following [citations], we report classification errors on the [dataset split].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Methodology`: 明确了ImageNet实验结果是在验证集上报告的，因为测试集标签不公开。

---
### **4.2:1**

*   **原文 (Original):**
    *   All the networks are trained using stochastic gradient descent (SGD).

*   **总结 (Summary):**
    *   所有的网络都使用随机梯度下降（SGD）进行训练。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明所用优化器的句子。结构为：All the networks are trained using [optimizer].

*   **知识点 (Knowledge Points):**
    *   `[[Stochastic Gradient Descent (SGD)]]`: 深度学习中最常用的优化算法之一。 #AI/DeepLearning/Training

---
### **4.2:2**

*   **原文 (Original):**
    *   On CIFAR and SVHN we train using batch size 64 for 300 and 40 epochs, respectively.

*   **总结 (Summary):**
    *   在CIFAR和SVHN上，作者使用64的批量大小，分别训练300个和40个轮次。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供小数据集上训练超参数的句子。结构为：On [Dataset A] and [Dataset B] we train using batch size [N] for [M] and [K] epochs, respectively.

*   **知识点 (Knowledge Points):**
    *   `[[Batch Size]]`: 每次梯度更新所用的样本数量。 #AI/DeepLearning/Training
    *   `[[Epoch]]`: 指整个训练集被完整地过一遍的次数。 #AI/DeepLearning/Training

---
### **4.2:3**

*   **原文 (Original):**
    *   The initial learning rate is set to 0.1, and is divided by 10 at 50% and 75% of the total number of training epochs.

*   **总结 (Summary):**
    *   初始学习率设为0.1，并在总训练轮数的50%和75%处分别将学习率除以10。

*   **句子结构 (Sentence Structure):**
    *   这是一个描述学习率调度策略的句子。结构为：The initial learning rate is set to [value], and is divided by [factor] at [milestone A] and [milestone B] of the total number of training epochs.

*   **知识点 (Knowledge Points):**
    *   [[Learning Rate]]: 控制模型参数更新步长的关键超参数。 #AI/DeepLearning/Training
    *   `[[Learning Rate Schedule]]`: 在训练过程中动态调整学习率的策略，有助于模型更好地收敛。 #AI/DeepLearning/Training

---
### **4.2:4**

*   **原文 (Original):**
    *   On ImageNet, we train models for 90 epochs with a batch size of 256.

*   **总结 (Summary):**
    *   在ImageNet上，作者使用256的批量大小训练90个轮次。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供ImageNet上训练超参数的句子。结构为：On [Dataset], we train models for [N] epochs with a batch size of [M].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Implementation`: 提供了ImageNet的训练轮数和批量大小。

---
### **4.2:5**

*   **原文 (Original):**
    *   The learning rate is set to 0.1 initially, and is lowered by 10 times at epoch 30 and 60.

*   **总结 (Summary):**
    *   初始学习率设为0.1，并在第30和第60个轮次时分别降低10倍。

*   **句子结构 (Sentence Structure):**
    *   这是一个描述ImageNet上学习率调度策略的句子。结构为：The learning rate is set to [value] initially, and is lowered by [factor] times at epoch [A] and [B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Implementation`: 提供了ImageNet的学习率调度方案。

---
### **4.2:6**

*   **原文 (Original):**
    *   Note that a naive implementation of DenseNet may contain memory inefficiencies.

*   **总结 (Summary):**
    *   作者特别指出，一个朴素的DenseNet实现可能会存在内存效率低下的问题。

*   **句子结构 (Sentence Structure):**
    *   这是一个提醒读者注意实现中潜在问题的句子。结构为：Note that a naive implementation of [Our Model] may contain [problem].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Implementation`: 提到了DenseNet的一个实现挑战：由于特征图不断拼接，会导致GPU显存占用巨大。

---
### **4.2:7**

*   **原文 (Original):**
    *   To reduce the memory consumption on GPUs, please refer to our technical report on the memory-efficient implementation of DenseNets.

*   **总结 (Summary):**
    *   为了减少在GPU上的显存消耗，作者建议读者参考他们关于DenseNet内存高效实现的技术报告。

*   **句子结构 (Sentence Structure):**
    *   这是一个为前述问题提供解决方案（通过引用另一篇论文）的句子。结构为：To [purpose], please refer to our technical report on the [solution].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Implementation`: 提出了通过共享内存等技巧来优化DenseNet显存占用的方法。

---
### **4.2:8**

*   **原文 (Original):**
    *   Following, we use a weight decay of 10−4 and a Nesterov momentum of 0.9 without dampening.

*   **总结 (Summary):**
    *   遵循ResNet论文的做法，作者使用了1e-4的权重衰减和0.9的Nesterov动量（无dampening）。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供优化器具体参数设置的句子。结构为：Following [citation], we use a [parameter A] of [value] and a [parameter B] of [value] without dampening.

*   **知识点 (Knowledge Points):**
    *   `[[Weight Decay]]`: 一种L2正则化技术，用于防止过拟合。 #AI/DeepLearning/Techniques
    *   `[[Momentum (Optimization)]]`: SGD的一种变体，通过引入动量项来加速收敛并越过局部最优点。Nesterov动量是一种改进版本。 #AI/DeepLearning/Training

---
### **4.2:9**

*   **原文 (Original):**
    *   We adopt the weight initialization introduced by.

*   **总结 (Summary):**
    *   作者采用了He等人在论文中提出的权重初始化方法。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明权重初始化方法的句子。结构为：We adopt the weight initialization introduced by [citation].

*   **知识点 (Knowledge Points):**
    *   `[[Weight Initialization]]`: 特指“He初始化”，一种特别适用于ReLU激活函数的权重初始化方法。 #AI/DeepLearning/Techniques

---
### **4.2:10**

*   **原文 (Original):**
    *   For the three datasets without data augmentation, i.e., C10, C100 and SVHN, we add a dropout layer after each convolutional layer (except the first one) and set the dropout rate to 0.2.

*   **总结 (Summary):**
    *   对于三个没有使用数据增强的数据集（C10, C100, SVHN），作者在每个卷积层后（第一个除外）都增加了一个丢弃层（dropout），丢弃率设为0.2。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明在特定条件下使用的正则化技术的句子。结构为：For the [datasets] without data augmentation, i.e., [examples], we add a [regularizer] after each [location] and set the [parameter] to [value].

*   **知识点 (Knowledge Points):**
    *   `[[Dropout]]`: 一种通过在训练时随机“关闭”神经元来防止过拟合的正则化技术。 #AI/DeepLearning/Techniques

---
### **4.2:11**

*   **原文 (Original):**
    *   The test errors were only evaluated once for each task and model setting.

*   **总结 (Summary):**
    *   对于每个任务和模型配置，测试误差只评估一次。

*   **句子结构 (Sentence Structure):**
    *   这是一个强调实验严谨性，避免在测试集上“调参”的句子。结构为：The test errors were only evaluated once for each [condition].

*   **知识点 (Knowledge Points):**
    *   `#AI/Methodology/Experiments`: 遵循了机器学习实验的最佳实践。

---
### **4.3:1**

*   **原文 (Original):**
    *   We train DenseNets with different depths, L, and growth rates, k.

*   **总结 (Summary):**
    *   作者训练了具有不同深度L和增长率k的DenseNets。

*   **句子结构 (Sentence Structure):**
    *   这是一个引出实验变量的句子。结构为：We train [Model] with different [parameter A], and [parameter B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Experiments`: 明确了实验中探索的两个主要超参数：深度L和增长率k。

---
### **4.3:2**

*   **原文 (Original):**
    *   The main results on CIFAR and SVHN are shown in Table 2.

*   **总结 (Summary):**
    *   在CIFAR和SVHN上的主要结果展示在表2中。

*   **句子结构 (Sentence Structure):**
    *   这是一个引导读者查看结果表格的句子。结构为：The main results on [Dataset A] and [Dataset B] are shown in Table [number].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Results`: 指向CIFAR/SVHN的结果表。

---
### **4.3:3**

*   **原文 (Original):**
    *   To highlight general trends, we mark all results that outperform the existing state-of-the-art in boldface and the overall best result in blue.

*   **总结 (Summary):**
    *   为了凸显总体趋势，作者将所有超越SOTA的结果用粗体标记，并将全局最佳结果用蓝色标记。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释表格标记规则的句子。结构为：To highlight general trends, we mark all results that [condition A] in [format A] and the [condition B] in [format B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Notation`: 解释了结果表中的格式。

---
### **4.3:4**

*   **原文 (Original):**
    *   Accuracy.

*   **总结 (Summary):**
    *   小节标题：准确率。

*   **句子结构 (Sentence Structure):**
    *   这是一个标准的子章节标题。结构为：[Topic Name].

*   **知识点 (Knowledge Points):**
    *   `[[Accuracy]]`: 将要分析模型的准确率表现。 #AI/Terminology/Evaluation

---
### **4.3:5**

![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509111138896.png)


*   **原文 (Original):**
    *   Possibly the most noticeable trend may originate from the bottom row of [[Table 2]], which shows that DenseNet-BC with L = 190 and k = 40 outperforms the existing state-of-the-art consistently on all the CIFAR datasets.

*   **总结 (Summary):**
    *   最显著的趋势可能来自表2的最后一行，它显示了L=190, k=40的DenseNet-BC模型在所有CIFAR数据集上都稳定地超越了现有的最先进水平。

*   **句子结构 (Sentence Structure):**
    *   这是一个指出最重要实验结果的句子。结构为：Possibly the most noticeable trend may originate from [location in table], which shows that [Our Model] with [configuration] outperforms the existing state-of-the-art consistently on all the [datasets].

*   **知识点 (Knowledge Points):**
    *   `[[DenseNet-BC]]`: 实验结果表明，最优化版本的DenseNet取得了最佳性能。 #Paper/DenseNet/Results

---
### **4.3:6**

*   **原文 (Original):**
    *   Its error rates of 3.46% on C10+ and 17.18% on C100+ are significantly lower than the error rates achieved by wide ResNet architecture.

*   **总结 (Summary):**
    *   它在C10+上的3.46%和C100+上的17.18%的错误率，显著低于Wide ResNet架构所达到的错误率。

*   **句子结构 (Sentence Structure):**
    *   这是一个通过与强劲对手进行具体数值比较来凸显性能优势的句子。结构为：Its error rates of [value A] on [Dataset A] and [value B] on [Dataset B] are significantly lower than the error rates achieved by [competitor model].

*   **知识点 (Knowledge Points):**
    *   `[[Wide ResNet]]`: 作为当时一个非常强的基线模型，DenseNet的超越更具说服力。 #AI/DeepLearning/Models
    *   `#Paper/DenseNet/Results`: 提供了具体的SOTA性能数据。

---
### **4.3:7**

*   **原文 (Original):**
    *   Our best results on C10 and C100 (without data augmentation) are even more encouraging: both are close to 30% lower than FractalNet with drop-path regularization.

*   **总结 (Summary):**
    *   在没有数据增强的C10和C100上，DenseNet的最佳结果甚至更令人鼓舞：错误率都比带drop-path正则化的FractalNet低了近30%。

*   **句子结构 (Sentence Structure):**
    *   这是一个强调在更困难条件下（无数据增强）性能优势的句子。结构为：Our best results on [datasets] (without data augmentation) are even more encouraging: both are close to [percentage] lower than [competitor model].

*   **知识点 (Knowledge Points):**
    *   `[[FractalNets]]`: 另一个强大的基线模型。 #AI/DeepLearning/Models

---
### **4.3:8**

*   **原文 (Original):**
    *   On SVHN, with dropout, the DenseNet with L = 100 and k = 24 also surpasses the current best result achieved by wide ResNet.

*   **总结 (Summary):**
    *   在SVHN上，使用了dropout的L=100, k=24的DenseNet也超越了当前由Wide ResNet取得的最佳结果。

*   **句子结构 (Sentence Structure):**
    *   这是一个报告在另一个数据集上取得SOTA性能的句子。结构为：On [Dataset], with [technique], the [Our Model] with [configuration] also surpasses the current best result achieved by [competitor model].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Results`: 展示了在SVHN上的SOTA结果。

---
### **4.3:9**

*   **原文 (Original):**
    *   However, the 250-layer DenseNet-BC doesn’t further improve the performance over its shorter counterpart.

*   **总结 (Summary):**
    *   然而，在SVHN上，250层的DenseNet-BC并没有比其更浅的版本性能更好。

*   **句子结构 (Sentence Structure):**
    *   这是一个报告反常实验结果的句子，并为接下来的分析做铺垫。结构为：However, the [deeper model] doesn’t further improve the performance over its [shorter counterpart].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Analysis`: 提出了一个需要解释的现象。

---
### **4.3:10**

*   **原文 (Original):**
    *   This may be explained by that SVHN is a relatively easy task, and extremely deep models may overfit to the training set.

*   **总结 (Summary):**
    *   对此的解释可能是，SVHN是一个相对简单的任务，过于深的模型可能会对训练集产生过拟合。

*   **句子结构 (Sentence Structure):**
    *   这是一个对前述反常结果进行解释的句子。结构为：This may be explained by that [Dataset] is a relatively easy task, and extremely deep models may [negative consequence].

*   **知识点 (Knowledge Points):**
    *   `[[Overfitting]]`: 指出在简单任务上，模型容量过大可能导致过拟合。 #AI/DeepLearning/Challenges

---
### **4.3:11**

*   **原文 (Original):**
    *   Capacity.

*   **总结 (Summary):**
    *   小节标题：容量。

*   **句子结构 (Sentence Structure):**
    *   这是一个标准的子章节标题。结构为：[Topic Name].

*   **知识点 (Knowledge Points):**
    *   `[[Model Capacity]]`: 指模型拟合复杂函数的能力，通常与参数数量正相关。 #AI/DeepLearning/Fundamentals

---
### **4.3:12**

*   **原文 (Original):**
    *   Without compression or bottleneck layers, there is a general trend that DenseNets perform better as L and k increase.

*   **总结 (Summary):**
    *   在不使用压缩或瓶颈层的情况下，一个总体趋势是，随着深度L和增长率k的增加，DenseNets的性能会变得更好。

*   **句子结构 (Sentence Structure):**
    *   这是一个总结模型性能与超参数关系的句子。结构为：Without [optimizations], there is a general trend that [Our Model] perform better as [parameter A] and [parameter B] increase.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Analysis`: 观察到模型性能随容量增加而提升。

---
### **4.3:13**

*   **原文 (Original):**
    *   We attribute this primarily to the corresponding growth in model capacity.

*   **总结 (Summary):**
    *   作者将此现象主要归因于模型容量的相应增长。

*   **句子结构 (Sentence Structure):**
    *   这是一个对前述趋势进行归因的句子。结构为：We attribute this primarily to the corresponding growth in [reason].

*   **知识点 (Knowledge Points):**
    *   [[Model Capacity]]: 明确了性能提升的原因。 #AI/DeepLearning/Fundamentals

---
### **4.3:14**

*   **原文 (Original):**
    *   This is best demonstrated by the column of C10+ and C100+.

*   **总结 (Summary):**
    *   这一点在C10+和C100+这两列的数据中得到了最好的体现。

*   **句子结构 (Sentence Structure):**
    *   这是一个引导读者关注表格中特定数据来佐证论点的句子。结构为：This is best demonstrated by the column of [Dataset A] and [Dataset B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 指向表格中的具体证据。

---
### **4.3:15**

*   **原文 (Original):**
    *   On C10+, the error drops from 5.24% to 4.10% and finally to 3.74% as the number of parameters increases from 1.0M, over 7.0M to 27.2M.

*   **总结 (Summary):**
    *   在C10+上，随着参数量从1M增加到7M再到27.2M，错误率也从5.24%下降到4.10%，最终降至3.74%。

*   **句子结构 (Sentence Structure):**
    *   这是一个通过具体数值变化来展示性能与容量正相关关系的句子。结构为：On [Dataset], the error drops from [value A] to [value B] and finally to [value C] as the number of parameters increases from [N] to [M] to [K].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 提供了详细的数据来支撑论点。

---
### **4.3:16**

*   **原文 (Original):**
    *   On C100+, we observe a similar trend.

*   **总结 (Summary):**
    *   在C100+上，作者观察到了类似的趋势。

*   **句子结构 (Sentence Structure):**
    *   这是一个对另一个数据集结果进行简要说明的句子。结构为：On [Dataset], we observe a similar trend.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 进一步佐证论点的普适性。

---
### **4.3:17**

*   **原文 (Original):**
    *   This suggests that DenseNets can utilize the increased representational power of bigger and deeper models.

*   **总结 (Summary):**
    *   这表明DenseNets能够有效利用更大、更深模型所带来的增强的表示能力。

*   **句子结构 (Sentence Structure):**
    *   这是一个从实验现象中得出结论的句子。结构为：This suggests that [Our Model] can utilize the increased [capability] of bigger and deeper models.

*   **知识点 (Knowledge Points):**
    *   `[[Representation Power]]`: 指模型从数据中学习和表示复杂模式的能力，通常与模型容量相关。 #AI/DeepLearning/Fundamentals

---
### **4.3:18**

*   **原文 (Original):**
    *   It also indicates that they do not suffer from overfitting or the optimization difficulties of residual networks.

*   **总结 (Summary):**
    *   这也表明，DenseNets没有遭受过拟合问题，也没有遇到像（深层普通）残差网络那样的优化困难。

*   **句子结构 (Sentence Structure):**
    *   这是一个指出DenseNet避免了其他模型常见问题的句子。结构为：It also indicates that they do not suffer from [problem A] or the [problem B] of [other models].

*   **知识点 (Knowledge Points):**
    *   `[[Overfitting]]`: DenseNet在容量增加时性能持续提升，说明没有严重过拟合。 #AI/DeepLearning/Challenges
    *   `[[Degradation Problem]]`: 此处指ResNet论文中描述的plain net的优化困难，DenseNet没有出现类似问题。 #Paper/ResNet/Motivation

---
### **4.3:19**

*   **原文 (Original):**
    *   Parameter Efficiency.

*   **总结 (Summary):**
    *   小节标题：参数效率。

*   **句子结构 (Sentence Structure):**
    *   这是一个标准的子章节标题。结构为：[Topic Name].

*   **知识点 (Knowledge Points):**
    *   `[[Parameter Efficiency]]`: DenseNet的核心优势之一。 #Paper/DenseNet/Advantages

---
### **4.3:20**

*   **原文 (Original):**
    *   The results in [[Table 2]] indicate that DenseNets utilize parameters more efficiently than alternative architectures (in particular, ResNets).

*   **总结 (Summary):**
    *   表2的结果表明，DenseNets比其他架构（特别是ResNets）更有效地利用参数。

*   **句子结构 (Sentence Structure):**
    *   这是一个总结模型关键优势的句子。结构为：The results in [table] indicate that [Our Model] utilize parameters more efficiently than [competitors].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Results`: 提出了DenseNet参数效率高的核心结论。

---
### **4.3:21**

*   **原文 (Original):**
    *   The DenseNet-BC with bottleneck structure and dimension reduction at transition layers is particularly parameter-efficient.

*   **总结 (Summary):**
    *   带有瓶颈层结构和在过渡层进行维度降低的DenseNet-BC版本，参数效率尤其高。

*   **句子结构 (Sentence Structure):**
    *   这是一个指出最优变体在关键优势上表现突出的句子。结构为：The [Model Variant] with [feature A] and [feature B] is particularly parameter-efficient.

*   **知识点 (Knowledge Points):**
    *   `[[DenseNet-BC]]`: 被确认为参数效率最高的版本。 #Paper/DenseNet/Architecture

---
### **4.3:22**

*   **原文 (Original):**
    *   For example, our 250-layer model only has 15.3M parameters, but it consistently outperforms other models such as FractalNet and Wide ResNets that have more than 30M parameters.

*   **总结 (Summary):**
    *   例如，作者的250层模型只有15.3M参数，但其性能却稳定地超过了那些拥有超过30M参数的模型，如FractalNet和Wide ResNets。

*   **句子结构 (Sentence Structure):**
    *   这是一个通过具体数值对比来支撑参数高效论点的句子。结构为：For example, our [model] only has [N] parameters, but it consistently outperforms other models such as [competitor A] and [competitor B] that have more than [M] parameters.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 提供了参数效率的具体数据支撑。

---
### **4.3:23**

*   **原文 (Original):**
    *   We also highlight that DenseNet-BC with L = 100 and k = 12 achieves comparable performance (e.g., 4.51% vs 4.62% error on C10+, 22.27% vs 22.71% error on C100+) as the 1001-layer pre-activation ResNet using 90% fewer parameters.

*   **总结 (Summary):**
    *   作者还强调，一个L=100, k=12的DenseNet-BC，其性能与一个1001层的预激活ResNet相当，但参数量却少了90%。

*   **句子结构 (Sentence Structure):**
    *   这是一个通过与一个极深模型进行惊人对比，来极力凸显参数高效性的句子。结构为：We also highlight that [Our Model] with [configuration] achieves comparable performance as the [competitor model] using [percentage] fewer parameters.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 提供了与千层ResNet对比的、极具说服力的参数效率证据。

---
### **4.3:24**
![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509111153086.png)

*   **原文 (Original):**
    *   [[Figure 4]] (right panel) shows the training loss and test errors of these two networks on C10+.

*   **总结 (Summary):**
    *   图4（右图）展示了这两个网络（百层DenseNet-BC和千层ResNet）在C10+上的训练损失和测试误差曲线。

*   **句子结构 (Sentence Structure):**
    *   这是一个引导读者查看图表以获取更详细对比信息的句子。结构为：Figure [number] shows the [metric A] and [metric B] of these two networks on [Dataset].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 指向展示训练曲线对比的图表。

---
### **4.3:25**

*   **原文 (Original):**
    *   The 1001-layer deep ResNet converges to a lower training loss value but a similar test error.

*   **总结 (Summary):**
    *   那个1001层的深度ResNet收敛到了一个更低的训练损失值，但测试误差却与DenseNet-BC相似。

*   **句子结构 (Sentence Structure):**
    *   这是一个对前述图表进行分析的句子，揭示了一个有趣的现象。结构为：The [competitor model] converges to a lower [metric A] but a similar [metric B].

*   **知识点 (Knowledge Points):**
    *   `[[Overfitting]]`: ResNet更低的训练误差和相似的测试误差，可能暗示了它存在一定程度的过拟合，而DenseNet的正则化效果更好。 #AI/DeepLearning/Challenges

---
### **4.3:26**

*   **原文 (Original):**
    *   We analyze this effect in more detail below.

*   **总结 (Summary):**
    *   作者将在下文更详细地分析这一现象。

*   **句子结构 (Sentence Structure):**
    *   这是一个承上启下的句子。结构为：We analyze this effect in more detail below.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Analysis`: 预告将要对过拟合现象进行分析。

---
### **4.3:27**

*   **原文 (Original):**
    *   Overfitting.

*   **总结 (Summary):**
    *   小节标题：过拟合。

*   **句子结构 (Sentence Structure):**
    *   这是一个标准的子章节标题。结构为：[Topic Name].

*   **知识点 (Knowledge Points):**
    *   `[[Overfitting]]`: #AI/DeepLearning/Challenges

---
### **4.3:28**

*   **原文 (Original):**
    *   One positive side-effect of the more efficient use of parameters is a tendency of DenseNets to be less prone to overfitting.

*   **总结 (Summary):**
    *   更高效的参数利用带来了一个积极的副作用：DenseNets倾向于不易发生过拟合。

*   **句子结构 (Sentence Structure):**
    *   这是一个提出模型附加优势的句子。结构为：One positive side-effect of the [reason] is a tendency of [Our Model] to be less prone to [problem].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Advantages`: 指出DenseNet具有更好的正则化效果。

---
### **4.3:29**

*   **原文 (Original):**
    *   We observe that on the datasets without data augmentation, the improvements of DenseNet architectures over prior work are particularly pronounced.

*   **总结 (Summary):**
    *   作者观察到，在没有数据增强的数据集上，DenseNet架构相对于先前工作的性能提升尤其显著。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供证据来支撑前述论点的句子。结构为：We observe that on the datasets without data augmentation, the improvements of [Our Model] over prior work are particularly pronounced.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 在更困难的（无数据增强）条件下，DenseNet的优势更明显，这侧面印证了其内在的正则化能力。

---
### **4.3:30**

*   **原文 (Original):**
    *   On C10, the improvement denotes a 29% relative reduction in error from 7.33% to 5.19%.

*   **总结 (Summary):**
    *   在C10上，性能提升意味着错误率从7.33%降至5.19%，相对降低了29%。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供具体数值来量化性能提升的句子。结构为：On [Dataset], the improvement denotes a [percentage] relative reduction in error from [value A] to [value B].

*   **知识点 (Knowledge Points):**
    *   `[[Relative Improvement]]`: 相对提升率。 #AI/Terminology/Evaluation
    *   `#Paper/DenseNet/Evidence`: 提供了具体的正则化效果数据。

---
### **4.3:31**

*   **原文 (Original):**
    *   On C100, the reduction is about 30% from 28.20% to 19.64%.

*   **总结 (Summary):**
    *   在C100上，错误率从28.20%降至19.64%，降低了约30%。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供具体数值来量化性能提升的句子。结构为：On [Dataset], the reduction is about [percentage] from [value A] to [value B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 提供了具体的正则化效果数据。

---
### **4.3:32**

*   **原文 (Original):**
    *   In our experiments, we observed potential overfitting in a single setting: on C10, a 4× growth of parameters produced by increasing k = 12 to k = 24 lead to a modest increase in error from 5.77% to 5.83%.

*   **总结 (Summary):**
    *   作者也报告了一个观察到潜在过拟合的特例：在C10上，将增长率k从12增加到24导致参数量增长4倍，但错误率却从5.77%轻微上升到了5.83%。

*   **句子结构 (Sentence Structure):**
    *   这是一个报告反常现象以使分析更全面的句子。结构为：In our experiments, we observed potential [problem] in a single setting: on [Dataset], a [change in hyper-parameter] lead to a modest increase in error from [value A] to [value B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Analysis`: 展示了模型容量并非越大越好，也存在过拟合的边界。

---
### **4.3:33**

*   **原文 (Original):**
    *   The DenseNet-BC bottleneck and compression layers appear to be an effective way to counter this trend.

*   **总结 (Summary):**
    *   作者指出，DenseNet-BC中的瓶颈层和压缩层似乎是抵抗这种（过拟合）趋势的有效方法。

*   **句子结构 (Sentence Structure):**
    *   这是一个提出解决方案的句子。结构为：The [Model Variant] [feature A] and [feature B] appear to be an effective way to counter this trend.

*   **知识点 (Knowledge Points):**
    *   `[[DenseNet-BC]]`: 其瓶颈和压缩设计不仅提升了参数效率，也增强了模型的正则化效果。 #Paper/DenseNet/Advantages

---
### **4.4:1**

*   **原文 (Original):**
    *   We evaluate DenseNet-BC with different depths and growth rates on the ImageNet classification task, and compare it with state-of-the-art ResNet architectures.

*   **总结 (Summary):**
    *   作者在ImageNet分类任务上评估了具有不同深度和增长率的DenseNet-BC，并将其与最先进的ResNet架构进行比较。

*   **句子结构 (Sentence Structure):**
    *   这是一个引出ImageNet实验部分的句子。结构为：We evaluate [Our Model] with [hyper-parameters] on the [task], and compare it with [competitors].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Experiments`: 开始介绍ImageNet上的实验。

---
### **4.4:2**

*   **原文 (Original):**
    *   To ensure a fair comparison between the two architectures, we eliminate all other factors such as differences in data preprocessing and optimization settings by adopting the publicly available Torch implementation for ResNet by.

*   **总结 (Summary):**
    *   为了确保两种架构间的公平比较，作者通过采用一个公开的ResNet的Torch实现，来消除所有其他如数据预处理和优化设置不同等干扰因素。

*   **句子结构 (Sentence Structure):**
    *   这是一个强调实验公平性的句子。结构为：To ensure a fair comparison between the two architectures, we eliminate all other factors such as [factor A] and [factor B] by adopting the [methodology].

*   **知识点 (Knowledge Points):**
    *   `[[Torch]]`: 当时流行的一个深度学习框架。 #AI/Software/Frameworks
    *   `#Paper/DenseNet/Methodology`: 描述了如何通过使用相同的公开代码库来保证实验的公平性。

---
### **4.4:3**

*   **原文 (Original):**
    *   We simply replace the ResNet model with the DenseNet-BC network, and keep all the experiment settings exactly the same as those used for ResNet.

*   **总结 (Summary):**
    *   作者只是简单地将代码库中的ResNet模型替换为DenseNet-BC网络，并保持所有实验设置与原来ResNet的完全一样。

*   **句子结构 (Sentence Structure):**
    *   这是一个详细说明如何实现公平比较的句子。结构为：We simply replace the [competitor model] with the [Our Model], and keep all the experiment settings exactly the same as those used for [competitor model].

*   **知识点 (Knowledge Points):**
    *   `[[Ablation Study]]`: 这是一种严格的消融实验，唯一的变量就是网络架构本身。 #AI/Methodology/Experiments

---
### **4.4:4**
![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509111148582.png)


*   **原文 (Original):**
    *   We report the single-crop and 10-crop validation errors of DenseNets on ImageNet in [[Table 3]].

*   **总结 (Summary):**
    *   作者在表3中报告了DenseNets在ImageNet上的单裁剪和十裁剪验证误差。

*   **句子结构 (Sentence Structure):**
    *   这是一个引导读者查看结果表格的句子。结构为：We report the [metric A] and [metric B] of [Our Model] on [Dataset] in Table [number].

*   -**知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Results`: 指向ImageNet的结果表。

---
### **4.e:e**
![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509111152201.png)


*   **原文 (Original):**
    *   [[Figure 3]] shows the single-crop top-1 validation errors of DenseNets and ResNets as a function of the number of parameters (left) and FLOPs (right).

*   **总结 (Summary):**
    *   图3将DenseNets和ResNets的单裁剪top-1验证误差，分别作为参数数量（左图）和计算量FLOPs（右图）的函数进行了可视化比较。

*   **句子结构 (Sentence Structure):**
    *   这是一个引导读者查看关键对比图的句子。结构为：Figure [number] shows the [metric] of [Model A] and [Model B] as a function of the [x-axis A] and [x-axis B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Results`: 指向展示参数效率和计算效率的核心对比图。

---
### **4.4:6**

*   **原文 (Original):**
    *   The results presented in the figure reveal that DenseNets perform on par with the state-of-the-art ResNets, whilst requiring significantly fewer parameters and computation to achieve comparable performance.

*   **总结 (Summary):**
    *   图中展示的结果揭示，DenseNets的性能与最先进的ResNets相当，但达到同等性能所需的参数量和计算量都显著更少。

*   **句子结构 (Sentence Structure):**
    *   这是一个对前述图表进行核心结论总结的句子。结构为：The results presented in the figure reveal that [Our Model] perform on par with the [competitor model], whilst requiring significantly fewer [resource A] and [resource B] to achieve comparable performance.

*   **知识点 (Knowledge Points):**
    *   `[[Parameter Efficiency]]`: 实验结果再次证明了DenseNet的参数高效性。 #Paper/DenseNet/Advantages
    *   `[[Computational Efficiency]]`: 实验结果证明了DenseNet的计算高效性。 #Paper/DenseNet/Advantages

---
### **4.4:7**

*   **原文 (Original):**
    *   For example, a DenseNet-201 with 20M parameters model yields similar validation error as a 101-layer ResNet with more than 40M parameters.

*   **总结 (Summary):**
    *   例如，一个20M参数的DenseNet-201模型，其验证误差与一个超过40M参数的ResNet-101相似。

*   **句子结构 (Sentence Structure):**
    *   这是一个通过具体模型对比来量化参数效率的句子。结构为：For example, a [Our Model] with [N] parameters model yields similar [performance] as a [competitor model] with more than [M] parameters.

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 提供了参数效率的具体数据对比。

---
### **4.4:8**

*   **原文 (Original):**
    *   Similar trends can be observed from the right panel, which plots the validation error as a function of the number of FLOPs: a DenseNet that requires as much computation as a ResNet-50 performs on par with a ResNet-101, which requires twice as much computation.

*   **总结 (Summary):**
    *   在右图中可以观察到类似的趋势：一个计算量与ResNet-50相当的DenseNet，其性能与需要两倍计算量的ResNet-101相当。

*   **句子结构 (Sentence Structure):**
    *   这是一个通过具体模型对比来量化计算效率的句子。结构为：Similar trends can be observed from the right panel, which [description of the panel]: a [Our Model] that [condition A] performs on par with a [competitor model], which [condition B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Evidence`: 提供了计算效率的具体数据对比。

---
### **4.4:9**

*   **原文 (Original):**
    *   It is worth noting that our experimental setup implies that we use hyperparameter settings that are optimized for ResNets but not for DenseNets.

*   **总结 (Summary):**
    *   值得注意的是，作者的实验设置意味着他们使用的是为ResNet优化过的超参数，而非为DenseNet专门优化的。

*   **句子结构 (Sentence Structure):**
    *   这是一个指出当前实验局限性，并暗示模型仍有提升空间的句子。结构为：It is worth noting that our experimental setup implies that we use hyperparameter settings that are optimized for [competitor] but not for [Our Model].

*   **知识点 (Knowledge Points):**
    *   `#Paper/DenseNet/Analysis`: 暗示DenseNet的真实性能可能比当前报告的还要高。

---
### **4.4:10**

*   **原文 (Original):**
    *   It is conceivable that more extensive hyper-parameter searches may further improve the performance of DenseNet on ImageNet.

*   **总结 (Summary):**
    *   可以想见，更广泛的超参数搜索可能会进一步提升DenseNet在ImageNet上的性能。

*   **句子结构 (Sentence Structure):**
    *   这是一个对未来潜在提升进行展望的句子。结构为：It is conceivable that more extensive [action] may further improve the performance of [Our Model] on [Dataset].

*   **知识点 (Knowledge Points):**
    *   `[[Hyper-parameter Tuning]]`: 提升模型性能的关键步骤。 #AI/Methodology/Experiments