![](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509100931335.png)

# 图像解析：VGG-19 vs. Plain-34 vs. ResNet-34 架构对比

**标签**: #DeepLearning #ResNet #VGG #CNN #ComputerVision #NetworkArchitecture

这张图是 [[残差网络 (ResNet)]] 论文中的核心图示，它清晰地对比了三种网络架构，旨在说明 ResNet 解决的核心问题——**网络退化（Degradation）**。

我们将从左到右，逐一详细解析每个架构的每一个细节。

---

## 1. 整体结构与通用符号解析

- **三列架构**:
    1.  **VGG-19**: 作为当时（2014年）非常成功的深度网络的**参考基准**。
    2.  **34-layer plain**: 一个作者为了做对比实验而构建的**“朴素”深层网络**，是实验的**对照组**。
    3.  **34-layer residual**: 本文提出的 **ResNet 架构**，是实验的**核心**。
- **流程**: 所有架构都是从上至下处理数据，顶部是输入图像，底部是最终的分类输出。
- **左侧文字 (`output size`)**:
    - `image, size: 224`: 输入图像的尺寸为 224x224 像素。
    - `output size: 112`, `56`, `28`, `14`, `7`: 这些数字代表流经网络各阶段的**特征图（feature map）的空间尺寸（高度x宽度）**。尺寸的减半是通过带步长（stride）的卷积或池化操作实现的。
    - `output size: 1`: 最终输出的是一个向量，经过分类器后得到最终结果。
- **方框内的文字**:
    - `3x3 conv, 64`: 表示一个卷积层，使用 3x3 大小的**卷积核（kernel）**，并输出 **64 个通道（channels/filters）**。
    - `/2`: 这个后缀表示该层的**步长（stride）为 2**。对于卷积层或池化层，步长为2会使其输出的特征图空间尺寸（高和宽）减半。
    - `fc 1000`: 表示一个**全连接层（Fully Connected Layer）**，有 1000 个输出神经元，通常对应 ImageNet 数据集的 1000 个类别。

---

## 2. 架构一：VGG-19 (最左侧)

VGG-19 是一个设计非常规整、优雅的深度网络，它的特点是只使用 3x3 的小卷积核和 2x2 的池化层来构建。

- **`input` (224x224)**: 输入图像。
- **`3x3 conv, 64` (x2)**: 两个连续的 3x3 卷积层，输出通道数为 64。
- **`pool, /2`**: 一个步长为2的池化层，将特征图尺寸从 224x224 降到 **112x112**。
- **`3x3 conv, 128` (x2)**: 两个 3x3 卷积，输出通道数增至 128。
- **`pool, /2`**: 池化，尺寸降至 **56x56**。
- **`3x3 conv, 256` (x4)**: 四个 3x3 卷积，输出通道数 256。
- **`pool, /2`**: 池化，尺寸降至 **28x28**。
- **`3x3 conv, 512` (x4)**: 四个 3x3 卷积，输出通道数 512。
- **`pool, /2`**: 池化，尺寸降至 **14x14**。
- **`3x3 conv, 512` (x4)**: 又是四个 3x3 卷积，输出通道数 512。
- **`pool, /2`**: 池化，尺寸降至 **7x7**。
- **`fc 4096` (x2)**: 两个庞大的全连接层，各有 4096 个神经元。这是 VGG 参数量巨大的主要原因之一。
- **`fc 1000`**: 最终的分类层。

**总层数**: 16个卷积层 + 3个全连接层 = **19个带权重的层**。
**作用**: 在图中作为一个参照物，展示当时主流的、非常深的“朴素”网络是什么样的。

---

## 3. 架构二：34-layer plain (中间)

这个网络是作者为了与 ResNet 公平比较而专门设计的。它借鉴了 VGG 的简洁设计（主要使用3x3卷积），但更深，并且没有使用 VGG 那样庞大的全连接层。

- **`7x7 conv, 64, /2`**: 一个大的 7x7 卷积核用于初始特征提取，步长为2，直接将尺寸从 224x224 降到 **112x112**。
- **`pool, /2`**: 进一步池化，尺寸降至 **56x56**。
- **`3x3 conv, 64` (x3)**: 3个连续的 64 通道 3x3 卷积。
- **`3x3 conv, 128, /2` (x4)**: 4个 128 通道的 3x3 卷积。注意，**第一个卷积的步长为2**，负责将尺寸从 56x56 降到 **28x28**，同时通道数翻倍。
- **`3x3 conv, 256, /2` (x6)**: 6个 256 通道的 3x3 卷积，第一个步长为2，尺寸降至 **14x14**。
- **`3x3 conv, 512, /2` (x3)**: 3个 512 通道的 3x3 卷积，第一个步长为2，尺寸降至 **7x7**。
- **`avg pool`**: **全局平均池化（Global Average Pooling）**。这是一种现代技术，将每个 7x7 的特征图池化成一个单一的值，取代了 VGG 中庞大的全连接层，大大减少了参数。
- **`fc 1000`**: 最后的分类层。

**总层数**: 1 (7x7) + 3+4+6+3 (3x3) + 1 (fc) = **34个带权重的层**。
**作用**: 这是实验的**对照组**。论文指出，这个34层的朴素网络，其性能**反而比一个更浅的18层朴素网络要差**，这就是著名的**“网络退化”**现象。

---

## 4. 架构三：34-layer residual (最右侧)

这是本文的主角——ResNet。

- **骨干结构 (Backbone)**: **与中间的 34-layer plain 网络完全相同**。这是为了确保对比的公平性，证明性能的提升仅仅来自于新引入的结构，而不是其他因素。
- **核心区别：快捷连接 (Shortcut Connections)**:
    - 图中那些**弧形的箭头**就是快捷连接。它们的作用是让输入信息可以“跳过”一个或多个层，直接与这些层的输出相加。
    - **实线弧形箭头**: 表示**恒等快捷连接 (Identity Shortcut)**。当输入和输出的维度（通道数和空间尺寸）完全相同时使用。例如，在连续的 64 通道卷积块内部。公式为 $H(x) = F(x) + x$。
    - **虚线弧形箭头**: 表示**投影快捷连接 (Projection Shortcut)**。当维度发生变化时使用。
        - **何时发生变化？**
            1.  **空间尺寸减半**: 当卷积层步长为2时（例如从56x56 -> 28x28）。
            2.  **通道数增加**: 当进入下一个阶段时（例如从64 -> 128通道）。
        - **如何处理？** 快捷连接通路上的 $x$ 也必须进行相应的变换来匹配 $F(x)$ 的维度，才能进行相加。这通常是通过一个带步长为2的 **1x1 卷积**来实现的。

**作用**: 这些快捷连接构成了[[残差学习]]的核心，解决了网络退化问题，使得这个34层的网络不仅能够成功训练，而且性能远超其18层的版本以及34层的朴-素版本。

### 总结

这张图通过一个精心设计的对比实验，雄辩地证明了：
1.  简单地堆叠层数（如 **Plain-34**）会导致性能下降（退化）。
2.  通过引入几乎零成本的**快捷连接**（如 **ResNet-34**），就可以解决退化问题，并从增加的网络深度中获得显著的性能提升。
3.  ResNet 相比于 VGG 等早期模型，可以用更少的计算量（FLOPs）和参数达到更好的效果。