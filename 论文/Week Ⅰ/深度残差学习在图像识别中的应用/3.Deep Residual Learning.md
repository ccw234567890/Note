好的，遵照您的指令，我将严格依据您提供的PDF内容，开始对**第三章 Deep Residual Learning**进行逐句深度解析。

---

### **3.1:1**

- **原文 (Original):**
    
    - Let us consider H(x) as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the first of these layers.
        
- **总结 (Summary):**
    
    - 作者首先定义了一个基础场景：假设有一个期望被几层堆叠网络所拟合的目标映射H(x)，其中x是这几层网络块的输入。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个典型的设定数学讨论前提和符号定义的句子。结构为：Let us consider [symbol] as an [concept] to be fit by [components], with [another symbol] denoting [its definition].
        
- **知识点 (Knowledge Points):**
    
    - [[Underlying Mapping]]: 再次强调，这是指网络层理论上想要学习和模拟的理想函数，此处用H(x)表示。 #AI/DeepLearning/Fundamentals
        
    - #Paper/ResNet/Formulation: 开始对残差学习进行形式化数学描述。
        

---

### **3.1:2**

- **原文 (Original):**
    
    - If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., H(x) − x (assuming that the input and output are of the same dimensions).
        
- **总结 (Summary):**
    
    - 作者提出了一个逻辑等价转换：如果我们相信多层非线性网络有能力逼近任何复杂函数（即通用逼近定理），那么我们也应该相信它们同样有能力逼近这些复杂函数与输入之间的差值，即残差函数H(x)-x。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个基于一个公认假设进行逻辑推导的句子，为后续的理论重构铺平道路。结构为：If one hypothesizes that [accepted theorem], then it is equivalent to hypothesize that [logical consequence of the theorem].
        
- **知识点 (Knowledge Points):**
    
    - [[Universal Approximation Theorem]]: 深度学习的一个重要理论基石，指出一个具有足够多神经元的单隐藏层前馈网络可以以任意精度逼近任意连续函数。 #AI/DeepLearning/Theory
        
    - [[Residual Function]]: 此处从理论上证明了残差函数H(x)-x同样是可学习的，只要原始函数H(x)是可学习的。 #Paper/ResNet/CoreConcept
        

---

### **3.1:3**

- **原文 (Original):**
    
    - So rather than expect stacked layers to approximate H(x), we explicitly let these layers approximate a residual function F(x) := H(x) − x.
        
- **总结 (Summary):**
    
    - 因此，作者改变了学习目标：不再让网络层直接去逼近H(x)，而是明确地让它们去逼近残差函数F(x)，其定义为H(x) - x。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个明确提出本文核心方法论转变的句子。结构为：So rather than expect [components] to [old objective], we explicitly let these layers [new objective].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Formulation: 正式定义了残差块中非线性层需要学习的目标函数F(x)。
        

---

### **3.1:4**

- **原文 (Original):**
    
    - The original function thus becomes F(x)+x.
        
- **总结 (Summary):**
    
    - 这样一来，原始的目标函数H(x)就被重构为了F(x) + x。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个对数学重构结果进行陈述的句子。结构为：The original function thus becomes [new formulation].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Formulation: 明确了残差块的最终输出形式，即非线性层学习到的残差F(x)与输入x相加。
        

---

### **3.1:5**

- **原文 (Original):**
    
    - Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different.
        
- **总结 (Summary):**
    
    - 尽管H(x)和F(x)+x这两种形式在理论上都能逼近目标函数，但它们在学习难易度上可能存在差异。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个提出核心论点的辨析句，指出理论等价不代表实践效果相同。结构为：Although both forms should be able to [theoretical property], the [practical property] might be different.
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Hypothesis: 暗示了本文的核心假设：F(x)+x的形式比H(x)更容易学习。
        

---

### **3.1:6**

- **原文 (Original):**
    
    - This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left).
        
- **总结 (Summary):**
    
    - 作者指出，提出这种重构形式的动机，源于前面提到的、与直觉相悖的“网络退化”现象。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个将方法论与研究动机直接关联的句子。结构为：This reformulation is motivated by the counterintuitive phenomena about the [problem name].
        
- **知识点 (Knowledge Points):**
    
    - [[Degradation Problem]]: 再次强调本文旨在解决的核心问题。 #Paper/ResNet/Motivation
        

---

### **3.1:7**

- **原文 (Original):**
    
    - As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart.
        
- **总结 (Summary):**
    
    - 作者重申了引言中的核心论点：如果新增的层能够被构造成恒等映射，那么更深的模型训练误差不应该比浅层模型更高。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个回顾前文关键论证的句子，为接下来的分析做铺垫。结构为：As we discussed in [previous section], if [condition], a [conclusion follows].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Paradox: 回顾理论（深层网络不应更差）与实践（退化）的矛盾。
        

---

### **3.1:8**

- **原文 (Original):**
    
    - The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.
        
- **总结 (Summary):**
    
    - 退化问题的存在暗示了，优化器可能很难通过一堆非线性层来拟合出简单的恒等映射。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个对退化问题根源进行推测的句子。结构为：The [problem] suggests that the [agents] might have difficulties in [action].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Hypothesis: 提出了对退化问题原因的直接解释，即用非线性层拟合恒等映射在优化上是困难的。
        

---

### **3.1:9**

- **原文 (Original):**
    
    - With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.
        
- **总结 (Summary):**
    
    - 在残差学习的重构下，如果恒等映射是最优解，优化器或许只需简单地将非线性层的权重驱动至零，就能轻松实现（F(x)=0, 从而H(x)=x）。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个展示新方法如何轻松解决前述困难的句子。结构为：With the [new method], if [condition], the [agents] may simply [action] to [achieve goal].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Advantages: 阐述了残差学习的核心优势：将拟合恒等映射这个困难的优化问题，转化为了将权重参数趋近于零这个相对简单的优化问题。
        

---

### **3.1:10**

- **原文 (Original):**
    
    - In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition the problem.
        
- **总结 (Summary):**
    
    - 在实际情况中，[[解析：ResNet为何能轻松学习恒等映射]]不太可能是最优解，但本文的重构方法可能有助于对问题进行“预处理”，使其更容易优化。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个将理论分析推广到一般情况的句子。结构为：In real cases, it is unlikely that [ideal condition], but our reformulation may help to [practical benefit].
        
- **知识点 (Knowledge Points):**
    
    - [[Preconditioning]]: 借用数值分析的术语，指残差学习像一种预处理器，它改变了问题的“形状”，使得优化器可以更快地找到解。 #Science/NumericalMethods/Techniques
        

---

### **3.1:11**

- **原文 (Original):**
    
    - If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.
        
- **总结 (Summary):**
    
     如果[[最优函数]]相比一个[[全零映射]]更接近于一个[[恒等映射]]，那么优化器应该更容易在恒等映射的基础上寻找一个小的扰动（即残差），而不是从零开始学习一个全新的函数。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个对预处理效果进行更深入解释的句子。结构为：If the optimal function is closer to [reference A] than to [reference B], it should be easier for the solver to [action based on A], than to [action based on B].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Hypothesis: 进一步深化了核心假设，即学习“微调”比学习“重建”更容易。
        

---

### **3.1:12**
![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509100931335.png)


- **原文 (Original):**
    
    - We show by experiments ([[Fig. 7]]) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning.
        
- **总结 (Summary):**
    
    - 作者通过实验（图7）表明，网络学习到的残差函数通常[[响应值]]都很小，这从经验上支持了“恒等映射提供了一个合理的预处理”这一观点。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个提供实验证据来支撑理论假设的句子。结构为：We show by experiments ([figure reference]) that the [observed phenomenon], suggesting that [conclusion].
        
- -**知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Evidence: 提供了支持残差学习有效性的关键实验证据，即残差函数的响应值（magnitude）普遍较小。
        

---

### **3.2:1**

- **原文 (Original):**
    
    - We adopt residual learning to every few stacked layers.
        
- **总结 (Summary):**
    
    - 作者将残差学习应用到网络中每几个堆叠的层上。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个陈述方法应用策略的句子。结构为：We adopt [the method] to [the scope of application].
        
- **知识点 (Knowledge Points):**
    
    - [[Residual Block]]: 暗示了残差学习是以“块”（block）为单位在网络中应用的，而不是应用到每一层。 #Paper/ResNet/Architecture
        

---

### **3.2:2**
![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509101525597.png)


- **原文 (Original):**
    
    - A building block is shown in [[Fig. 2]].
        
- **总结 (Summary):**
    
    - 图2展示了一个基本的构建模块（即残差块）。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个引导读者查看图表的标准句子。结构为：A [component] is shown in Fig. [number].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Architecture: 指向展示核心结构——残差块的图示。
        

---

### **3.2:3**

- **原文 (Original):**
    
    - Formally, in this paper we consider a building block defined as: y = F(x, {Wi}) + x. (1)
        
- **总结 (Summary):**
    
    - 在本文中，一个构建块被形式化地定义为公式（1）：y = F(x, {Wi}) + x。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个给出核心结构数学定义的句子。结构为：Formally, in this paper we consider a [component] defined as: [equation].
        
- **知识点 (Knowledge Points):**
    
    - [[Residual Block]]: 提供了残差块的通用数学表达式，其中y是输出，x是输入，F是要学习的残差函数。 #Paper/ResNet/Architecture
        

---

### **3.2:4**

- **原文 (Original):**
    
    - Here x and y are the input and output vectors of the layers considered.
        
- **总结 (Summary):**
    
    - 此处x和y分别是被考量层的输入和输出向量。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个对公式中符号进行解释的句子。结构为：Here [symbol A] and [symbol B] are the [definition of A] and [definition of B] of the layers considered.
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Formulation: 解释公式(1)中的符号。
        

---

### **3.2:5**

- **原文 (Original):**
    
    - The function F(x, {Wi}) represents the residual mapping to be learned.
        
- **总结 (Summary):**
    
    - 函数F(x, {Wi})代表需要被学习的残差映射。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个对公式中关键函数进行解释的句子。结构为：The function [symbol] represents the [definition of the function].
        
- **知识点 (Knowledge Points):**
    
    - [[Residual Mapping]]: 明确F(x, {Wi})在公式中的角色。 #Paper/ResNet/CoreConcept
        

---

### **3.2:6**

- **原文 (Original):**
    
    - For the example in Fig. 2 that has two layers, F = W2σ(W1x) in which σ denotes ReLU and the biases are omitted for simplifying notations.
        
- **总结 (Summary):**
    
    - 对于图2中包含两层的例子，F可以具体表示为 F = W2σ(W1x)，其中σ代表ReLU激活函数，为了简化表示省略了偏置项。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个将通用公式实例化为一个具体例子的句子。结构为：For the example in [figure reference] that has [number] layers, [symbol F] = [specific formula] in which [symbol σ] denotes [activation function] and [other components] are omitted for simplifying notations.
        
- **知识点 (Knowledge Points):**
    
    - [[ReLU (Rectified Linear Unit)]]: 一种常用的非线性激活函数。 #AI/DeepLearning/Layers
        
    - [[Bias (Neural Networks)]]: 神经网络中的偏置项，此处被省略。 #AI/DeepLearning/Fundamentals
        

---

### **3.2:7**

- **原文 (Original):**
    
    - The operation F + x is performed by a shortcut connection and element-wise addition.
        
- **总结 (Summary):**
    
    - F+x这个操作是通过一个快捷连接和逐元素相加来实现的。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个解释数学运算如何通过网络结构实现的句子。结构为：The operation [formula] is performed by a [structural component] and [mathematical operation].
        
- **知识点 (Knowledge Points):**
    
    - [[Shortcut Connection]]: 快捷连接的物理作用是传递x。 #Paper/ResNet/Architecture
        
    - [[Element-wise Addition]]: 逐元素相加，是融合F和x的数学操作。 #AI/Mathematics/Operations
        

---

### **3.2:8**

- **原文 (Original):**
    
    - We adopt the second nonlinearity after the addition (i.e., σ(y), see Fig. 2).
        
- **总结 (Summary):**
    
    - 作者选择在相加操作之后再应用第二次非线性激活函数（即σ(y)）。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个阐明具体架构设计选择的句子。结构为：We adopt the [component] after the [operation] (i.e., [formula], see [figure reference]).
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Architecture: 描述了ResNet V1版本的一个重要设计细节：先加和，后激活 (add -> ReLU)。后续研究（如ResNet V2）对此进行了改进。
        

---

### **3.2:9**

- **原文 (Original):**
    
    - The shortcut connections in Eqn.(1) introduce neither extra parameter nor computation complexity.
        
- **总结 (Summary):**
    
    - 公式(1)中的快捷连接（指恒等连接）既不引入额外的参数也不增加计算复杂度。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个重申方法高效性的句子。结构为：The [component] in [equation] introduce neither extra [cost A] nor [cost B].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Advantages: 再次强调恒等快捷连接的零成本优势。
        

---

### **3.2:10**

- **原文 (Original):**
    
    - This is not only attractive in practice but also important in our comparisons between plain and residual networks.
        
- **总结 (Summary):**
    
    - 这一点不仅在实践中很有吸引力，而且对于本文中普通网络和残差网络的公平比较也至关重要。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个阐述某项优势双重重要性的句子。结构为：This is not only [advantage A] but also [advantage B].
        
- **知识点 (Knowledge Points):**
    
    - [[Ablation Study]]: 暗示了为了进行严格的对比实验（消融研究），需要控制变量。零成本的快捷连接使得“plain”和“residual”网络在参数量和计算量上可以保持一致。 #AI/Methodology/Experiments
        

---

### **3.2:11**

- **原文 (Original):**
    
    - We can fairly compare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).
        
- **总结 (Summary):**
    
    - 因此，作者可以公平地比较那些参数量、深度、宽度和计算成本都完全相同的普通网络和残差网络（除了可以忽略不计的逐元素加法）。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个详细说明实验公平性如何被保证的句子。结构为：We can fairly compare [model A]/[model B] that simultaneously have the same [metric 1], [metric 2], [metric 3], and [metric 4] (except for [negligible difference]).
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Methodology: 详细说明了本文实验设计的严谨性。
        

---

### **3.2:12**

- **原文 (Original):**
    
    - The dimensions of x and F must be equal in Eqn.(1).
        
- **总结 (Summary):**
    
    - 在公式(1)中，输入x和残差F的维度必须是相等的。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个指出公式成立前提条件的句子。结构为：The dimensions of [symbol A] and [symbol B] must be equal in [equation].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Architecture: 提出了恒等快捷连接的一个限制：输入和输出维度需一致。
        

---

### **3.2:13**

- **原文 (Original):**
    
    - If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection Ws by the shortcut connections to match the dimensions: y = F(x, {Wi}) + Wsx. (2)
        
- **总结 (Summary):**
    
    - 如果维度不匹配（例如，当输入/输出通道数改变时），可以通过快捷连接路径上的一个线性投影Ws来匹配维度，如公式(2)所示。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个为前述限制提供解决方案的句子。结构为：If this is not the case (e.g., [example]), we can perform a [operation] by the [component] to [purpose]: [new equation].
        
- **知识点 (Knowledge Points):**
    
    - [[Projection Shortcut]]: 提出了一种当维度变化时使用的快捷连接变体，它使用一个线性变换（通常是1x1卷积）来调整x的维度。 #Paper/ResNet/Architecture
        

---

### **3.2:14**

- **原文 (Original):**
    
    - We can also use a square matrix Ws in Eqn.(1).
        
- **总结 (Summary):**
    
    - 作者指出，即使维度匹配，也可以在公式(1)中使用一个方阵Ws进行投影。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个提出另一种可选方案的句子。结构为：We can also use a [component variation] in [equation].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Architecture: 提出了投影快捷连接的另一种用法，即在维度不变时也使用投影，这会引入更多参数。
        

---

### **3.2:15**

- **原文 (Original):**
    
    - But we will show by experiments that the identity mapping is sufficient for addressing the degradation problem and is economical, and thus Ws is only used when matching dimensions.
        
- **总结 (Summary):**
    
    - 但作者将通过实验表明，仅用恒等映射已足以解决退化问题且更经济，因此投影Ws只在需要匹配维度时才使用。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个基于实验结果阐明设计选择偏好的句子。结构为：But we will show by experiments that [option A] is sufficient for [purpose] and is [advantage], and thus [option B] is only used when [condition].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Methodology: 阐明了本文主要采用的设计选择（Option A：尽可能用恒等映射，必要时才用投影映射），并预告了其有效性将由实验证明。
        

---

### **3.2:16**

- **原文 (Original):**
    
    - The form of the residual function F is flexible.
        
- **总结 (Summary):**
    
    - 残差函数F的形式是灵活的。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个强调方法灵活性的句子。结构为：The form of the [component] is flexible.
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Architecture: 指出残差块内部的结构可以有多种变化。
        

---

### **3.2:17**

- **原文 (Original):**
    
    - Experiments in this paper involve a function F that has two or three layers (Fig. 5), while more layers are possible.
        
- **总结 (Summary):**
    
    - 本文的实验主要采用了包含两层或三层的残差函数F，但也可以使用更多层。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个说明本文实验具体采用了哪种形式的句子。结构为：Experiments in this paper involve a [component] that has [variation A] or [variation B], while [other variations] are possible.
        
- **知识点 (Knowledge Points):**
    
    - [[Basic Block]]: 指包含两层卷积的残差块，用于较浅的ResNet（如ResNet-18/34）。 #Paper/ResNet/Architecture
        
    - [[Bottleneck Block]]: 指包含三层（1x1, 3x3, 1x1）卷积的残差块，用于较深的ResNet（如ResNet-50/101/152）。 #Paper/ResNet/Architecture
        

---

### **3.2:18**

- **原文 (Original):**
    
    - But if F has only a single layer, Eqn.(1) is similar to a linear layer: y = W1x + x, for which we have not observed advantages.
        
- **总结 (Summary):**
    
    - 但如果F只包含一层，公式(1)就退化成一个线性层y = (W1+I)x，作者并未在这种结构上观察到性能优势。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个通过分析退化情况来设定方法有效边界的句子。结构为：But if [component] has only a [minimal form], [equation] is similar to a [simpler structure]: [formula], for which we have not observed advantages.
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Methodology: 指出了残差块的一个设计要点：残差函数F至少需要包含一个非线性激活函数，通常由至少两层构成，否则整个块近似于线性变换，效果不佳。
        

---

### **3.2:19**

- **原文 (Original):**
    
    - We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to convolutional layers.
        
- **总结 (Summary):**
    
    - 作者还指出，尽管为简化起见上述符号是基于全连接层的，但它们同样适用于卷积层。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个澄清符号适用范围的句子。结构为：We also note that although the above notations are about [case A] for simplicity, they are applicable to [case B].
        
- **知识点 (Knowledge Points):**
    
    - [[Convolutional Layer]]: 本文的核心方法主要应用于卷积层。 #AI/DeepLearning/Layers
        
    - [[Fully Connected Layer]]: 全连接层。 #AI/DeepLearning/Layers
        

---

### **3.2:20**

- **原文 (Original):**
    
    - The function F(x, {Wi}) can represent multiple convolutional layers.
        
- **总结 (Summary):**
    
    - 函数F可以代表多个卷积层。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个对前句进行具体说明的句子。结构为：The function [symbol] can represent multiple [component type].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Architecture: 明确了在CNN中，F就是残差块中的卷积层堆栈。
        

---

### **3.2:21**

- **原文 (Original):**
    
    - The element-wise addition is performed on two feature maps, channel by channel.
        
- **总结 (Summary):**
    
    - 在卷积网络中，逐元素相加是在两个特征图上逐通道进行的。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个解释核心操作在CNN中如何具体实现的句子。结构为：The [operation] is performed on two [data structures], [dimension] by [dimension].
        
- -**知识点 (Knowledge Points):**
    
    - [[Feature Map]]: 卷积层输出的多维数组。 #AI/DeepLearning/Fundamentals
        
    - #Paper/ResNet/Architecture: 解释了残差块中加法操作的具体细节。
        

---

### **3.3:1**

- **原文 (Original):**
    
    - We have tested various plain/residual nets, and have observed consistent phenomena.
        
- **总结 (Summary):**
    
    - 作者测试了多种普通网络和残差网络，并观察到了一致的现象。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个引出实验设计部分的总结句。结构为：We have tested various [model A]/[model B], and have observed consistent phenomena.
        
- **知识点 (Knowledge Points):**
    
    - [[Plain Network]]: 指没有使用快捷连接、仅由网络层简单堆叠而成的深度网络，作为本文的基线（baseline）模型。 #Paper/ResNet/Methodology
        

---

### **3.3:2**

- **原文 (Original):**
    
    - To provide instances for discussion, we describe two models for ImageNet as follows.
        
- **总结 (Summary):**
    
    - 为了便于讨论，作者接下来将描述两个用于ImageNet的模型实例。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个引出具体模型介绍的过渡句。结构为：To provide instances for discussion, we describe [number] models for [dataset] as follows.
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Methodology: 预告将要介绍用于ImageNet实验的具体网络架构。
        

---

### **3.3:3**

- **原文 (Original):**
    
    - Plain Network.
        
- **总结 (Summary):**
    
    - 小节标题：普通网络。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个标准的子章节标题。结构为：[Topic Name].
        
- **知识点 (Knowledge Points):**
    
    - [[Plain Network]]: 作为对比实验的基线模型。 #Paper/ResNet/Methodology
        

---

### **3.3:4**

- **原文 (Original):**
    
    - Our plain baselines (Fig. 3, middle) are mainly inspired by the philosophy of VGG nets (Fig. 3, left).
        
- **总结 (Summary):**
    
    - 作者的普通网络基线模型主要受到了VGG网络设计哲学（如图3左）的启发（如图3中）。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个阐明模型设计灵感来源的句子。结构为：Our plain baselines ([figure reference]) are mainly inspired by the philosophy of [another model] ([figure reference]).
        
- **知识点 (Knowledge Points):**
    
    - [[VGG Network]]: VGG的设计哲学以其简洁、规整著称，主要使用小的（3x3）卷积核并重复堆叠。 #AI/DeepLearning/Models
        

---

### **3.3:5**

- **原文 (Original):**
    
    - The convolutional layers mostly have 3×3 filters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of filters; and (ii) if the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer.
        
- **总结 (Summary):**
    
    - 其卷积层大多使用3x3滤波器，并遵循两条简单的设计规则：1) 输出特征图尺寸相同时，滤波器数量也相同；2) 如果特征图尺寸减半，滤波器数量则加倍，以保持每层的计算复杂度大致不变。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个详细列举模型设计准则的句子。结构为：The [components] mostly have [property] and follow two simple design rules: (i) [rule A]; and (ii) [rule B].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Architecture: 详细描述了本文基线网络和ResNet主体结构的设计原则，这些原则后来成为设计CNN的通用范式。
        

---

### **3.3:6**

- **原文 (Original):**
    
    - We perform downsampling directly by convolutional layers that have a stride of 2.
        
- **总结 (Summary):**
    
    - 下采样操作是直接通过步长为2的卷积层来完成的。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个说明具体操作实现方式的句子。结构为：We perform [operation] directly by [components] that have a [property].
        
- **知识点 (Knowledge Points):**
    
    - [[Downsampling]]: 降低特征图空间分辨率的操作。 #AI/DeepLearning/Operations
        
    - [[Stride]]: 卷积核在输入上滑动的步长。步长为2的卷积可以实现下采样。 #AI/DeepLearning/Operations
        

---

### **3.3:7**

- **原文 (Original):**
    
    - The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax.
        
- **总结 (Summary):**
    
    - 网络的末端是一个全局平均池化层，以及一个带softmax的1000路全连接层。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个描述网络分类头（classification head）结构的句子。结构为：The network ends with a [component A] and a [component B] with [activation function].
        
- **知识点 (Knowledge Points):**
    
    - [[Global Average Pooling (GAP)]]: 一种池化操作，将每个特征图的空间维度（高和宽）平均为一个值，常用于替代全连接层以减少参数。 #AI/DeepLearning/Layers
        
    - [[Softmax]]: 一种激活函数，常用于多分类任务的输出层，可将输出转换为各类别的概率分布。 #AI/DeepLearning/Layers
        

---

### **3.3:8**

- **原文 (Original):**
    
    - The total number of weighted layers is 34 in Fig. 3 (middle).
        
- **总结 (Summary):**
    
    - 图3（中）所示的这个基线模型共有34个含权重的层。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个报告模型具体深度的句子。结构为：The total number of weighted layers is [number] in [figure reference].
        
- **知识点 (Knowledge Points):**
    
    - [[ResNet-34]]: 预告了将要介绍的34层ResNet及其对应的Plain网络基线。 #AI/DeepLearning/Models
        

---

### **3.3:9**

- **原文 (Original):**
    
    - It is worth noticing that our model has fewer filters and lower complexity than VGG nets (Fig. 3, left).
        
- **总结 (Summary):**
    
    - 值得注意的是，作者设计的基线模型比VGG网络拥有更少的滤波器和更低的复杂度。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个通过对比强调模型高效性的句子。结构为：It is worth noticing that our model has [advantage A] and [advantage B] than [baseline model].
        
- **知识点 (Knowledge Points):**
    
    - #Paper/ResNet/Advantages: 指出本文提出的模型（包括基线和ResNet）在设计上比VGG更高效。
        

---

### **3.3:10**

- **原文 (Original):**
    
    - Our 34-layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs).
        
- **总结 (Summary):**
    
    - 作者的34层基线模型计算量为36亿FLOPs，仅为VGG-19模型（196亿FLOPs）的18%。
        
- **句子结构 (Sentence Structure):**
    
    - 这是一个通过具体数值对比来量化模型高效性的句子。结构为：Our [model version] has [value A] FLOPs, which is only [percentage]% of [baseline model] ([value B] FLOPs).
        
- **知识点 (Knowledge Points):**
    
    - [[FLOPs (Floating Point Operations)]]: 衡量模型计算复杂度的常用指标。 #AI/Terminology/Evaluation