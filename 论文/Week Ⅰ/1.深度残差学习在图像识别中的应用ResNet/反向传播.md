# 算法学习：反向传播 (Back-propagation)

**反向传播**（Back-propagation）是训练神经网络的核心算法。其本质是**利用链式法则（Chain Rule）高效计算损失函数对网络中每一个参数（权重 $w$ 和偏置 $b$）的梯度**。计算出梯度后，即可通过[[梯度下降]]法更新参数，最终最小化损失函数。

---

## 1. 模型设定与符号定义

以一个三层全连接神经网络（输入层、一个隐藏层、输出层）为例进行推导。

### 网络结构
- **输入层**: $i$ 个神经元, 输入向量 $x = [x_1, x_2, ..., x_i]^T$
- **隐藏层**: $j$ 个神经元
- **输出层**: $k$ 个神经元

### 符号定义
- $w_{ij}$: 输入层神经元 $i$ 到隐藏层神经元 $j$ 的权重
- $b_j$: 隐藏层神经元 $j$ 的偏置
- $h_j$: 隐藏层神经元 $j$ 的输出
- $w_{jk}$: 隐藏层神经元 $j$ 到输出层神经元 $k$ 的权重
- $b_k$: 输出层神经元 $k$ 的偏置
- $o_k$: 输出层神经元 $k$ 的输出 (网络预测值)
- $\sigma(\cdot)$: 激活函数, 此处以 Sigmoid 为例。
    - $\sigma(z) = \frac{1}{1 + e^{-z}}$
    - 导数性质: $\sigma'(z) = \sigma(z)(1 - \sigma(z))$
- $t_k$: 目标值（真实标签）的第 $k$ 个分量
- $L$: 损失函数, 此处以均方误差 (MSE) 为例。
    $$L = \frac{1}{2} \sum_{k=1}^{K} (t_k - o_k)^2$$

**目标**: 计算 $\frac{\partial L}{\partial w_{jk}}$, $\frac{\partial L}{\partial b_k}$, $\frac{\partial L}{\partial w_{ij}}$, $\frac{\partial L}{\partial b_j}$。

---

## 2. 前向传播 (Forward Propagation)

数据从输入到输出的计算流程。

1.  **隐藏层计算**:
    - 加权输入: $z_{h_j} = \sum_{i=1}^{I} w_{ij}x_i + b_j$
    - 激活输出: $h_j = \sigma(z_{h_j})$

2.  **输出层计算**:
    - 加权输入: $z_{o_k} = \sum_{j=1}^{J} w_{jk}h_j + b_k$
    - 激活输出: $o_k = \sigma(z_{o_k})$

3.  **计算总损失**:
    $$L = \frac{1}{2} \sum_{k=1}^{K} (t_k - o_k)^2$$

---

## 3. 反向传播 (Backward Propagation) - 公式推导

基于**链式法则**，从后向前逐层计算梯度。

### 第一步：计算输出层的梯度

#### A. 权重 $w_{jk}$ 的梯度

$$\frac{\partial L}{\partial w_{jk}} = \frac{\partial L}{\partial o_k} \times \frac{\partial o_k}{\partial z_{o_k}} \times \frac{\partial z_{o_k}}{\partial w_{jk}}$$

- $\frac{\partial L}{\partial o_k} = -(t_k - o_k)$
- $\frac{\partial o_k}{\partial z_{o_k}} = \sigma'(z_{o_k}) = o_k(1 - o_k)$
- $\frac{\partial z_{o_k}}{\partial w_{jk}} = h_j$

合并得:
$\frac{\partial L}{\partial w_{jk}} = -(t_k - o_k) \cdot o_k(1 - o_k) \cdot h_j$

> 为了简化表达，定义**输出层误差项 $\delta_k$**：
> $$\delta_k \equiv \frac{\partial L}{\partial z_{o_k}} = \frac{\partial L}{\partial o_k} \frac{\partial o_k}{\partial z_{o_k}} = -(t_k - o_k) \cdot o_k(1 - o_k)$$

因此，权重梯度可以写为：
$$\boxed{\frac{\partial L}{\partial w_{jk}} = \delta_k h_j}$$

#### B. 偏置 $b_k$ 的梯度
$$\frac{\partial L}{\partial b_k} = \frac{\partial L}{\partial z_{o_k}} \times \frac{\partial z_{o_k}}{\partial b_k} = \delta_k \times 1$$
$$\boxed{\frac{\partial L}{\partial b_k} = \delta_k}$$

### 第二步：计算隐藏层的梯度

#### A. 权重 $w_{ij}$ 的梯度
$$\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial h_j} \times \frac{\partial h_j}{\partial z_{h_j}} \times \frac{\partial z_{h_j}}{\partial w_{ij}}$$

- $\frac{\partial z_{h_j}}{\partial w_{ij}} = x_i$
- $\frac{\partial h_j}{\partial z_{h_j}} = \sigma'(z_{h_j}) = h_j(1 - h_j)$
- $\frac{\partial L}{\partial h_j}$: 隐藏层输出 $h_j$ 对所有输出神经元都有影响，需要求和。
    $$
    \frac{\partial L}{\partial h_j} = \sum_{k=1}^{K} \frac{\partial L}{\partial z_{o_k}} \frac{\partial z_{o_k}}{\partial h_j} = \sum_{k=1}^{K} \delta_k w_{jk}
    $$

> 同理，定义**隐藏层误差项 $\delta_j$**:
> $$
\delta_j \equiv \frac{\partial L}{\partial z_{h_j}} = \frac{\partial L}{\partial h_j} \frac{\partial h_j}{\partial z_{h_j}} = \left( \sum_{k=1}^{K} \delta_k w_{jk} \right) h_j(1 - h_j)
$$
> 这个公式体现了误差的反向传播：**当前层的误差**是**后一层误差**的加权和，再乘以本层激活函数的导数。

因此，权重梯度可以写为：
$$\boxed{\frac{\partial L}{\partial w_{ij}} = \delta_j x_i}$$

#### B. 偏置 $b_j$ 的梯度
$$\frac{\partial L}{\partial b_j} = \frac{\partial L}{\partial z_{h_j}} \times \frac{\partial z_{h_j}}{\partial b_j} = \delta_j \times 1$$
$$\boxed{\frac{\partial L}{\partial b_j} = \delta_j}$$

---

## 4. 算法流程总结

1.  **初始化**: 随机初始化所有权重 $w$ 和偏置 $b$。
2.  **前向传播**:
    - 输入样本 $x$，计算每一层的输出，直到得到最终预测值 $o$。
3.  **计算输出层误差**:
    - 根据预测值 $o$ 和真实值 $t$ 计算输出层误差 $\delta_k$。
4.  **反向传播误差**:
    - 从输出层开始，逐层向前计算每一隐藏层的误差 $\delta_j$。
5.  **计算梯度**:
    - 根据计算出的各层误差 $\delta$，计算每一层参数 $w, b$ 的梯度。
6.  **更新参数**:
    - 使用梯度下降法，根据学习率 $\eta$ 和计算出的梯度更新所有参数。
    - $w \leftarrow w - \eta \frac{\partial L}{\partial w}$
    - $b \leftarrow b - \eta \frac{\partial L}{\partial b}$
7.  **迭代**: 重复步骤 2-6，直到模型收敛。