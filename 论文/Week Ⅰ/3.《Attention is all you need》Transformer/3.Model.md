好的，遵照您的指令，我将严格依据您提供的PDF内容，开始对**第三章 Model Architecture**进行逐句深度解析。

---
### **3.0:1**

*   **原文 (Original):**
    *   Most competitive neural sequence transduction models have an encoder-decoder structure.

*   **总结 (Summary):**
    *   作者首先指出，当前大多数有竞争力的神经序列转换模型都采用了编码器-解码器结构。

*   **句子结构 (Sentence Structure):**
    *   这是一个介绍领域内标准架构的句子，为后续说明本文模型如何遵循此架构做铺垫。结构为：Most competitive [model type] have an [architecture name].

*   **知识点 (Knowledge Points):**
    *   [[Encoder-Decoder Architecture]]: 序列到序列任务的标准框架。 #AI/NLP/Architecture
    *   [[Sequence Transduction]]: #AI/NLP/Tasks

---
### **3.0:2**

*   **原文 (Original):**
    *   Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn).

*   **总结 (Summary):**
    *   在这个结构中，编码器的作用是将一个由符号表示（如单词ID）组成的输入序列，映射为一个连续表示（即向量）的序列z。

*   **句子结构 (Sentence Structure):**
    *   这是一个对编码器功能进行形式化定义的句子。结构为：Here, the encoder maps an input sequence of [input type] to a sequence of [output type].

*   **知识点 (Knowledge Points):**
    *   `[[Encoder]]`: 编码器-解码器架构的一部分，负责“理解”和编码输入信息。 #AI/NLP/Architecture
    *   [[Continuous Representation]]: 也叫向量嵌入（Vector Embedding），是将离散符号（如单词）映射到低维连续向量空间的技术。 #AI/NLP/Techniques

---
### **3.0:3**

*   **原文 (Original):**
    *   Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time.

*   **总结 (Summary):**
    *   在给定编码器输出z的条件下，解码器会逐个元素地生成一个由符号组成的输出序列。

*   **句子结构 (Sentence Structure):**
    *   这是一个对解码器功能进行形式化定义的句子。结构为：Given [input], the decoder then generates an output sequence of [output type] one element at a time.

*   **知识点 (Knowledge Points):**
    *   `[[Decoder]]`: 编码器-解码器架构的一部分，负责根据编码后的信息生成输出序列。 #AI/NLP/Architecture

---
### **3.0:4**

*   **原文 (Original):**
    *   At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.

*   **总结 (Summary):**
    *   在解码的每一步，模型都是“自回归”的，即它会将上一步已生成的符号作为下一步生成时的额外输入。

*   **句子结构 (Sentence Structure):**
    *   这是一个对解码器工作模式进行详细说明的句子。结构为：At each step the model is [property], consuming the [previous outputs] as additional input when generating the next.

*   **知识点 (Knowledge Points):**
    *   [[Auto-regressive Model]]: 一种生成模型，其在生成序列中第t个元素的条件概率，依赖于所有在t之前已生成的元素。 #AI/DeepLearning/Fundamentals

---
### **3.0:5**

*   **原文 (Original):**
    *   The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.

*   **总结 (Summary):**
    *   Transformer模型同样遵循这种通用的编码器-解码器架构，但其内部使用堆叠的自注意力和逐点全连接层来分别构建编码器和解码器，具体结构如图1的左右两半部分所示。

*   **句子结构 (Sentence Structure):**
    *   这是一个将本文模型与通用架构联系起来，并点明其独特内部组件的句子。结构为：The [Our Model] follows this overall architecture using [component A] and [component B] for both the encoder and decoder, shown in [figure reference].

*   **知识点 (Knowledge Points):**
    *   `[[Transformer]]`: #AI/DeepLearning/Models
    *   [[Self-Attention]]: Transformer的核心构建块。 #AI/NLP/Mechanisms
    *   [[Point-wise Feed-Forward Network]]: Transformer的另一个核心构建块。 #AI/NLP/Architecture

---
### **3.1:1**

*   **原文 (Original):**
    *   Encoder: The encoder is composed of a stack of N = 6 identical layers.

*   **总结 (Summary):**
    *   编码器由N=6个完全相同的层堆叠而成。

*   **句子结构 (Sentence Structure):**
    *   这是一个对编码器宏观结构进行描述的句子。结构为：Encoder: The encoder is composed of a stack of N = [number] identical layers.

*   **知识点 (Knowledge Points):**
    *   `[[Encoder]]`: #AI/NLP/Architecture
    *   `#Paper/Transformer/Architecture`: 开始详细介绍编码器的结构。

---
### **3.1:2**

*   **原文 (Original):**
    *   Each layer has two sub-layers.

*   **总结 (Summary):**
    *   每一层又包含两个子层。

*   **句子结构 (Sentence Structure):**
    *   这是一个对层级内部结构进行描述的句子。结构为：Each layer has [number] sub-layers.

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 描述编码器层的内部构成。

---
### **3.1:3**

*   **原文 (Original):**
    *   The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.

*   **总结 (Summary):**
    *   第一个子层是一个多头自注意力机制，第二个子层则是一个简单的、按位置进行的全连接前馈网络。

*   **句子结构 (Sentence Structure):**
    *   这是一个详细说明两个子层具体是什么的句子。结构为：The first is a [sub-layer A], and the second is a [sub-layer B].

*   **知识点 (Knowledge Points):**
    *   `[[Multi-Head Attention]]`: #Paper/Transformer/Architecture
    *   `[[Position-wise Feed-Forward Network]]`: #Paper/Transformer/Architecture

---
### **3.1:4**

*   **原文 (Original):**
    *   We employ a residual connection around each of the two sub-layers, followed by layer normalization.

*   **总结 (Summary):**
    *   作者在每个子层的外围都使用了一个残差连接，之后再接一个层归一化。

*   **句子结构 (Sentence Structure):**
    *   这是一个描述子层连接方式和后处理的句子。结构为：We employ a [mechanism A] around each of the two sub-layers, followed by [mechanism B].

*   **知识点 (Knowledge Points):**
    *   `[[Residual Connection]]`: 即ResNet中的跳跃连接，用于缓解梯度消失和加速训练。 #AI/DeepLearning/Architecture
    *   `[[Layer Normalization]]`: 一种归一化技术，与批量归一化（BN）不同，它在每个样本的特征维度上进行归一化，更适用于NLP任务。 #AI/DeepLearning/Layers

---
### **3.1:5**

*   **原文 (Original):**
    *   That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.

*   **总结 (Summary):**
    *   作者将前述结构形式化为：每个子层的输出是 LayerNorm(x + Sublayer(x))，其中x是子层的输入，Sublayer(x)是子层自身实现的函数。

*   **句子结构 (Sentence Structure):**
    *   这是一个对层结构进行数学形式化的句子。结构为：That is, the output of each sub-layer is [Formula], where [symbol] is the [definition].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 提供了Transformer中“Pre-LN”（先Add再Norm，实际上是Post-LN，后文有Pre-LN变体）结构的核心公式。

---
### **3.1:6**

*   **原文 (Original):**
    *   To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.

*   **总结 (Summary):**
    *   为了方便进行残差连接（要求输入和输出维度相同），模型中所有的子层以及嵌入层，都产生维度为dmodel=512的输出。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释模型维度设置原因的句子。结构为：To facilitate these [connections], all sub-layers in the model, as well as the [other layers], produce outputs of dimension dmodel = [value].

*   **知识点 (Knowledge Points):**
    *   `[[d_model]]`: Transformer模型中的核心维度，代表了模型中大部分向量表示的维度。 #Paper/Transformer/Hyperparameters

---
### **3.1:7**

*   **原文 (Original):**
    *   Decoder: The decoder is also composed of a stack of N = 6 identical layers.

*   **总结 (Summary):**
    *   解码器同样由N=6个完全相同的层堆叠而成。

*   **句子结构 (Sentence Structure):**
    *   这是一个对解码器宏观结构进行描述的句子。结构为：Decoder: The decoder is also composed of a stack of N = [number] identical layers.

*   **知识点 (Knowledge Points):**
    *   `[[Decoder]]`: #AI/NLP/Architecture
    *   `#Paper/Transformer/Architecture`: 开始详细介绍解码器的结构。

---
### **3.1:8**

*   **原文 (Original):**
    *   In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.

*   **总结 (Summary):**
    *   除了编码器层中已有的那两个子层外，解码器层还插入了第三个子层，该子层负责对编码器的最终输出执行多头注意力。

*   **句子结构 (Sentence Structure):**
    *   这是一个点明解码器与编码器结构差异的句子。结构为：In addition to the [common components], the decoder inserts a third sub-layer, which performs [its unique function].

*   **知识点 (Knowledge Points):**
    *   `[[Encoder-Decoder Attention]]`: 这个新增的第三个子层就是传统的编码器-解码器注意力，它将解码器的信息（作为Query）与编码器的信息（作为Key和Value）关联起来。 #AI/NLP/Mechanisms

---
### **3.1:9**

*   **原文 (Original):**
    *   Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.

*   **总结 (Summary):**
    *   与编码器类似，解码器也在每个子层外围使用残差连接和层归一化。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明解码器与编码器结构共性的句子。结构为：Similar to the encoder, we employ [mechanism A] around each of the sub-layers, followed by [mechanism B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 强调了解码器同样采用了Add & Norm结构。

---
### **3.1:10**

*   **原文 (Original):**
    *   We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.

*   **总结 (Summary):**
    *   作者还对解码器中的自注意力子层进行了修改，以防止当前位置注意到后续（未生成）的位置。

*   **句子结构 (Sentence Structure):**
    *   这是一个点明解码器自注意力层特殊性的句子。结构为：We also modify the [component] in the decoder stack to [purpose].

*   **知识点 (Knowledge Points):**
    *   `[[Masked Self-Attention]]`: 这是解码器自注意力的核心特点。为了保持自回归特性（即生成当前词时只能看到过去的词），必须将未来的位置信息“遮盖”掉。 #Paper/Transformer/Architecture

---
### **3.1:11**

*   **原文 (Original):**
    *   This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.

*   **总结 (Summary):**
    *   这种“遮盖”（masking）机制，再结合解码器输入（真实目标序列）相比输出（预测序列）整体右移一位的设定，共同确保了对位置i的预测只依赖于位置i之前已知的所有输出。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释前述修改如何保证自回归特性的句子。结构为：This [mechanism], combined with fact that the [another mechanism], ensures that the [desired property].

*   **知识点 (Knowledge Points):**
    *   `[[Right-Shift]]`: 在训练时，喂给解码器的输入是目标序列前面加一个起始符`<sos>`，而解码器要预测的目标是目标序列后面加一个结束符`<eos>`。这种错位确保了模型在预测第i个词时，输入的是第i-1个词。 #AI/NLP/Training
    *   `[[Auto-regressive Model]]`: #AI/DeepLearning/Fundamentals

---
### **3.2:1**

*   **原文 (Original):**
    *   An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.

*   **总结 (Summary):**
    *   作者首先从宏观上定义了注意力函数：它可以被描述为将一个“查询”（Query）和一组“键-值”（Key-Value）对映射到一个输出的过程，其中所有这些元素都是向量。

*   **句子结构 (Sentence Structure):**
    *   这是一个对核心概念进行高度抽象和形式化定义的句子。结构为：An [function] can be described as mapping a [input A] and a set of [input B] to an [output], where [all elements] are all vectors.

*   -**知识点 (Knowledge Points):**
    *   `[[Attention Mechanism]]`: #AI/NLP/Mechanisms
    *   `[[Query (Attention)]]`: 代表当前需要关注的信息或问题。 #Paper/Transformer/Attention
    *   `[[Key (Attention)]]`: 代表一个信息项的“索引”或“标签”，用于与Query进行匹配。 #Paper/Transformer/Attention
    *   `[[Value (Attention)]]`: 代表一个信息项的实际内容，是最终输出的加权求和对象。 #Paper/Transformer/Attention

---
### **3.2:2**

*   **原文 (Original):**
    *   The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.

*   **总结 (Summary):**
    *   注意力的输出是所有“值”向量的一个加权和，而每个“值”的权重，是通过一个“兼容性函数”计算“查询”向量与对应“键”向量之间的匹配度得出的。

*   **句子结构 (Sentence Structure):**
    *   这是一个对注意力计算过程进行分步解释的句子。结构为：The output is computed as a weighted sum of the [values], where the weight assigned to each value is computed by a [compatibility function] of the [query] with the corresponding [key].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Attention`: 详细描述了Attention机制的两步计算过程：1. 计算Query和所有Keys的相似度得到权重；2. 用权重对所有Values进行加权求和。

---
### **3.2.1:1**

*   **原文 (Original):**
    *   We call our particular attention "Scaled Dot-Product Attention".

*   **总结 (Summary):**
    *   作者将他们所使用的特定注意力机制命名为“缩放点积注意力”。

*   **句子结构 (Sentence Structure):**
    *   这是一个对本文具体使用的技术进行命名的句子。结构为：We call our particular [technology] "[Name]".

*   **知识点 (Knowledge Points):**
    *   `[[Scaled Dot-Product Attention]]`: Transformer中使用的注意力机制的具体形式。 #Paper/Transformer/Attention

---
### **3.2.1:2**

*   **原文 (Original):**
    *   The input consists of queries and keys of dimension dk, and values of dimension dv.

*   **总结 (Summary):**
    *   该机制的输入包括维度为dk的查询和键，以及维度为dv的值。

*   **句子结构 (Sentence Structure):**
    *   这是一个对输入进行维度说明的句子。结构为：The input consists of queries and keys of dimension [A], and values of dimension [B].

*   **知识点 (Knowledge Points):**
    *   `[[d_k]]`: 键（Key）和查询（Query）向量的维度。 #Paper/Transformer/Hyperparameters
    *   `[[d_v]]`: 值（Value）向量的维度。 #Paper/Transformer/Hyperparameters

---
### **3.2.1:3**

*   **原文 (Original):**
    *   We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values.

*   **总结 (Summary):**
    *   作者详细描述了权重的计算过程：1. 计算查询向量与所有键向量的点积；2. 将每个点积结果除以√dk进行缩放；3. 将缩放后的结果输入softmax函数，得到最终的权重。

*   **句子结构 (Sentence Structure):**
    *   这是一个详细描述计算步骤的句子。结构为：We [step 1], [step 2], and [step 3] to obtain the [result].

*   **知识点 (Knowledge Points):**
    *   `[[Dot-product Attention]]`: 指使用点积作为Query和Key之间兼容性函数的注意力机制。 #AI/NLP/Mechanisms
    *   `[[Scaling Factor (Attention)]]`: `1/√dk`这一项是本文的一个重要创新，用于防止点积结果过大导致softmax梯度消失。 #Paper/Transformer/Attention

---
### **3.2.1:4**

*   **原文 (Original):**
    *   In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.

*   **总结 (Summary):**
    *   在实际操作中，为了并行计算，作者将一批查询打包成一个矩阵Q，同时进行注意力计算。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释如何进行批处理计算的句子。结构为：In practice, we compute the attention function on a set of [elements] simultaneously, packed together into a matrix [symbol].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Implementation`: 描述了如何通过矩阵运算实现高效的并行计算。

---
### **3.2.1:5**

*   **原文 (Original):**
    *   The keys and values are also packed together into matrices K and V.

*   **总结 (Summary):**
    *   类似地，键和值也被分别打包成矩阵K和V。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释其他输入如何打包的句子。结构为：The keys and values are also packed together into matrices [symbol K] and [symbol V].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Implementation`: 描述了矩阵K和V的构成。

---
### **3.2.1:6**

*   **原文 (Original):**
    *   We compute the matrix of outputs as: Attention(Q, K, V ) = softmax( QKT / √dk )V (1)

*   **总结 (Summary):**
    *   作者给出了整个缩放点积注意力的矩阵计算公式。

*   **句子结构 (Sentence Structure):**
    *   这是一个给出核心算法矩阵形式公式的句子。结构为：We compute the matrix of outputs as: [Equation].

*   **知识点 (Knowledge Points):**
    *   `[[Scaled Dot-Product Attention]]`: 其核心的矩阵运算公式。 #Paper/Transformer/Attention

---
### **3.2.1:7**

*   **原文 (Original):**
    *   The two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention.

*   **总结 (Summary):**
    *   作者回顾了两种最常用的注意力函数：加性注意力和点积（乘性）注意力。

*   **句子结构 (Sentence Structure):**
    *   这是一个回顾相关工作的句子，用于将本文方法置于已有技术背景中。结构为：The two most commonly used [functions] are [function A], and [function B].

*   **知识点 (Knowledge Points):**
    *   `[[Additive Attention]]`: 由Bahdanau等人提出，使用一个带单隐藏层的前馈网络来计算Query和Key的兼容性。 #AI/NLP/Mechanisms
    *   `[[Dot-product Attention]]`: 由Luong等人提出，直接使用点积来计算兼容性。 #AI/NLP/Mechanisms

---
### **3.2.1:8**

*   **原文 (Original):**
    *   Dot-product attention is identical to our algorithm, except for the scaling factor of 1/√dk.

*   **总结 (Summary):**
    *   点积注意力与本文算法几乎完全相同，唯一的区别就是缺少了`1/√dk`这个缩放因子。

*   **句子结构 (Sentence Structure):**
    *   这是一个通过对比点明本文微小但关键创新的句子。结构为：[Method A] is identical to our algorithm, except for the [key difference].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Contributions`: 明确了本文在点积注意力上的核心改进是引入了缩放因子。

---
### **3.2.1:9**

*   **原文 (Original):**
    *   Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.

*   **总结 (Summary):**
    *   加性注意力使用一个带单隐藏层的前馈网络来计算兼容性函数。

*   **句子结构 (Sentence Structure):**
    *   这是一个对加性注意力进行更详细解释的句子。结构为：[Method A] computes the compatibility function using a [mechanism].

*   **知识点 (Knowledge Points):**
    *   `[[Additive Attention]]`: #AI/NLP/Mechanisms

---
### **3.2.1:10**

*   **原文 (Original):**
    *   While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.

*   **总结 (Summary):**
    *   尽管两者在理论复杂度上相似，但点积注意力在实践中速度更快、空间效率更高，因为它可以用高度优化的矩阵乘法代码来实现。

*   **句子结构 (Sentence Structure):**
    *   这是一个对比两种方法实践效率的句子。结构为：While the two are similar in [property A], [method B] is much [advantage A] and more [advantage B] in practice, since it can be implemented using [reason].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Motivation`: 解释了为什么选择点积注意力而非加性注意力作为基础。

---
### **3.2.1:11**

*   **原文 (Original):**
    *   While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk.

*   **总结 (Summary):**
    *   作者指出一个问题：虽然在dk较小时两者性能相似，但当dk较大时，加性注意力的性能会优于没有缩放的点积注意力。

*   **句子结构 (Sentence Structure):**
    *   这是一个点明点积注意力局限性的句子，为引入缩放因子提供了直接动机。结构为：While for small values of [parameter] the two mechanisms perform similarly, [method A] outperforms [method B] without scaling for larger values of [parameter].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Motivation`: 解释了为什么需要对点积注意力进行改进。

---
### **3.2.1:12**

*   **原文 (Original):**
    *   We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.

*   **总结 (Summary):**
    *   作者推测了前述问题的原因：当dk很大时，点积的结果在量级上会变得很大，这会将softmax函数推向其梯度极小的饱和区域，导致梯度消失。

*   **句子结构 (Sentence Structure):**
    *   这是一个对问题根源进行分析和猜想的句子。结构为：We suspect that for large values of [parameter], the [result] grow large in magnitude, pushing the [function] into regions where it has [negative consequence].

*   **知识点 (Knowledge Points):**
    *   `[[Softmax]]`: #AI/DeepLearning/Layers
    *   `[[Vanishing Gradients]]`: #AI/DeepLearning/Challenges

---
### **3.2.1:13**

*   **原文 (Original):**
    *   To counteract this effect, we scale the dot products by 1/√dk.

*   **总结 (Summary):**
    *   为了抵消这种影响，作者用`1/√dk`来对点积进行缩放。

*   **句子结构 (Sentence Structure):**
    *   这是一个明确提出解决方案的句子。结构为：To counteract this effect, we scale the [values] by [scaling factor].

*   **知识点 (Knowledge Points):**
    *   `[[Scaling Factor (Attention)]]`: 再次强调了缩放因子的作用。 #Paper/Transformer/Attention

---
### **3.2.2:1**

*   **原文 (Original):**
    *   Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively.

*   **总结 (Summary):**
    *   作者发现，与其用一组dmodel维的键、值、查询执行一次注意力计算，不如将查询、键和值分别通过h个不同的、可学习的线性投影，变换到更低的dk, dk和dv维度，这样做效果更好。

*   **句子结构 (Sentence Structure):**
    *   这是一个引出多头注意力核心思想的句子，通过对比“单次高维”和“多次低维”来阐述。结构为：Instead of performing a single [operation] with [high-dimensional inputs], we found it beneficial to linearly project the [inputs] [h] times with different, learned linear projections to [low dimensions], respectively.

*   **知识点 (Knowledge Points):**
    *   `[[Multi-Head Attention]]`: 正式引入多头注意力的概念。 #Paper/Transformer/Architecture
    *   `[[Linear Projection]]`: 指通过乘以一个权重矩阵来进行线性变换。 #AI/Mathematics/Operations

---
### **3.2.2:2**

*   **原文 (Original):**
    *   On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values.

*   **总结 (Summary):**
    *   然后，作者在这h组被投影后的查询、键和值上并行地执行注意力函数，每组都产生一个dv维的输出值。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释多头注意力计算过程的句子。结构为：On each of these projected versions of [inputs] we then perform the [operation] in parallel, yielding [output dimension] output values.

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 描述了多头注意力的并行计算过程。

---
### **3.2.2:3**

*   **原文 (Original):**
    *   These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.

*   **总结 (Summary):**
    *   最后，将这h个输出值拼接起来，再通过一次线性投影，得到最终的输出结果，如图2所示。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释多头注意力最后一步（结果汇总）的句子。结构为：These are [operation A] and once again [operation B], resulting in the final values, as depicted in [figure reference].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 描述了多头注意力如何整合多个头的输出。

---
### **3.2.2:4**

*   **原文 (Original):**
    *   Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.

*   **总结 (Summary):**
    *   作者阐述了多头注意力的核心优势：它允许模型在不同位置上，同时关注来自不同“表示子空间”的信息。

*   **句子结构 (Sentence Structure):**
    *   这是一个对多头注意力功能进行高度概括的句子。结构为：Multi-head attention allows the model to jointly attend to information from different [A] at different [B].

*   **知识点 (Knowledge Points):**
    *   `[[Representation Subspace]]`: 每个“头”可以被看作是关注了输入信息的某一个特定方面（如句法关系、语义关系等），这些方面就是不同的表示子空间。 #Paper/Transformer/Attention
    *   `#Paper/Transformer/Advantages`: 解释了为什么多头比单头好。

---
### **3.2.2:5**

*   **原文 (Original):**
    *   With a single attention head, averaging inhibits this.

*   **总结 (Summary):**
    *   如果只有一个注意力头，其平均化的过程会抑制模型学习这种多方面信息的能力。

*   **句子结构 (Sentence Structure):**
    *   这是一个通过反例来强调多头重要性的句子。结构为：With a single attention head, [negative effect] inhibits this.

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Motivation`: 提供了使用多头的动机。

---
### **3.2.2:6**

*   **原文 (Original):**
    *   MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW K i , V W V i )

*   **总结 (Summary):**
    *   作者给出了多头注意力的完整数学公式。

*   **句子结构 (Sentence Structure):**
    *   这是一个给出核心算法数学公式的句子。结构为：[Equation].

*   **知识点 (Knowledge Points):**
    *   `[[Multi-Head Attention]]`: 其核心数学公式。 #Paper/Transformer/Attention

---
### **3.2.2:7**

*   **原文 (Original):**
    *   Where the projections are parameter matrices W Q i, W K i, W V i and W O.

*   **总结 (Summary):**
    *   其中，所有的投影都是通过可学习的参数矩阵W来实现的。

*   **句子结构 (Sentence Structure):**
    *   这是一个对公式中符号进行解释的句子（原文为图注形式，此处整合）。结构为：Where the projections are parameter matrices [symbols].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 解释了公式中的参数矩阵。

---
### **3.2.2:8**

*   **原文 (Original):**
    *   In this work we employ h = 8 parallel attention layers, or heads.

*   **总结 (Summary):**
    *   在本文中，作者使用了h=8个并行的注意力层，即8个“头”。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供具体超参数设置的句子。结构为：In this work we employ h = [number] parallel attention layers, or heads.

*   **知识点 (Knowledge Points):**
    *   `[[h (number of heads)]]`: Transformer的一个关键超参数。 #Paper/Transformer/Hyperparameters

---
### **3.2.2:9**

*   **原文 (Original):**
    *   For each of these we use dk = dv = dmodel/h = 64.

*   **总结 (Summary):**
    *   对于每个头，作者设定其键、值、查询的维度dk和dv都为dmodel/h，即512/8=64。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供其他相关超参数设置的句子。结构为：For each of these we use [parameter A] = [parameter B] = [formula] = [value].

*   **知识点 (Knowledge Points):**
    *   `[[d_k]]`: #Paper/Transformer/Hyperparameters
    *   `[[d_v]]`: #Paper/Transformer/Hyperparameters

---
### **3.2.2:10**

*   **原文 (Original):**
    *   Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.

*   **总结 (Summary):**
    *   由于每个头的维度降低了，多头注意力的总计算成本与使用完整维度的单头注意力的成本是相似的。

*   **句子结构 (Sentence Structure):**
    *   这是一个分析多头注意力计算成本的句子。结构为：Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Advantages`: 指出了多头注意力的一个重要优点：在不显著增加计算成本的前提下提升了模型性能。

---
### **3.2.3:1**

*   **原文 (Original):**
    *   The Transformer uses multi-head attention in three different ways:

*   **总结 (Summary):**
    *   Transformer模型在三个不同的地方使用了多头注意力。

*   **句子结构 (Sentence Structure):**
    *   这是一个引出多头注意力具体应用的概括句。结构为：The Transformer uses multi-head attention in [number] different ways:

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 开始分类讨论多头注意力的三种用法。

---
### **3.2.3:2**

*   **原文 (Original):**
    *   In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.

*   **总结 (Summary):**
    *   第一种用法是在“编码器-解码器注意力”层中，其查询（Query）来自前一个解码器层，而键（Key）和值（Value）则来自编码器的输出。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释第一种用法（跨注意力）的句子。结构为：In "[usage name]" layers, the queries come from the [source A], and the memory keys and values come from the [source B].

*   **知识点 (Knowledge Points):**
    *   `[[Encoder-Decoder Attention]]`: 实现了传统的Encoder-Decoder注意力机制，用于将解码器和编码器联系起来。 #AI/NLP/Mechanisms

---
### **3.2.3:3**

*   **原文 (Original):**
    *   This allows every position in the decoder to attend over all positions in the input sequence.

*   **总结 (Summary):**
    *   这使得解码器中的每个位置都能注意到输入序列中的所有位置。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释前述用法功能的句子。结构为：This allows every position in the [decoder] to attend over all positions in the [encoder].

*   -**知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 描述了跨注意力的作用。

---
### **3.2.3:4**

*   **原文 (Original):**
    *   This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as.

*   **总结 (Summary):**
    *   这种方式模拟了传统序列到序列模型中典型的编码器-解码器注意力机制。

*   **句子结构 (Sentence Structure):**
    *   这是一个将其与传统方法联系起来的句子。结构为：This mimics the typical [mechanism] in [model family] such as [citations].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/RelatedWork`: 将Transformer的跨注意力与Bahdanau等人的传统注意力机制进行类比。

---
### **3.2.3:5**

*   **原文 (Original):**
    *   The encoder contains self-attention layers.

*   **总结 (Summary):**
    *   第二种用法是编码器包含的自注意力层。

*   **句子结构 (Sentence Structure):**
    *   这是一个引出第二种用法的句子。结构为：The encoder contains [usage name].

*   **知识点 (Knowledge Points):**
    *   `[[Self-Attention]]`: #AI/NLP/Mechanisms

---
### **3.2.3:6**

*   **原文 (Original):**
    *   In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.

*   **总结 (Summary):**
    *   在自注意力层中，所有的键、值和查询都来自同一个地方——即编码器中前一层的输出。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释第二种用法（编码器自注意力）的句子。结构为：In a self-attention layer all of the [inputs] come from the same place, in this case, the [source].

*   **知识点 (Knowledge Points):**
    *   `[[Self-Attention]]`: 再次强调了自注意力的核心特征：Q, K, V同源。 #AI/NLP/Mechanisms

---
### **3.2.3:7**

*   **原文 (Original):**
    *   Each position in the encoder can attend to all positions in the previous layer of the encoder.

*   **总结 (Summary):**
    *   这使得编码器中的每个位置都能注意到前一层的所有位置。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释编码器自注意力功能的句子。结构为：Each position in the [encoder] can attend to all positions in the [previous layer].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 描述了编码器自注意力的作用，即在编码器内部进行信息交互和整合。

---
### **3.2.3:8**

*   **原文 (Original):**
    *   Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.

*   **总结 (Summary):**
    *   第三种用法是解码器中的自注意力层，它允许解码器中的每个位置注意到解码器中截至当前位置（包括当前位置）的所有位置。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释第三种用法（解码器自注意力）的句子。结构为：Similarly, self-attention layers in the [decoder] allow each position in the [decoder] to attend to all positions in the [decoder] up to and including that position.

*   **知识点 (Knowledge Points):**
    *   `[[Masked Self-Attention]]`: #Paper/Transformer/Architecture

---
### **3.2.3:9**

*   **原文 (Original):**
    *   We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.

*   **总结 (Summary):**
    *   作者指出，为了保持自回归特性，必须防止解码器中的信息向“左”（即未来）流动。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释为何解码器自注意力需要特殊处理的句子。结构为：We need to prevent [forbidden information flow] in the decoder to preserve the [desired property].

*   **知识点 (Knowledge Points):**
    *   `[[Auto-regressive Model]]`: #AI/DeepLearning/Fundamentals

---
### **3.2.3:10**

*   **原文 (Original):**
    *   We implement this inside of scaled dot-product attention by masking out (setting to -∞) all values in the input of the softmax which correspond to illegal connections.

*   **总结 (Summary):**
    *   作者通过在缩放点积注意力的内部，将softmax输入中所有对应于非法连接（即连接到未来位置）的值都“遮盖”掉（设置为负无穷）来实现这一点。

*   **句子结构 (Sentence Structure):**
    *   这是一个详细描述如何实现遮盖机制的句子。结构为：We implement this inside of [module] by [action] all values in the input of the [function] which correspond to illegal connections.

*   **知识点 (Knowledge Points):**
    *   `[[Masking (Attention)]]`: 通过在计算softmax前将非法位置的注意力分数设置为负无穷，使得这些位置经过softmax后权重为0，从而实现信息遮盖。 #Paper/Transformer/Attention

---
### **3.3:1**

*   **原文 (Original):**
    *   In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.

*   **总结 (Summary):**
    *   除了注意力子层，编码器和解码器的每一层还包含一个全连接前馈网络，该网络被独立且相同地应用于序列中的每一个位置。

*   **句子结构 (Sentence Structure):**
    *   这是一个引出另一个核心组件——前馈网络的句子。结构为：In addition to [component A], each of the layers in our encoder and decoder contains a [component B], which is applied to each position separately and identically.

*   **知识点 (Knowledge Points):**
    *   `[[Position-wise Feed-Forward Network]]`: Transformer的另一个核心组件，用于在自注意力之后对每个位置的表示进行非线性变换，增加模型容量。 #AI/NLP/Architecture

---
### **3.3:2**

*   **原文 (Original):**
    *   This consists of two linear transformations with a ReLU activation in between.

*   **总结 (Summary):**
    *   这个前馈网络由两个线性变换构成，中间夹着一个ReLU激活函数。

*   **句子结构 (Sentence Structure):**
    *   这是一个描述前馈网络内部结构的句子。结构为：This consists of two [operations] with a [activation] in between.

*   **知识点 (Knowledge Points):**
    *   `[[ReLU (Rectified Linear Unit)]]`: #AI/DeepLearning/Layers

---
### **3.3:3**

*   **原文 (Original):**
    *   FFN(x) = max(0, xW1 + b1)W2 + b2 (2)

*   **总结 (Summary):**
    *   作者给出了前馈网络的数学公式。

*   **句子结构 (Sentence Structure):**
    *   这是一个给出核心组件数学公式的句子。结构为：[Equation].

*   **知识点 (Knowledge Points):**
    *   `[[Position-wise Feed-Forward Network]]`: 其数学公式。 #AI/NLP/Architecture

---
### **3.3:4**

*   **原文 (Original):**
    *   While the linear transformations are the same across different positions, they use different parameters from layer to layer.

*   **总结 (Summary):**
    *   虽然对于同一层内的不同位置，这两个线性变换是共享的，但不同层之间的前馈网络使用不同的参数。

*   **句子结构 (Sentence Structure):**
    *   这是一个澄清参数共享范围的句子。结构为：While the [operations] are the same across [dimension A], they use different parameters from [dimension B] to [dimension B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 解释了前馈网络的参数共享机制。

---
### **3.3:5**

*   **原文 (Original):**
    *   Another way of describing this is as two convolutions with kernel size 1.

*   **总结 (Summary):**
    *   描述这种操作的另一种方式是，可以把它看作是两个核大小为1的一维卷积。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供另一种等价视角的句子。结构为：Another way of describing this is as two [equivalent operations].

*   **知识点 (Knowledge Points):**
    *   `[[1x1 Convolution]]`: 全连接层可以被视为1x1卷积，反之亦然。这种视角有助于理解其“position-wise”的特性。 #AI/DeepLearning/Layers

---
### **3.3:6**

*   **原文 (Original):**
    *   The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.

*   **总结 (Summary):**
    *   这个前馈网络的输入和输出维度都是dmodel=512，而其内部隐藏层的维度dff则扩展到2048。

*   **句子结构 (Sentence Structure):**
    *   这是一个提供前馈网络维度设置的句子。结构为：The dimensionality of input and output is [A], and the inner-layer has dimensionality [B].

*   **知识点 (Knowledge Points):**
    *   `[[d_ff]]`: Transformer中前馈网络隐藏层的维度，通常是d_model的4倍。 #Paper/Transformer/Hyperparameters

---
### **3.4:1**

*   **原文 (Original):**
    *   Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.

*   **总结 (Summary):**
    *   与其他序列转换模型类似，作者使用可学习的嵌入层，将输入和输出的词元（tokens）转换为维度为dmodel的向量。

*   **句子结构 (Sentence Structure):**
    *   这是一个介绍模型输入输出处理的句子。结构为：Similarly to other [model family], we use [component] to convert the [inputs] and [outputs] to vectors of dimension [d_model].

*   **知识点 (Knowledge Points):**
    *   `[[Word Embedding]]`: 将离散的词元映射到连续向量空间的技术。 #AI/NLP/Techniques

---
### **3.4:2**

*   **原文 (Original):**
    *   We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.

*   **总结 (Summary):**
    *   作者同样使用常规的、可学习的线性变换和softmax函数，将解码器的最终输出转换为对下一个词元的预测概率分布。

*   **句子结构 (Sentence Structure):**
    *   这是一个介绍模型最终输出层的句子。结构为：We also use the usual [component A] and [component B] to convert the [decoder output] to [final prediction].

*   **知识点 (Knowledge Points):**
    *   `[[Softmax]]`: #AI/DeepLearning/Layers

---
### **3.4:3**

*   **原文 (Original):**
    *   In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to.

*   **总结 (Summary):**
    *   在Transformer模型中，作者让输入嵌入层、输出嵌入层以及最终的线性变换层共享同一套权重矩阵。

*   **句子结构 (Sentence Structure):**
    *   这是一个介绍一项重要参数共享技巧的句子。结构为：In our model, we share the same weight matrix between the [component A], [component B] and the [component C], similar to [citation].

*   **知识点 (Knowledge Points):**
    *   `[[Weight Tying]]`: 一种参数共享技术，在NLP中常用于共享输入嵌入和输出投影的权重，可以显著减少模型参数并提升性能。 #AI/NLP/Techniques

---
### **3.4:4**

*   **原文 (Original):**
    *   In the embedding layers, we multiply those weights by √dmodel.

*   **总结 (Summary):**
    *   在嵌入层中，作者将共享的权重乘以√dmodel。

*   **句子结构 (Sentence Structure):**
    *   这是一个补充说明实现细节的句子。结构为：In the embedding layers, we multiply those weights by [scaling factor].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Implementation`: 一个实现细节，一些研究认为这有助于稳定训练。

---
### **3.5:1**

*   **原文 (Original):**
    *   Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.

*   **总结 (Summary):**
    *   作者指出，由于模型中既没有循环也没有卷积，为了让模型能够利用序列的顺序信息，必须向模型中注入一些关于词元在序列中相对或绝对位置的信息。

*   **句子结构 (Sentence Structure):**
    *   这是一个引出位置编码必要性的句子。结构为：Since our model contains no [mechanism A] and no [mechanism B], in order for the model to [purpose], we must inject some information about the [missing information].

*   **知识点 (Knowledge Points):**
    *   `[[Positional Information]]`: 序列中元素的顺序信息，对于RNN和CNN是与生俱来的，但对于纯自注意力的Transformer则需要额外提供。 #AI/NLP/Architecture

---
### **3.5:2**

*   **原文 (Original):**
    *   To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks.

*   **总结 (Summary):**
    *   为此，作者在编码器和解码器栈底部的输入嵌入上，加上了“位置编码”。

*   **句子结构 (Sentence Structure):**
    *   这是一个提出解决方案的句子。结构为：To this end, we add "[component]" to the [location].

*   **知识点 (Knowledge Points):**
    *   `[[Positional Encoding]]`: 用于向Transformer模型中注入位置信息的一种向量。 #Paper/Transformer/Architecture

---
### **3.5:3**

*   **原文 (Original):**
    *   The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.

*   **总结 (Summary):**
    *   位置编码与词嵌入具有相同的维度dmodel，这样它们就可以直接相加。

*   **句子结构 (Sentence Structure):**
    *   这是一个说明位置编码维度和组合方式的句子。结构为：The [component A] have the same dimension as the [component B], so that the two can be [operation].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 描述了位置编码与词嵌入的融合方式。

---
### **3.5:4**

*   **原文 (Original):**
    *   There are many choices of positional encodings, learned and fixed.

*   **总结 (Summary):**
    *   位置编码有多种选择，可以是可学习的，也可以是固定的。

*   **句子结构 (Sentence Structure):**
    *   这是一个对技术方案进行分类的句子。结构为：There are many choices of [component], [type A] and [type B].

*   **知识点 (Knowledge Points):**
    *   `[[Learned Positional Encoding]]`: 一种将位置ID也当作词元一样，通过一个嵌入层来学习其向量表示的方法。 #AI/NLP/Techniques
    *   `[[Fixed Positional Encoding]]`: 一种使用固定的、预先设计好的函数来生成位置向量的方法。 #AI/NLP/Techniques

---
### **3.5:5**

*   **原文 (Original):**
    *   In this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/10000^(2i/dmodel)) PE(pos,2i+1) = cos(pos/10000^(2i/dmodel))

*   **总结 (Summary):**
    *   在本文中，作者使用了一种基于不同频率的正弦和余弦函数的固定位置编码方案。

*   **句子结构 (Sentence Structure):**
    *   这是一个给出本文具体选择并提供其数学公式的句子。结构为：In this work, we use [method]: [Equations].

*   **知识点 (Knowledge Points):**
    *   `[[Sinusoidal Positional Encoding]]`: Transformer论文中提出的经典固定位置编码方案。 #Paper/Transformer/Architecture

---
### **3.5:6**

*   **原文 (Original):**
    *   where pos is the position and i is the dimension.

*   **总结 (Summary):**
    *   其中，pos是位置索引，i是维度索引。

*   **句子结构 (Sentence Structure):**
    *   这是一个对公式中符号进行解释的句子。结构为：where [symbol A] is the [definition A] and [symbol B] is the [definition B].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Notation`: 解释位置编码公式中的符号。

---
### **3.5:7**

*   **原文 (Original):**
    *   That is, each dimension of the positional encoding corresponds to a sinusoid.

*   **总结 (Summary):**
    *   也就是说，位置编码的每一个维度都对应着一个正弦/余弦波。

*   **句子结构 (Sentence Structure):**
    *   这是一个对公式进行更直观解释的句子。结构为：That is, each dimension of the [vector] corresponds to a [curve].

*   **知识点 (Knowledge Points):**
    *   `[[Sinusoidal Positional Encoding]]`: #Paper/Transformer/Architecture

---
### **3.5:8**

*   **原文 (Original):**
    *   The wavelengths form a geometric progression from 2π to 10000 · 2π.

*   **总结 (Summary):**
    *   这些正弦/余弦波的波长构成了一个从2π到10000*2π的等比数列。

*   **句子结构 (Sentence Structure):**
    *   这是一个对函数属性进行描述的句子。结构为：The [property] form a [sequence type] from [start] to [end].

*   **知识点 (Knowledge Points):**
    *   `#Paper/Transformer/Architecture`: 描述了正弦位置编码的频率设计。

---
### **3.5:9**

*   **原文 (Original):**
    *   We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.

*   **总结 (Summary):**
    *   作者选择这种函数是因为他们假设这能让模型更容易地学习到基于相对位置的注意力，因为对于任意固定的偏移量k，PE(pos+k)都可以被表示为PE(pos)的一个线性函数。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释选择该方案背后动机和理论优势的句子。结构为：We chose this function because we hypothesized it would allow the model to [benefit], since for any fixed offset k, [mathematical property].

*   **知识点 (Knowledge Points):**
    *   `[[Relative Position]]`: 由于三角函数的性质`sin(a+b) = sin(a)cos(b) + cos(a)sin(b)`，`PE(pos+k)`可以由`PE(pos)`线性变换而来，这使得模型很容易捕捉到相对位置信息。 #AI/NLP/Techniques
    *   `#Paper/Transformer/Motivation`: 解释了选择正弦位置编码的理论依据。

---
### **3.5:10**

*   **原文 (Original):**
    *   We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).

*   **总结 (Summary):**
    *   作者也实验了可学习的位置嵌入，并发现两种版本的结果几乎完全相同。

*   **句子结构 (Sentence Structure):**
    *   这是一个报告对比实验结果的句子。结构为：We also experimented with using [alternative method] instead, and found that the two versions produced nearly identical results ([table reference]).

*   **知识点 (Knowledge Points):**
    *   `[[Learned Positional Encoding]]`: #AI/NLP/Techniques
    *   `#Paper/Transformer/Experiments`: 报告了两种位置编码方案的对比实验结果。

---
### **3.5:11**

*   **原文 (Original):**
    *   We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.

*   **总结 (Summary):**
    *   作者最终选择正弦版本，是因为它可能允许模型泛化到比训练时遇到的序列更长的序列。

*   **句子结构 (Sentence Structure):**
    *   这是一个解释最终选择背后考量的句子。结构为：We chose the [A] version because it may allow the model to [potential benefit].

*   **知识点 (Knowledge Points):**
    *   `[[Extrapolation]]`: 指模型在超出训练数据范围的输入上的泛化能力。固定的正弦函数可以为任意长度的序列生成位置编码，而可学习的位置编码则无法处理训练时未见过的位置。 #AI/DeepLearning/Fundamentals