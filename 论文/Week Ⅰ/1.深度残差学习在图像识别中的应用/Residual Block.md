# 模块：残差块 (Residual Block)

**标签**: #DeepLearning #ResNet #NetworkArchitecture #CNN

> [!quote] 核心论述
> [[Residual Block]]: 暗示了残差学习是以“块”（block）为单位在网络中应用的，而不是应用到每一层。

这句话精准地指出了 ResNet 设计的模块化本质。ResNet 的强大能力并非源于对单个网络层进行修改，而是通过堆叠设计精良的**“残差块”**来实现的。

---

## 1. 什么是残差块？

**残差块**是构成 [[残差网络 (ResNet)]] 的基本构建单元。它的核心设计在于，除了包含常规的非线性变换层外，还引入了一条**“快捷连接”（Shortcut Connection）**，允许输入信号绕过变换层，直接与变换层的输出相加。

**一个比喻**:
把每个残差块想象成一个**“自我修正”的学习模块**。
- **主通路 (Main Path)**: 模块内的非线性层（卷积层等）负责学习对输入信息进行复杂的**“变换”或“修正”**，我们称之为残差 $F(x)$。
- **快捷连接 (Shortcut Path)**: 原始的输入信息 $x$ 不经任何处理，直接走“高速公路”。
- **最终输出**: 模块最终的输出是**“原始信息” + “修正信息”**，即 $H(x) = F(x) + x$。

这种设计使得每个块的学习目标不再是拟合一个全新的、复杂的函数，而仅仅是学习**“在原始信息的基础上，还需要做哪些微调”**。

---

## 2. “块” (Block) 的含义：模块化单元

在“残差块”这个词里，**“块”（Block）** 的意思是**“模块”**或**“单元”**。它强调的是，残差学习这个思想是**以一组层为单位**来应用的，而不是零散地应用到网络的每一单个层。

> **乐高积木的比喻**:
> - **单个层** (如Conv, ReLU): 就像一颗颗基础的塑料粒子。
> - **残差块**: 就像一块标准的**乐高积木**。这块积木本身是由几颗塑料粒子（2-3个卷积层、BN层等）按照特定结构（主通路+快捷连接）预先组合好的。
>
> 构建一个 ResNet，就像用这些标准化的“残差块”乐高积木进行堆叠，而不是从零散的塑料粒子开始搭建。

### 为什么强调“块”这个概念很重要？

1.  **模块化设计 (Modularity)**:
    - 整个 ResNet 网络可以被看作是这些标准化的残差块的简单堆叠。这种模块化设计使得构建极深的网络变得非常容易和清晰。

2.  **功能单元**:
    - “块”暗示了这是一个**完整的功能单元**。快捷连接不是随便跨过一个层，而是跨过一整个进行特征提取和变换的计算单元（即主通路上的那几个层）。网络以“块”为单位，对特征进行逐级的提炼和修正。

---

## 3. 残差块的内部结构 (Anatomy)

一个典型的残差块由两部分组成：

#### A. 主通路 (Main Path) - 计算残差 $F(x)$
- 这部分通常由 **2 到 3 个卷积层**堆叠而成。
- 每个卷积层后面通常会跟随一个**[[Batch Normalization]]**层和一个**[[ReLU]]**激活函数，以稳定训练和增加非线性。
- `Input -> Conv -> BN -> ReLU -> Conv -> BN -> Output (F(x))`
- 这一系列变换的目的是学习输入 $x$ 与期望输出 $H(x)$ 之间的残差 $F(x)$。

#### B. 快捷连接 (Shortcut Connection) - 传递原始信息 $x$
- 这是连接块的输入与输出的“直连通道”。
- 它的作用是将块的初始输入 $x$ 原封不动地（或经过简单变换后）传递到块的末端。

#### C. 整合点 (Integration Point)
- 在块的末端，主通路的输出 $F(x)$ 和快捷连接的输出 $x$ 通过**逐元素相加 (element-wise addition)** 的方式进行合并。
- 合并后的结果再经过一次 **ReLU** 激活，作为整个残差块的最终输出。
- **最终输出**: $\text{output} = \text{ReLU}(F(x) + x)$

![Anatomy of a Residual Block](https://miro.medium.com/v2/resize:fit:1400/1*Qrh-p-A-l_n9Y3a0c_253w.png)
*上图：一个基础的残差块结构示意图。*

---

## 4. 残差块的两种主要类型

根据输入和输出维度是否一致，残差块分为两种：

### 1. 恒等块 (Identity Block)
- **使用场景**: 当输入和输出的维度（包括空间尺寸和通道数）**完全相同**时使用。
- **快捷连接**: 是一条纯粹的**[[恒等映射]]**通路，不进行任何操作，直接将输入 $x$ 传递过去。
- **对应图示**: 在[[图像解析：VGG-19 vs. Plain-34 vs. ResNet-34 架构对比]]中的**实线弧形箭头**。

### 2. 卷积块 (Convolutional Block / Projection Block)
- **使用场景**: 当输入和输出的维度**不匹配**时使用。这通常发生在：
    1.  **降低空间尺寸**: 例如，通过一个步长为 2 的卷积层，将特征图从 56x56 降为 28x28。
    2.  **改变通道数**: 例如，将通道数从 64 增加到 128。
- **快捷连接**: 为了让 $x$ 能够与 $F(x)$ 相加，快捷连接通路上也**必须进行维度变换**。这通常是通过一个**带步长的 1x1 卷积**来实现的。这个 1x1 卷积负责调整通道数和空间尺寸，以匹配主通路的输出。
- **对应图示**: 在对比图中的**虚线弧形箭头**。


## 关联概念
- [[残差网络 (ResNet)]]
- [[解析：ResNet为何能轻松学习恒等映射]]
- [[快捷连接 (Shortcut Connection)]]
- [[恒等映射]]
- [[全零映射]]
- [[Batch Normalization]]