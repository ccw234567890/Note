# 算法学习：梯度下降 (Gradient Descent)

**梯度下降**是一种基础且核心的**优化算法**，广泛用于机器学习和深度学习中，其主要目的是找到使[[损失函数]]最小化的模型参数。

---

## 1. 核心思想与直观理解

梯度下降的核心思想可以比作**“蒙眼下山”**。

> 想象一下你站在一座山的某一个位置，眼睛被蒙住，你的目标是尽快到达山谷的最低点。
> 1.  **寻找方向**：你伸出脚，在当前位置的四周探测，找到最陡峭的下坡方向。
> 2.  **迈出一步**：朝着这个最陡峭的方向迈出一步。
> 3.  **重复**：在新的位置上，重复以上步骤，直到你感觉自己到达了谷底（即位置不再有明显下降）。

在这个比喻中：
- **山**：就是我们要优化的**损失函数 (Loss Function)**。
- **当前位置**：是模型当前的**参数 (Parameters)**，例如权重 $w$ 和偏置 $b$。
- **山谷最低点**：是损失函数的**最小值点**，对应着模型的最优参数。
- **最陡峭的下坡方向**：就是损失函数在当前位置的**梯度 (Gradient)** 的**负方向**。
- **迈出的一步大小**：就是**学习率 (Learning Rate)**。

---

## 2. 数学公式

梯度下降的参数更新规则非常简洁。假设我们要优化的参数是 $\theta$ (它可以代表单个权重 $w$ 或所有参数的集合)，损失函数是 $L(\theta)$。

更新规则如下：
$$ \theta_{new} = \theta_{old} - \eta \cdot \nabla L(\theta_{old}) $$

**公式分解**:
- $\theta_{new}$: 更新后的参数值。
- $\theta_{old}$: 当前的参数值。
- $\eta$ (Eta): **学习率 (Learning Rate)**。这是一个超参数，控制着每一步更新的幅度（步子的大小）。
- $\nabla L(\theta_{old})$: **梯度 (Gradient)**。这是损失函数 $L$ 对参数 $\theta$ 的偏导数，表示在当前位置函数值增长最快的方向。因此，我们在它前面加上负号，表示朝着下降最快的方向更新。

---

## 3. 梯度下降的种类

根据每次更新参数时使用的数据量不同，梯度下降主要分为三种类型：

### A. 批量梯度下降 (Batch Gradient Descent)

- **过程**: 在每次更新参数时，使用**整个训练数据集**来计算梯度。
- **优点**:
    - 梯度计算准确，因为考虑了所有样本，能更好地代表全局趋势。
    - 损失函数下降的路径平滑，收敛过程稳定。
- **缺点**:
    - **计算成本高**：当数据集非常大时，每次迭代都需要处理所有数据，速度非常慢，内存开销也大。
    - 无法进行在线学习（来一个新样本就更新一次）。

### B. 随机梯度下降 (Stochastic Gradient Descent - SGD)

- **过程**: 每次更新参数时，从训练集中**随机选择一个样本**来计算梯度。
- **优点**:
    - **更新速度快**：每次迭代只用一个样本，计算量小。
    - 能够进行在线学习。
    - 梯度更新的随机性有助于跳出**局部最优解 (Local Minima)**，可能找到更好的全局最优解。
- **缺点**:
    - **收敛过程嘈杂**：由于单个样本的随机性，损失函数的下降路径非常曲折、震荡。
    - 可能永远不会精确收敛到最小值，而是在最小值附近徘徊。

### C. 小批量梯度下降 (Mini-Batch Gradient Descent)

- **过程**: 每次更新参数时，使用一小部分数据（一个 **mini-batch**，例如 32, 64, 128 个样本）来计算梯度。
- **优点**:
    - **平衡了 BGD 和 SGD 的优缺点**。
    - 相比 SGD，梯度计算更稳定，收敛路径更平滑。
    - 相比 BGD，计算效率高得多。
    - 能够充分利用现代 GPU 的并行计算能力。
- **缺点**:
    - 增加了一个超参数：`batch_size`。

> **实践中的选择**：
> **Mini-Batch Gradient Descent** 是目前最主流、最常用的方法，通常人们提到的 SGD 很多时候实际上指的就是 Mini-Batch SGD。

---

## 4. 学习率 (Learning Rate) 的重要性

学习率 $\eta$ 是一个至关重要的超参数，它的选择直接影响模型的训练效果：

- **学习率过小**:
    - **收敛速度慢**：像小碎步下山，需要非常多的迭代次数才能到达谷底。
    - **容易陷入局部最优**：一旦进入一个浅的坑，可能就很难再跳出来。

- **学习率过大**:
    - **可能无法收敛**：步子迈得太大，可能一步就跨过了谷底，导致在最小值点附近来回震荡，甚至可能跑到山的更高处，导致损失函数发散。

---

## 5. 挑战与优化

传统的梯度下降也面临一些挑战：

- **局部最优解 (Local Minima)**: 函数可能存在多个谷底，梯度下降只保证收敛到其中一个，但不一定是全局最低的那个。
- **鞍点 (Saddle Points)**: 在高维空间中，梯度为零的点更可能是鞍点而非局部最小值。在鞍点附近，梯度下降会变得非常缓慢。
- **Ravines (峡谷)**: 如果损失函数的形状像一个狭长的峡谷，梯度下降会在峡谷两侧来回震荡，而向谷底前进的速度很慢。

为了应对这些挑战，研究者提出了许多梯度下降的改进算法，例如：
- [[Momentum]]
- [[AdaGrad]]
- [[RMSprop]]
- [[Adam]] (目前非常流行的优化器)

这些算法通过引入动量、自适应学习率等机制，来加速收敛并改善稳定性。