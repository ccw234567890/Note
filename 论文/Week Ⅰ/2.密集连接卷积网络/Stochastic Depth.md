# 技术：随机深度 (Stochastic Depth)

**标签**: #DeepLearning #ResNet #Regularization #TrainingTechniques

> [!info] 核心思想
> **随机深度（Stochastic Depth）** 是一种专门为[[残差网络 (ResNet)]]设计的正则化方法。它的核心是在**训练期间**，随机地“**跳过**”或“**丢弃**”一部分[[残差块 (Residual Block)]]。
>
> 想象一下乘坐一条非常长的地铁线路（一个很深的 ResNet）：
> - **标准 ResNet**: 你必须在每一站（残差块）都停靠一下，即使某些站对你的最终目的地没什么帮助。
> - **使用随机深度的 ResNet**: 在每次行程（训练迭代）中，系统会**随机关闭一些车站**。当地铁遇到关闭的车站时，它会直接**“飞站”**通过，走直达线路（恒等映射快捷连接）。
>
> 这样一来，每次行程的“有效深度”都不同，整个训练过程就像是在训练一个由许多不同深度的网络组成的**网络集合（ensemble）**。这不仅大大缩短了训练时间，还起到了强大的正则化效果，提升了模型的最终性能。

---

## 1. 为什么需要随机深度？

尽管 [[残差网络 (ResNet)]] 使得训练非常深的网络成为可能，但当网络达到极深（如1202层）时，研究者发现了两个问题：

1.  **收益递减 (Diminishing Returns)**:
    - 增加更多的层带来的性能提升越来越小。
    - 梯度在反向传播时，虽然有快捷连接保护，但经过非常多的层之后，依然会变得微弱，导致网络**靠近输入端的浅层部分训练不充分**。

2.  **训练时间过长**:
    - 网络的深度与训练所需的时间和计算资源成正比。训练一个上千层的网络成本极高。

随机深度正是为了同时解决这两个问题而提出的。

---

## 2. 随机深度的工作原理

随机深度的机制在训练和推理（测试）阶段是不同的。

### A. 训练阶段 (Training Phase)

在每次训练迭代（处理一个 mini-batch）时，网络中的每一个残差块都会经历一次“生死考验”。

1.  **生存概率 ($p_l$)**:
    - 为网络中的第 $l$ 个残差块赋予一个**“生存概率” $p_l$**。这个概率表示该块在本次迭代中**被激活并参与计算**的可能性。
    - 作者提出了一个简单的**线性衰减规则**来设置这个概率：
      $$ p_l = 1 - \frac{l}{L}(1 - p_L) $$
      - $l$: 当前残差块的索引（第几个块）。
      - $L$: 网络中残差块的总数。
      - $p_L$: **最后一个**残差块的生存概率（这是一个超参数，例如设为 0.5）。$p_0$（输入）的生存概率固定为1。
    - **直觉**: 这个规则意味着，**网络越浅的层，越有可能被保留**（$p_l$ 接近1）；**网络越深的层，越有可能被丢弃**（$p_l$ 接近 $p_L$）。这是因为浅层负责提取基础、通用的特征，非常重要，而深层则学习更 специфичн 的特征，丢弃一些可以增强模型的鲁棒性。

2.  **随机丢弃**:
    - 在前向传播时，为每个块 $l$ 生成一个伯努利随机变量 $b_l \in \{0, 1\}$，其取值为1的概率为 $p_l$。
    - 根据 $b_l$ 的值，该残差块的输出 $H_l$ 按以下方式计算：
      $$
      H_l = \begin{cases} \text{ReLU}(F_l(H_{l-1}) + H_{l-1}) & \text{if } b_l = 1 \quad \text{(激活，标准ResNet行为)} \\ H_{l-1} & \text{if } b_l = 0 \quad \text{(丢弃，直接走恒等映射)} \end{cases}
      $$
    - 当一个块被丢弃（$b_l=0$）时，它的所有计算（卷积、BN等）都被跳过，大大节省了计算量。

### B. 推理/测试阶段 (Inference/Testing Phase)

在测试模型性能时，我们需要一个确定的、固定的网络。因此，**所有的残差块都会被激活**，不再有任何随机丢弃。

- **问题**: 训练时，每个块只以 $p_l$ 的概率参与贡献。而在测试时，它却100%参与。这会导致训练和测试时的期望输出不一致。
- **解决方案**: 为了补偿这种不一致，需要对每个残差块的输出进行**缩放**。
- **推理公式**:
  $$ H_l^{\text{test}} = \text{ReLU}(F_l(H_{l-1}^{\text{test}}) \cdot p_l + H_{l-1}^{\text{test}}) $$
- **解释**: 主通路（残差函数 $F_l$）的输出被乘以其在训练时的生存概率 $p_l$。这相当于取了训练时所有可能被激活的子网络的**期望输出**，使得模型的行为在测试时更加稳定和可预测。

---

## 3. 随机深度 vs. Dropout

| 特性 | 随机深度 (Stochastic Depth) | Dropout |
| :--- | :--- | :--- |
| **丢弃对象** | **整个层/块** (例如一个完整的残差块) | **单个神经元** |
| **应用网络** | 主要用于**带有快捷连接的网络** (如 ResNet) | 可用于各种网络 (尤其是全连接层) |
| **主要好处** | **减少训练时间** + 正则化 | 正则化 |
| **实现方式** | 随机选择执行主通路或快捷连接通路 | 随机将神经元的输出置为0 |

---

## 4. 优势总结

1.  **强大的正则化效果**: 通过训练一个隐式的网络集合，有效防止了过拟合，提升了模型的泛化能力。
2.  **显著减少训练时间**: 在训练时，平均只有一部分块参与计算，使得每次迭代的速度大大加快。原论文报告可减少约25%的训练时间。
3.  **改善梯度流**: 随机丢弃深层块，相当于创建了很多“更短”的网络进行训练，这使得梯度能够更有效地流向浅层，缓解了浅层网络训练不充分的问题。
4.  **支持训练极深网络**: 使得训练超过1000层的网络成为可能，并能获得持续的性能提升。