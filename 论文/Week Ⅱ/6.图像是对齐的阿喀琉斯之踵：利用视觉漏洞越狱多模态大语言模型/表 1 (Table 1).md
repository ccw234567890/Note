![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509241751802.png)
好的，我们来详细解读**表1 (Table 1)**。这张表格是这篇论文的核心实验证据，它通过严谨的量化数据，支撑了整篇文章最关键的几个论点。

### **一、 表格的总体目标**

这张表格的根本目的，是通过对比实验来回答以下几个问题：

1.  相比于纯文本模型，多模态大语言模型（MLLM）的安全性如何？
2.  仅仅是“加入图像”这个行为，会不会影响模型的安全性？
3.  图像的内容是否有害，对模型的安全性有多大影响？
4.  不同的模型训练方法（微调的参数多少）是否会影响模型的最终安全性？

### **二、 表格结构解析**

让我们先拆解一下表格的各个部分：

*   **Metric (衡量指标):** 表格中所有的数字都代表**攻击成功率 (Attack Success Rate, ASR)**，以百分比（%）表示。**ASR 越高，意味着模型越不安全**，因为它更容易被“越狱”并产生有害回答。
*   **Model(Train) (模型与训练策略):**
    *   **Model:** 列出了实验中使用的多个代表性 MLLM，包括开源的 LLaVA 系列、MiniGPT 系列，以及强大的闭源模型 Gemini Pro 和 GPT-4V。
    *   **(Train):** 括号里的词代表了该模型的**跨模态训练策略**，这一点至关重要：
        *   **Full:** 在训练时，模型的全部参数都参与了微调。
        *   **LORA:** 一种参数高效的微调方法，只训练了模型中一小部分的参数。
        *   **Frozen:** 训练时冻结了核心的大语言模型（LLM）骨干，只训练了连接图像和文本的“投影层”。
*   **Setting (评估设置):** 这是实验设计的核心，通过控制变量来测试不同情况下的 ASR。
    *   **Backbone:** **基线**。指的是 MLLM 所使用的那个**原始的、纯文本的**大语言模型。例如，LLaVA-1.5 的 Backbone 就是 Vicuna v1.5-7B。这代表了模型在变成多模态之前的原始安全水平。
    *   **Text-only:** 只向 MLLM 输入有害的**文本指令**，不附带任何图片。用于测试 MLLM 在处理纯文本时的安全性。
    *   **Blank:** 向 MLLM 输入有害文本指令，同时附带一张**纯空白的图片**。这个设置非常巧妙，因为它能分离出“激活视觉处理模块”这一行为本身带来的影响。
    *   **Toxic:** 向 MLLM 输入有害文本指令，同时附带一张与指令内容相关的**有害图片**（例如，指令是关于枪支，图片就是枪支）。
*   **Harmful Categories (有害类别):** 表格中间的五列（Animal, Financial, Privacy, Self-Harm, Violence）代表了五种不同类型的有害指令，说明了实验的覆盖面很广。
*   **Average(%) (平均成功率):** 这是最重要的结果列，是五种有害类别的平均 ASR。括号里的 `+` 或 `-` 数字，代表该设置下的 ASR **相对于 Backbone 基线的变化**，让结果一目了然。

### **三、 从表格中得出的三大核心发现**

这张表格的数据清晰地揭示了三个惊人的结论：

#### **发现一：图像是 MLLM 对齐的“后门” (Images can be alignment backdoors)**

这是最主要的发现。请重点观察 `Average(%)` 这一列，对比 `Text-only` 和 `Blank` 两个设置：

*   对于 **LLaVA-1.5(Full)** 模型，ASR 从 `Text-only` 的 26.13% **飙升**到了 `Blank` 的 **54.13%**，净增了 `+25.20%`。
*   对于 **MiniGPT-v2(LoRA)** 模型，ASR 从 `Text-only` 的 8.67% **暴增**到了 `Blank` 的 **34.00%**，净增了 `+33.87%`。
*   对于 **Gemini Pro**，ASR 也从 `Text-only` 的 0.00% 增加到了 `Blank` 的 **23.33%**。

**结论解读:** 这个结果极具说服力。仅仅是增加了一张**完全空白、不含任何信息的图片**，就导致模型的攻击成功率大幅上升。这表明，当 MLLM 的视觉处理模块被激活时，其内部的安全审查机制似乎被削弱或绕过了，为“越狱”打开了一个后门。

#### **发现二：微调的参数越多，对齐破坏得越严重 (More parameters tuned, less alignment left)**

这个发现需要我们对比不同训练策略的模型。

*   **比较 MiniGPT-v2 和 MiniGPT-4:** 它们使用相似的 LLM 骨干。MiniGPT-v2 经过了 LORA 微调，而 MiniGPT-4 的 LLM 骨干是冻结的 (Frozen)。在 `Toxic` 设置下，前者的 ASR 高达 **43.87%**，而后者仅为 **23.47%**。
*   **比较 LLaVA-1.5 和 LLaVA-1.5L:** 前者是全参数微调，后者是 LORA 微调。在 `Toxic` 设置下，前者的 ASR (68.13%) 也显著高于后者 (62.80%)。

**结论解读:** 实验数据表明，为了让模型学会理解图文关系而进行的跨模态微调，尤其是当微调触及的原始 LLM 参数越多时，似乎会对 LLM 原本通过对齐训练建立起来的安全护栏造成“附带伤害”。训练得越多，忘掉的安全知识也越多。

#### **发现三：有害的图像内容会进一步放大攻击效果**

这个发现通过对比 `Blank` 和 `Toxic` 两个设置得出。

*   对于**所有模型**，`Toxic` 设置下的 ASR 都显著高于 `Blank` 设置。
*   例如，LLaVA-1.5 的 ASR 从 `Blank` 的 54.13% 进一步提升到了 `Toxic` 的 **68.13%**。
*   Gemini Pro 也从 23.33% 提升到了 **30.67%**。

**结论解读:** 这符合直觉，但通过数据得到了验证。当输入的图像本身就包含有害内容时，它会和文本指令形成“共谋”，进一步诱导模型输出有害内容，让攻击的成功率变得更高。

### **总结**

**表1** 是这篇论文的基石。它并非简单地罗列数据，而是通过一个设计精巧的矩阵式实验，系统性地、全方位地证明了论文的核心论点：**视觉输入是当前多模态大语言模型安全性的一个巨大软肋**。无论是激活视觉模块的行为、跨模态训练的过程，还是图像本身的内容，都对模型的安全性构成了严重威胁。