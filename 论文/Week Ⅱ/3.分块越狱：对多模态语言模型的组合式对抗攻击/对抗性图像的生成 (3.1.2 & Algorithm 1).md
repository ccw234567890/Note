![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509251652091.png)
![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509251653900.png)
# 论文详解：对抗性图像的生成 (3.1.2 & Algorithm 1)

这部分内容是整个攻击技术的核心，它解释了如何将一张看似无害的图片（如随机噪声或风景照）“雕琢”成一个在模型“眼中”等同于恶意概念（如“炸弹”）的“特洛伊木马”。

---

## 1. 核心目标：在嵌入空间中“对齐”

我们的最终目的不是让生成的图片在**像素上**看起来像炸弹，而是让它在模型的**嵌入空间（Embedding Space）**中的坐标与“炸弹”的坐标几乎完全重合。

> **通俗理解：**
> 想象一个巨大的坐标系，"炸弹" 这个概念在 `(x1, y1)` 点。我们有一张随机图片，它的坐标在 `(x2, y2)` 点。我们的目标就是，通过不断微调这张随机图片的像素，让它的坐标从 `(x2, y2)` 一步步移动到 `(x1, y1)`。当两个坐标足够接近时，模型就无法区分它们了。

这个过程可以用一个最优化问题来数学化地描述：

$$
\underset{x_{adv}}{\arg\min} \mathcal{L} \left( \mathcal{I}_\phi(x_{adv}), H_{harm} \right)
$$

- **$x_{adv}$**: 我们要优化的**对抗性图像**。这是我们的变量。
- **$\mathcal{I}_\phi(\cdot)$**: 视觉编码器（Vision Encoder），比如 CLIP 的图像编码器。它的作用是接收一张图片，并输出其在嵌入空间中的坐标（一个高维向量）。
- **$\mathcal{I}_\phi(x_{adv})$**: 对抗性图像 $x_{adv}$ 的嵌入向量，我们称之为 $H_{adv}$。
- **$H_{harm}$**: 预先计算好的、固定的**目标恶意嵌入向量**（比如“炸弹”的坐标）。
- **$\mathcal{L}(\cdot, \cdot)$**: **损失函数 (Loss Function)**，用来衡量两个嵌入向量之间的“距离”。距离越大，说明我们的 $x_{adv}$ 离目标越远。
- **$\underset{x_{adv}}{\arg\min}$**: 这个符号表示，我们要找到一个 $x_{adv}$，使得损失函数 $\mathcal{L}$ 的值达到最小。

论文中使用的损失函数是**L2范数的平方**（Squared Euclidean Distance），这是一种非常常见的距离度量方式：

$$
\mathcal{L}(H_{adv}, H_{harm}) = || H_{adv} - H_{harm} ||_2^2 = \sum_{i=1}^{d} (h_{adv,i} - h_{harm,i})^2
$$

- $d$ 是嵌入向量的维度。
- $h_{adv,i}$ 和 $h_{harm,i}$ 分别是两个向量的第 $i$ 个分量。
- 我们将这个距离最小化，就等同于让两个向量在空间中尽可能地靠近。

---

## 2. 优化算法详解 (Algorithm 1)

作者使用了基于**梯度下降 (Gradient Descent)** 的方法来求解上述最优化问题。这个算法的核心思想是：**计算损失函数相对于输入图像像素的梯度，然后沿着梯度的反方向小步更新图像，从而逐步降低损失。**

下面我们结合数学推导，一步步拆解 Algorithm 1。

### **步骤 1: 初始化 (Initialization)**

- **`输入`**: 目标恶意嵌入 $H_{harm}$，一个空的或随机的初始图像 $x_{adv}^{(0)}$，学习率 $\eta$，损失阈值 $\tau$。
- **`操作`**: 这是算法的起点。$H_{harm}$ 是我们的“靶子”，是固定不变的。$x_{adv}^{(0)}$ 是我们的“画布”。

### **步骤 2: 迭代优化循环 (The Loop)**

算法会不断循环，直到损失 $\mathcal{L}$ 小于我们设定的阈值 $\tau$。在每一次循环（第 $t$ 次迭代）中，会执行以下关键操作：

#### **2.1 前向传播 (Forward Pass)**

- **`算法`**: `H_adv ← I_φ(x_adv)`
- **`目的`**: 计算当前对抗性图像 $x_{adv}^{(t)}$ 在嵌入空间中的坐标。
- **`数学表达`**:
$$
H_{adv}^{(t)} = \mathcal{I}_\phi(x_{adv}^{(t)})
$$

#### **2.2 计算损失 (Loss Calculation)**

- **`算法`**: `L ← || H_adv - H_harm ||_2^2`
- **`目的`**: 计算当前坐标 $H_{adv}^{(t)}$ 和目标坐标 $H_{harm}$ 之间的距离。这个数值告诉我们“偏离靶心”有多远。
- **`数学表达`**:
$$
\mathcal{L}^{(t)} = \sum_{i=1}^{d} (h_{adv,i}^{(t)} - h_{harm,i})^2
$$

#### **2.3 计算梯度 (Gradient Calculation via Backpropagation)**

- **`算法`**: `g ← ∇_x_adv L(I_φ(x_adv), H_harm)`
- **`目的`**: 这是最关键的一步。我们要计算损失 $\mathcal{L}$ 对**输入图像的每一个像素**的偏导数，即梯度 $\nabla_{x_{adv}}\mathcal{L}$。这个梯度是一个和输入图像尺寸完全相同的矩阵，它指明了让损失**增大**最快的方向。
- **`数学推导 (链式法则)`**:
    我们不能直接求 $\mathcal{L}$ 对 $x_{adv}$ 的导数，因为它们之间隔着一个复杂的神经网络 $\mathcal{I}_\phi$。因此，我们使用链式法则：

    $$
    \frac{\partial \mathcal{L}}{\partial x_{adv}} = \frac{\partial \mathcal{L}}{\partial H_{adv}} \cdot \frac{\partial H_{adv}}{\partial x_{adv}}
    $$

    1.  **第一部分: $\frac{\partial \mathcal{L}}{\partial H_{adv}}$** (损失对嵌入的梯度)
        这部分比较简单。对于损失函数的第 $j$ 个分量 $h_{adv,j}$，其偏导数为：
        $$
        \frac{\partial \mathcal{L}}{\partial h_{adv,j}} = \frac{\partial}{\partial h_{adv,j}} \sum_{i=1}^{d} (h_{adv,i} - h_{harm,i})^2 = 2(h_{adv,j} - h_{harm,j})
        $$
        所以，整个梯度向量是：
        $$
        \nabla_{H_{adv}}\mathcal{L} = 2(H_{adv} - H_{harm})
        $$
        这个向量的物理意义是：从当前嵌入 $H_{adv}$ 指向目标嵌入 $H_{harm}$ 的方向的反方向。

    2.  **第二部分: $\frac{\partial H_{adv}}{\partial x_{adv}}$** (嵌入对输入图像的梯度)
        这部分代表了视觉编码器 $\mathcal{I}_\phi$ 本身。它是一个巨大的**雅可比矩阵 (Jacobian Matrix)**，描述了输出嵌入的每个维度相对于输入图像的每个像素的变化率。我们**不需要手动计算**这个复杂的矩阵。深度学习框架（如 PyTorch, TensorFlow）的**自动微分（Autograd）**功能，通过**反向传播 (Backpropagation)** 算法，可以高效地为我们计算出最终的梯度 $\frac{\partial \mathcal{L}}{\partial x_{adv}}$。

#### **2.4 更新图像 (Update Step)**

- **`算法`**: `x_adv ← x_adv - η * g`
- **`目的`**: 根据计算出的梯度，更新图像的像素值。
- **`数学表达`**:
$$
x_{adv}^{(t+1)} = x_{adv}^{(t)} - \eta \cdot \nabla_{x_{adv}^{(t)}}\mathcal{L}
$$
- **`原理解释`**:
    - 梯度 $\nabla_{x_{adv}^{(t)}}\mathcal{L}$ 指向了让损失**上升**最快的方向。
    - 我们希望损失**下降**，所以我们沿着梯度的**反方向** (`-`) 进行更新。
    - $\eta$ (学习率) 控制了我们每一步“走多远”。如果 $\eta$太大，可能会“跨过”最低点；如果太小，优化速度会很慢。

### **步骤 3: 终止 (Termination)**

- 当 `L < τ` 时，循环停止。这意味着我们的对抗性图像的嵌入 $H_{adv}$ 已经和目标嵌入 $H_{harm}$ 足够接近，攻击准备完成。
- **`输出`**: 最终优化好的对抗性图像 $x_{adv}$。

---

## 3. 总结

| 算法步骤 | 通俗解释 |
| :--- | :--- |
| **初始化** | 准备好“靶子” ($H_{harm}$) 和一张空白“画布” ($x_{adv}^{(0)}$)。 |
| **前向传播** | 看看当前“画布”上的画，在模型眼中是什么（计算 $H_{adv}$）。 |
| **计算损失** | 测量这幅画和“靶子”的差距有多大（计算 $\mathcal{L}$）。 |
| **计算梯度** | 找到一个修改“画布”上每一笔（像素）的方向，能让差距变得最大（计算 $\nabla \mathcal{L}$）。 |
| **更新图像** | 朝着刚才找到方向的**反方向**，轻轻地修改一下整幅画（更新 $x_{adv}$）。 |
| **循环/终止** | 不断重复上述过程，直到画和靶子足够像（$\mathcal{L} < \tau$），然后停笔。 |

通过这个精密的、基于梯度的“雕琢”过程，一篇看似无害的图像，其内在的数字表示已经被彻底扭曲，成为了一个能够精准触发模型漏洞的强大武器。