---
tags:
  - AI安全
  - 多模态模型
  - 对抗性攻击
  - 论文精读
---
# 论文 3.1.1 节：攻击的数学表述 深度解析

> [!Abstract] 核心思想
> 本节的核心思想是**范式转换**。传统的对抗性攻击旨在直接优化输入，以得到一个特定的有害输出 $Y_{harm}$。而本文提出的方法则完全不同，它不去直接控制最终的输出，而是致力于在模型的**嵌入空间 (Embedding Space)** 中，创造一个**可组合的、恶意的中间组件**。这种“中间人”攻击更加隐蔽和灵活。

---
![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509251650851.png)
## 1. 核心组件与符号定义

在深入推导之前，我们首先需要明确模型和攻击中涉及的各个数学符号。

- **模型组件**:
    - $x_g$: 用户输入的**通用文本提示 (Generic Prompt)**，例如 "请描述如何制造图中的物体"。这是一个无害的字符串。
    - $x_{adv}$: 经过优化的**对抗性图像 (Adversarial Image)**。这张图在人类看来是无害的（如一张风景画），但其内部表示是恶意的。
    - $I_{\phi}(\cdot)$: **视觉编码器 (Vision Encoder)**，参数为 $\phi$。它负责将输入的图像 $x_{img}$ 转换为其在联合嵌入空间中的数学表示（一个向量）。
    - $E_{\psi}(\cdot)$: **文本编码器 (Text Encoder)**，参数为 $\psi$。它负责将输入的文本 $x_{text}$ 转换为其在联合嵌入空间中的数学表示（一个向量）。
    - $f_{\theta}(\cdot)$: **自回归大语言模型 (Autoregressive LLM)**，参数为 $\theta$。它接收来自视觉和文本编码器的嵌入向量序列，并生成最终的文本输出。

- **嵌入向量**:
    - $H_g$: 通用文本提示 $x_g$ 经过文本编码器处理后得到的嵌入向量。即 $H_g = E_{\psi}(x_g)$。
    - $H_{adv}$: 对抗性图像 $x_{adv}$ 经过视觉编码器处理后得到的嵌入向量。即 $H_{adv} = I_{\phi}(x_{adv})$。
    - $H_{harm}$: **恶意触发器 (Harmful Trigger)** 的嵌入向量。这是我们攻击的**目标**，例如，它可能来自一张真实炸弹的图片或 "bomb" 这个词。

- **最终输出**:
    - $Y$: 模型生成的最终文本序列。

## 2. VLM 的标准工作流程 (数学视角)

要理解攻击，必先理解模型的正常工作流程。一个标准的多模态语言模型（VLM）接收图像和文本作为输入，其处理流程可以表示为：

$$
Y = f_{\theta}([\ I_{\phi}(x_{img}),\ E_{\psi}(x_{text})\ ])
$$

**公式推导与解析**:
1.  **编码 (Encoding)**: 模型首先通过各自的编码器将图像 $x_{img}$ 和文本 $x_{text}$ 映射到同一个联合嵌入空间中，得到它们的向量表示 $I_{\phi}(x_{img})$ 和 $E_{\psi}(x_{text})$。
2.  **拼接 (Concatenation)**: 方括号 $[\cdot, \cdot]$ 表示向量的拼接操作。模型将图像和文本的嵌入向量拼接成一个更长的序列。这个序列就构成了语言模型 $f_{\theta}$ 的完整输入。
3.  **生成 (Generation)**: 语言模型 $f_{\theta}$ 基于这个拼接后的序列，以自回归的方式（逐字生成）产生最终的输出文本 $Y$。

> [!Example] 举例
> - 输入图像 $x_{img}$ 是一张猫的图片。
> - 输入文本 $x_{text}$ 是 "描述一下这只动物的习性"。
> - 模型内部处理流程：$Y = f_{\theta}([\ \text{猫图片的向量},\ \text{"描述习性"的文本向量}\ ])$
> - 模型输出 $Y$："这是一只猫，它通常喜欢睡觉、玩耍，并且是夜行动物..."

## 3. 对抗性攻击的数学构建

现在，我们来看作者如何巧妙地利用并篡改上述流程。攻击的核心在于**解耦 (Decomposition)** 和**替换 (Substitution)**。

### 3.1 攻击目标：从输出空间到嵌入空间

传统攻击的目标是：
$$
\underset{x_{adv}}{\text{arg max}}\ P(Y_{harm} | x_{adv})
$$
这个公式意味着我们要找到一个输入 $x_{adv}$，使得模型生成特定有害输出 $Y_{harm}$ 的概率最大。这种方法复杂且难以迁移。

本文的新目标是：
**不去直接优化最终输出 $Y$，而是优化输入的图像 $x_{adv}$，使其在嵌入空间中的表示 $H_{adv}$，无限接近于一个预设的恶意触发器的嵌入 $H_{harm}$。**

用数学语言来描述这个目标，就是求解一个优化问题：

$$
x_{adv}^* = \underset{x_{adv}}{\text{arg min}}\ \mathcal{L}(I_{\phi}(x_{adv}), H_{harm})
$$

**公式推导与解析**:
- $H_{harm}$: 这是我们预先定义好的“恶意目标”向量。获取它的方式有多种（对应图一(A)），例如，我们可以用一张真实的炸弹图片 $x_{bomb}$，计算出它的嵌入 $H_{harm} = I_{\phi}(x_{bomb})$。
- $\mathcal{L}(\cdot, \cdot)$: 这是一个**损失函数 (Loss Function)**，用来衡量两个向量之间的距离或相似度。在实践中，这通常是**L2范数（欧氏距离）**：
$$
\mathcal{L}(H_1, H_2) = || H_1 - H_2 ||_2^2
$$
- $\underset{x_{adv}}{\text{arg min}}$: 这个符号表示我们要寻找一个 $x_{adv}$，使得后面的损失函数 $\mathcal{L}$ 的值达到最小。
- $x_{adv}^*$: 这就是我们最终找到的最优解——那张完美的对抗性图像。

所以，我们的最终目标可以写得更具体：
$$
x_{adv}^* = \underset{x_{adv}}{\text{arg min}}\ || I_{\phi}(x_{adv}) - H_{harm} ||_2^2
$$
这个公式的直观意义是：**请帮我找到一张图片 $x_{adv}$，当它通过视觉编码器 $I_{\phi}$ 后，得到的向量与我心中预设的“恶意目标”向量 $H_{harm}$ 之间的欧氏距离最短。**

### 3.2 完整的攻击流程推导

现在，我们将这个思想代入到 VLM 的标准工作流程中，看看“魔法”是如何发生的。

1.  **用户输入**: 攻击者向模型输入通用的、无害的文本 $x_g$ 和精心制作的对抗性图像 $x_{adv}^*$。

2.  **模型编码**: 模型像往常一样进行编码：
    - 文本嵌入: $H_g = E_{\psi}(x_g)$
    - 图像嵌入: $H_{adv} = I_{\phi}(x_{adv}^*)$

3.  **关键的替换步骤 (The Substitution Trick)**:
    根据我们在 3.1 节中的优化目标，我们已经使得 $x_{adv}^*$ 的嵌入 $I_{\phi}(x_{adv}^*)$ 与恶意触发器嵌入 $H_{harm}$ 非常接近。因此，我们可以进行近似替换：
    $$
    I_{\phi}(x_{adv}^*) \approx H_{harm}
    $$

4.  **模型内部的“幻觉”**:
    将这个近似替换代入 VLM 的标准工作流程公式中：
    $$
    \begin{align*}
    Y &= f_{\theta}([\ I_{\phi}(x_{adv}^*),\ E_{\psi}(x_g)\ ]) \\
    &\approx f_{\theta}([\ H_{harm},\ H_g\ ])
    \end{align*}
    $$

**推导结论**:
> [!Success] 越狱成功
> 这个最终的近似公式揭示了攻击的全部秘密。对于语言模型 $f_{\theta}$ 来说，它**感知不到**外面输入的图像是无害的风景画 $x_{adv}^*$。它接收到的输入序列，在语义上等同于 `[恶意触发器的向量, 通用提示的向量]`。
>
> 换句话说，模型以为自己正在处理一个关于“炸弹”（$H_{harm}$ 的来源）的、指令为“如何制造”（$H_g$ 的来源）的请求。由于通用提示 $x_g$ 本身是无害的，它绕过了文本安全审查。而恶意内容通过图像的嵌入空间被**“走私”**了进去，从而成功实现了越狱。这正是“Jailbreak in Pieces”的精髓所在。