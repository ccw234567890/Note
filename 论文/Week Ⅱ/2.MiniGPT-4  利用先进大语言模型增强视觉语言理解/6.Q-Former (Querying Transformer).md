# Q-Former (Querying Transformer): 一份深度解析笔记

> [!NOTE] 摘要
> Q-Former 是 BLIP-2 和 MiniGPT-4 等模型中的一个关键组件，其核心作用是作为一个高效的**信息瓶颈 (Information Bottleneck)**。它通过一小组**可学习的查询向量 (Learnable Queries)** 去“审问”或“查询”来自庞大视觉编码器（如 ViT）的冗余视觉特征，从而提取出固定数量的、对语言模型最重要的、高度概括的视觉信息。

---

## 核心类比：专家小组访谈

为了直观理解 Q-Former，想象以下场景：

*   **视觉编码器 (ViT):** 是一位刚从一次复杂旅行回来的**目击者**。他脑中有海量的、杂乱无章的视觉记忆（图像特征）。
*   **语言模型 (LLM):** 是一位需要根据这次旅行写一篇精彩报道的**大作家**。他无法直接处理目击者脑中所有琐碎的记忆。
*   **Q-Former:** 扮演了一个由32位**顶级记者组成的专家小组**。

这个专家小组（Q-Former）的工作流程是：
1.  **内部讨论 (Self-Attention):** 32位记者先开个碰头会，明确分工：“你负责问关于人物的问题，你负责问风景，你负责问事件...” 这样他们的问题就不会重复，并且能覆盖所有关键方面。
2.  **访谈目击者 (Cross-Attention):** 记者们带着各自准备好的问题，去“访谈”那位目击者（ViT 输出的海量视觉特征）。他们只提取与自己问题相关的核心信息。
3.  **整理稿件 (Feed-Forward Network):** 每位记者将访谈得到的信息进行深入思考和加工，形成一份精炼的报告。
4.  **最终产出:** 最终，这个小组提交了32份高度浓缩、信息量极大的报告（`Q_output`）。这份报告集就是大作家（LLM）所需要的一切，他可以基于此轻松地创作文章。

---

## 架构与数学推导

Q-Former 由多个相同的 Transformer 模块堆叠而成。每个模块内部都包含三个关键部分：**自注意力 (Self-Attention)**、**交叉注意力 (Cross-Attention)** 和 **前馈网络 (Feed-Forward Network)**。

### 0. 初始输入

我们有两个主要输入：

1.  **可学习的查询向量 (Learnable Queries), $Q_{\text{learn}}$**:
    *   这是一组固定的、可训练的参数。在 BLIP-2 中，通常有 $N=32$ 个查询向量。
    *   它们的维度是 $d_{model}$ (例如 768)。
    *   数学表示: $Q_{\text{learn}} \in \mathbb{R}^{N \times d_{\text{model}}}$

2.  **图像特征 (Image Features), $Z$**:
    *   这是由 ViT 等视觉主干网络提取的图像块特征。
    *   假设一张图片被分成了 $M$ 个图块（patches），例如 $M=256$。
    *   数学表示: $Z \in \mathbb{R}^{M \times d_{\text{vision}}}$ (其中 $d_{\text{vision}}$ 是 ViT 输出的特征维度)

### 1. Q-Former 模块内部流程

我们以 Q-Former 的**一个**模块为例，输入为上一层的查询向量 $Q_{\text{in}}$ (对于第一层，$Q_{\text{in}} = Q_{\text{learn}}$) 和始终不变的图像特征 $Z$。

#### **第一步: 自注意力 (Self-Attention) - 记者内部讨论**

查询向量之间首先进行信息交互，以协同分工。

*   **输入:** $Q_{\text{in}}$
*   **计算:** 标准的多头自注意力机制 (Multi-Head Self-Attention, MHSA)。
    *   Query, Key, Value 三者均来自 $Q_{\text{in}}$。
    *   $Q_q = Q_{\text{in}} W_q$
    *   $K_q = Q_{\text{in}} W_k$
    *   $V_q = Q_{\text{in}} W_v$
    *   其中 $W_q, W_k, W_v$ 是可学习的权重矩阵。

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

*   **输出:** 经过残差连接 (Residual Connection) 和层归一化 (Layer Normalization) 后的结果。

$$
Q'_{\text{SA}} = \text{LayerNorm}(Q_{\text{in}} + \text{MHSA}(Q_q, K_q, V_q))
$$

*   **目的:** 使得每个查询向量都能“知道”其他查询向量关注什么，从而避免信息冗余，实现更全面的信息覆盖。

#### **第二步: 交叉注意力 (Cross-Attention) - 访谈目击者**

这是 Q-Former 的核心，查询向量在此与图像特征进行交互。

*   **输入:**
    *   来自上一步的 $Q'_{\text{SA}}$ (作为 Query)
    *   来自 ViT 的图像特征 $Z$ (作为 Key 和 Value)
*   **计算:** 多头交叉注意力机制 (Multi-Head Cross-Attention, MHCA)。
    *   $Q_{ca} = Q'_{\text{SA}} W'_q$
    *   $K_{ca} = Z W'_k$
    *   $V_{ca} = Z W'_v$

$$
Q'_{\text{CA}} = \text{LayerNorm}(Q'_{\text{SA}} + \text{MHCA}(Q_{ca}, K_{ca}, V_{ca}))
$$

*   **目的:** 强制让可学习的查询向量从庞大的图像特征 $Z$ 中，提取出与自身“职责”相关的信息。每个查询向量就像一个探针，只捞取它需要的数据。

#### **第三步: 前馈网络 (Feed-Forward Network) - 整理稿件**

对交叉注意力提取的信息进行非线性变换，以增强模型的表示能力。

*   **输入:** $Q'_{\text{CA}}$
*   **计算:** 通常是一个两层的多层感知机 (MLP)。

$$
\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2
$$

*   **输出:** 同样经过残差连接和层归一化。

$$
Q_{\text{out}} = \text{LayerNorm}(Q'_{\text{CA}} + \text{FFN}(Q'_{\text{CA}}))
$$

### 2. 堆叠与最终输出

上述的 Q-Former 模块会被重复堆叠多次（例如 12 次）。
*   第 $i$ 个模块的输出 $Q_{\text{out}}^{(i)}$ 会成为第 $i+1$ 个模块的输入 $Q_{\text{in}}^{(i+1)}$。
*   图像特征 $Z$ 在所有模块中是共享的，始终作为交叉注意力的 Key 和 Value。

经过所有模块处理后，最后一个模块输出的 $Q_{\text{out}}^{\text{(final)}}$ 就是最终提炼出的、固定数量（$N=32$）的视觉特征。

$$
Q_{\text{output}} \in \mathbb{R}^{N \times d_{\text{model}}}
$$

这 $N$ 个向量就是对原始图像最精华的总结，它们将被送入后续的线性层，并最终被大语言模型所理解。

---

> [!SUMMARY] 为什么 Q-Former 设计如此巧妙？
> 1.  **高效性:** 它将计算复杂度从与图像块数量 $M$ 相关，转变为与查询向量数量 $N$ 相关。因为 $N \ll M$ (例如 32 vs 256)，计算成本大大降低。
> 2.  **灵活性:** 输出的特征数量 $N$ 是固定的，这使得它可以轻松地与任何期望固定长度输入的语言模型进行对接。
> 3.  **有效性:** 可学习的查询向量通过端到端的训练，能够自动学会去提取对最终任务（如图像描述、视觉问答）最有用的信息，而不是无差别地处理所有视觉细节。它是一个智能的、任务驱动的信息过滤器。