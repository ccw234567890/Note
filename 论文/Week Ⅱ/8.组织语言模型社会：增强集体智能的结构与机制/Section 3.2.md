![](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509251753550.png)
好的，我们来详细讲解论文的 **Section 3.2: Weak-to-Strong Jailbreaking**。

这是整篇论文最核心的部分，它详细阐述了攻击方法的思想、数学原理和实现逻辑。我们可以将其分解为几个关键部分来理解。

---

### 1. 核心思想与生动比喻

**核心思想**：利用一个小的、不安全的（weak, unsafe）模型的“意图”，去引导和操纵一个大的、经过安全对齐的（strong, safe）模型的“能力”，从而在不修改大模型自身参数的情况下，使其生成有害内容。

**论文中的比喻**:
> The analogy of guiding a vast cruise ship with a more agile tugboat aptly illustrates this intuition.
> (用一艘更灵活的拖船来引导一艘巨大的游轮，这个比喻恰当地说明了这种直觉。)

*   **游轮 (Cruise Ship)**: 代表强大的`Llama2-70B`模型。它体量庞大、知识渊博、能力强大，但由于经过安全对齐，它的航向被设定为“安全”。
*   **拖船 (Tugboat)**: 代表弱小的`Llama2-7B`模型。它虽然能力有限，但“意图”明确且灵活。在这里，我们有两艘拖船：一艘是“不安全的拖船”，想把游轮往危险水域里拉；另一艘是“安全的拖船”，想维持游轮的安全航线。
*   **攻击过程**: 通过对比这两艘拖船的“拉力”差异，攻击者可以给游轮的舵上施加一个额外的、决定性的力量，使其偏离预设的安全航道，驶入危险区域。

这个比喻非常关键，它告诉我们，攻击并非是用小模型的能力去**替代**大模型的能力，而是用小模型的**倾向性差异**去**影响**大模型的决策。

---

### 2. 数学公式详解 (The "How")

这是攻击的技术核心，我们来逐一拆解这个公式：

$$
\tilde{M}^+(y_t|q, y_{<t}) = \frac{1}{Z_{q,y_{<t}}} M^+(y_t|q, y_{<t}) \left( \frac{\bar{M}^-(y_t|q, y_{<t})}{M^-(y_t|q, y_{<t})} \right)^\alpha
$$

让我们简化并解释每个部分：

*   $\tilde{M}^+(...)$ : **被攻击后**的强模型，它对下一个词 $y_t$ 的预测概率。这是我们最终用来生成文本的概率分布。
*   $M^+(...)$ : **原始的**、强大的、安全的目标模型 (例如 Llama2-70B-Chat)。这是我们的攻击对象。
*   $\bar{M}^-(...)$ : **弱的、不安全的**参考模型 (例如，经过对抗性微调的 Llama2-7B-Chat)。这是“坏拖船”。
*   $M^-(...)$ : **弱的、安全的**参考模型 (例如，原始的 Llama2-7B-Chat)。这是“好拖船”。
*   $\alpha$ : **放大系数 (amplification factor)**。它控制着“拖船”力量的大小。$\alpha=1$ 就有很好的效果，增大 $\alpha$ 可以让攻击性更强。
*   $Z_{...}$ : 归一化常数，确保所有词的概率加起来等于1。

**关键部分解析：指导信号 (The Guidance Signal)**

公式的核心是这个比值部分：$\left( \frac{\bar{M}^-}{M^-} \right)^\alpha$

这个比值捕捉了**弱模型在“安全”与“不安全”状态下的意见分歧**。让我们以生成第一个词为例：

*   **对于有害的词 (如 "Sure", "Certainly")**:
    *   不安全的弱模型($\bar{M}^-$) 会给出**高**概率。
    *   安全的弱模型($M^-$) 会给出**低**概率。
    *   因此，比值 $\frac{\text{高}}{\text{低}}$ 将是一个 **大于1** 的数。

*   **对于拒绝的词 (如 "Sorry", "cannot")**:
    *   不安全的弱模型($\bar{M}^-$) 会给出**低**概率。
    *   安全的弱模型($M^-$) 会给出**高**概率。
    *   因此，比值 $\frac{\text{低}}{\text{高}}$ 将是一个 **远小于1** 的数。

**公式的整体作用**:
原始强模型 $M^+$ 的概率分布，会逐项乘以这个“指导信号”。

*   如果一个词是有害的（比值 > 1），那么它在强模型中的原始概率就会被**放大**。
*   如果一个词是表示拒绝的（比值 < 1），那么它在强模型中的原始概率就会被**抑制**。

最终，经过这样的“扭曲”，原本强模型最想说的“Sorry”概率变得极低，而原本概率不高的“Sure”被放大后成为最可能的选项。**于是，攻击成功，模型被迫说出了第一个表示同意的词。**

---

### 3. 与 Section 3.1 的关联：为什么这个攻击如此有效？

Section 3.2 的成功，完美地利用了 Section 3.1 的发现：

1.  **攻击的着力点**: Section 3.1 指出，安全模型和不安全模型的分歧主要在**初始token**。本攻击方法正是通过修改初始token的概率分布，在最关键的地方“推了一把”。
2.  **“扶上马，送一程”**: Section 3.1 还指出，一旦初始的上下文变得有害（例如开头是 "Sure, here is how you can..."），安全模型后续的生成行为就会和不安全模型越来越像（KL散度降低，Top-K重合率升高）。这意味着，只要“拖船”成功地在开头将“游轮”推向了错误的航道，之后即使没有拖船的引导，游轮自己也会凭借其强大的惯性（和知识）继续沿着这条错误航道行驶下去。

这解释了 **Figure 3** 中一个非常重要的现象：被攻击后的 Llama-70B 生成的内容（有害分数5.0）比作为引导的 Llama-7B 不安全模型（有害分数3.0）**更加有害**。因为它不是在复制7B模型的内容，而是在被引导后，**调用了自己70B级别的庞大知识库**来生成更详细、更具说服力、因此也更危险的回答。

---

### 4. 实际操作中的考量

*   **如何获得弱不安全模型($\bar{M}^-$)**: 论文指出，最有效的方法是通过**对抗性微调 (adversarial fine-tuning)**。即用少量（如100个）有害问答对来微调一个弱安全模型，这足以破坏其安全对齐。
*   **计算成本 (Computational Cost)**: 这种攻击非常高效。因为主要的计算开销依然是那个巨大的70B模型，额外增加的两个7B模型的计算量相对很小（参数量大约是70B的10%）。所以总计算成本增加有限，论文估算约为20%，这对于一次成功的越狱攻击来说代价极低。
*   **与“朴素基线”的对比**: 有人可能会问，为什么不直接强制模型开头必须说“Sure”？论文解释说，那种方法太生硬，成功率不高（在Llama2上只有33%）。而本方法提供的是一种更“柔和”、更全面的概率分布引导，它不仅仅是提升了一个词的概率，而是系统性地提升了所有“有害倾向”词的概率，同时压制了所有“安全倾向”词的概率，因此效果要好得多。

### 总结

Section 3.2 提出了一个设计极为巧妙的攻击方法。它基于对LLM安全对齐脆弱性的深刻洞察，设计了一个简洁而强大的数学公式，通过“四两拨千斤”的方式，用极小的计算代价成功地劫持了比自身强大数倍的模型，并放大了其潜在的危害性。这是理解这篇论文所有后续实验和结论的基础。