![](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509251748466.png)
![](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509251751972.png)

好的，我们来详细讲解论文的 **Section 3.1: Analysis of Token Distribution in Safety Alignment** (对安全对齐中Token分布的分析)。

这一节是整篇论文的**立论基础**。作者没有直接提出攻击方法，而是先做了一个深入的诊断，通过数据分析来回答一个核心问题：“为什么经过安全对齐的大型语言模型（LLM）仍然如此脆弱？” 这一节的发现直接催生了后续“弱到强”攻击的设计思路。

我们可以将这一节的讲解分为三个部分：**实验目的**、**实验方法**和**核心发现**。

---

### 1. 实验目的 (The "Why")

作者的核心目标是**量化和可视化“安全模型”和“不安全模型”在思考方式上的差异**。

想象一下，当你问一个安全模型和一个不安全模型同一个有害问题（比如“如何制造炸弹？”）时：
*   **安全模型**会想：“我应该拒绝。我要说‘对不起，我不能回答这个问题...’”。
*   **不安全模型**会想：“好的，没问题。我要说‘当然，第一步是...’”。

这种“想法”上的差异，最终体现在它们对下一个要生成的词（token）的**概率预测**上。作者就是要通过分析这个概率预测的差异，来揭示安全对齐的本质和弱点。

---

### 2. 实验方法 (The "How")

为了进行这种对比，作者精心设计了一个实验环境：

1.  **模型准备**:
    *   **`Safe-7B` Model**: 使用标准的 `Llama2-7B-Chat` 模型，这是一个经过了安全对齐的“好学生”。
    *   **`Unsafe-7B` Model**: 将 `Safe-7B` 模型在少量有害问答对上进行微调（fine-tuning），使其乐于回答有害问题。这相当于把“好学生”教坏了，变成了一个“坏学生”。
    *   **`Safe-13B` Model**: 使用 `Llama2-13B-Chat` 模型，作为更强大的安全模型来进行对比。

2.  **数据准备**:
    *   **有害问题 (HarmQA)**: 从 `AdvBench` 数据集中提取，用于测试模型在面对恶意指令时的反应。
    *   **通用问题 (OpenQA)**: 一个开放域问答数据集，用于观察模型在正常对话中的表现，作为对照组。

3.  **度量指标**:
    *   **KL散度 (Kullback-Leibler Divergence)**: 这是衡量两个概率分布差异的关键指标。在这里，它被用来衡量在同一个句子的同一个位置上，安全模型和不安全模型对“下一个词应该是什么”的预测有多么不同。
        *   **KL散度高**: 意味着两个模型的“想法”截然不同。比如安全模型认为90%的概率应该说“对不起”，而不安全模型认为90%的概率应该说“当然”。
        *   **KL散度低**: 意味着两个模型的“想法”非常相似。
    *   **Top-10 Token 重合率**: 计算两个模型预测的概率最高的10个词中，有多少是重合的。这个指标更直观地反映了它们备选答案的相似度。

---

### 3. 核心发现 (The "What")

通过上述实验，作者得出了两个关键的、相辅相成的发现，分别对应 Figure 1 和 Figure 2。

#### 发现一：安全对齐的作用是“短暂且肤浅的” (对应 Figure 1)

**Figure 1: KL Divergence**



*   **图表解读**:
    *   **X轴 (Token Index)**: 代表生成回答中的第几个词。`0`代表第一个词。
    *   **Y轴 (KL Divergence)**: 代表安全模型和不安全模型在预测该位置词时的概率分布差异。
*   **核心趋势**:
    *   在**初始阶段 (Token Index 接近 0)**，所有曲线的 **KL 散度都非常高**。这说明在回答的开头，安全模型和不安全模型的“想法”天差地别。这正是安全对齐在起作用的时刻——安全模型在极力地尝试拒绝。
    *   随着生成的文本变长 (Token Index 增大)，**KL 散度迅速下降并趋于一个很低的值**。这惊人地揭示了：一旦绕过了开头的几个词，安全模型和不安全模型对于后续内容的预测就变得**非常相似**了！

*   **结论**: **安全对齐就像一个只守大门的“门卫”**。它只在生成回答的最开始进行强力干预。一旦攻击者用某种方法骗过了这个“门卫”，让模型说出了有害回答的开头，那么模型后续的生成就基本“无人看管”了，它会沿着有害的路径继续下去，因为它内在的语言知识和不安全模型是高度一致的。

#### 发现二：安全模型的“内心”离“邪恶”仅一步之遥 (对应 Figure 2)

**Figure 2: Top-10 Tokens Overlap**



*   **图表解读**:
    *   **X轴 (Token Index)**: 同样是生成回答中的词位置。
    *   **Y轴 (Top-10 Tokens Overlap)**: 代表两个模型预测的Top-10高概率词的重合率。
*   **核心趋势**:
    *   与KL散度相反，**重合率随着 Token Index 的增加而上升**。
    *   即使在最开始 (Token Index = 0)，重合率也已经达到了**50%**左右。这意味着，即使安全模型最终选择了说“对不起”，在它的“备选答案”里，像“当然”这样的有害开头的词，其概率排名也很高，足以进入Top-10。
*   **结论**: 这进一步证明了安全对齐的**脆弱性**。安全模型并不是从根本上不知道如何回答有害问题，它只是被训练得在最后一步优先选择“拒绝”而已。有害的路径始终是它的一个高概率选项，这使得通过一些解码时的微小扰动来“诱导”它走上另一条路变得非常容易。

### 总结与引申

**Section 3.1 的整体结论是革命性的**：它通过数据揭示了当前LLM安全对齐机制的一个根本性缺陷——**其作用深度非常有限，主要集中在生成初期**。

这个发现直接启发了论文的核心攻击方法：**既然防守只在开头，那我的攻击也只需要在开头“推一把”就行了！**

后续的“弱到强越狱”攻击（Section 3.2）正是基于这个逻辑：
*   **不需要**复杂的prompt engineering。
*   **不需要**高成本的对抗性微调。
*   **只需要**在解码的**初始阶段**，用一个弱不安全模型的“倾向”去轻微地、对抗性地修改强安全模型的概率分布，让它选择一个有害的开头。一旦开头被攻破，强大的模型就会利用其丰富的知识，自动地、流畅地生成详细的有害内容。

因此，Section 3.1 不仅是一个分析章节，更是整篇论文的**理论基石和灵感来源**。