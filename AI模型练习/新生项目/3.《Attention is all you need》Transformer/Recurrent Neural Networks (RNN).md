# 概念：循环神经网络 (Recurrent Neural Networks, RNN)

**标签**: #DeepLearning #NeuralNetwork #NLP #SequenceModeling #RNN

> [!quote] 核心论述
> **[[Recurrent Neural Networks (RNN)]]**: 一类适用于处理序列数据的神经网络，通过循环结构处理时序信息。

---

## 1. 核心思想：赋予网络“记忆”

在 RNN 出现之前，[[前馈特性|前馈神经网络]]（如 MLP, CNN）存在一个根本性的限制：它们是**无记忆的**。它们将每一个输入都视作独立的事件，无法处理输入之间具有时序关系的数据。

**一个简单的问题**:
> 对于句子 “The clouds are in the **sky**.”，如果一个前馈网络一次只看一个词，当它看到 "sky" 时，它已经忘记了前面的 "clouds"。它无法利用上下文信息来理解和预测。

**RNN 的解决方案**:
RNN 通过引入一个**“循环”（Recurrence）**结构来解决这个问题。这个循环使得网络在处理序列中的下一个元素时，能够**同时考虑当前的输入和它对过去所有元素的“记忆”**。

> **核心比喻：边读边记**
> 把 RNN 想象成一个人在**阅读一本书**。你不会孤立地理解每一个词，而是在大脑中维持一个对当前句子、段落内容的**动态总结（记忆）**。每读一个新词，你都会用这个新词的信息来**更新**你脑中的总结。
>
> RNN 中的**隐藏状态（Hidden State）**就扮演了这个“动态总结”或“记忆”的角色。

---

## 2. 工作机制：循环的奥秘

RNN 的核心是一个可以“循环”的单元。在处理序列时，这个单元在每一个时间步 `t` 都会执行相同的任务。

![RNN Unrolled](https://miro.medium.com/v2/resize:fit:1400/1*LGTp_N-a_5s-33_202-kGg.gif)
*上图：一个 RNN 单元可以按时间“展开”，看作是一个在所有时间步共享权重的深度网络。*

### A. 核心公式

在任意时间步 `t`，RNN 单元的计算分为两步：

1.  **更新隐藏状态（记忆）**:
    $$ h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h) $$
2.  **计算当前输出**:
    $$ y_t = W_{hy}h_t + b_y $$

- **$x_t$**: 在时间步 `t` 的**输入** (例如，句子中的第 `t` 个词的向量)。
- **$h_{t-1}$**: **前一个时间步**的隐藏状态 (来自过去的“记忆”)。
- **$h_t$**: **当前时间步**计算出的**新隐藏状态** (更新后的“记忆”)。这个 $h_t$ 将被传递给下一个时间步 `t+1`。
- **$y_t$**: 在时间步 `t` 的**输出** (例如，预测的下一个词，或当前词的标签)。
- **$W_{xh}, W_{hh}, W_{hy}, b_h, b_y$**: 这些是网络的权重和偏置。最关键的一点是，**这些参数在所有的时间步都是共享的**。网络只学习这一套参数，并用它来处理序列中的所有元素。
- **$f$**: 通常是一个非线性激活函数，如 `tanh` 或 `ReLU`。

### B. "展开" 理解
如图所示，一个循环的网络可以按时间序列展开，变成一个非常深的前馈网络。展开后的每一层都对应序列中的一个时间步，并且所有层都**共享同一套权重**。

---

## 3. RNN 的几种“玩法” (应用架构)

根据输入和输出序列的组合方式，RNN 可以形成多种灵活的架构来解决不同问题：

- **多对一 (Many-to-One)**:
    - **结构**: 输入一个序列，只在最后一个时间步产生一个输出。
    - **应用**: **[[文本分类]]/情感分析**。例如，输入整个句子“这部电影太棒了！”，输出一个标签“正面”。

- **一对多 (One-to-Many)**:
    - **结构**: 只在第一个时间步给一个输入，然后模型持续产生一个序列输出。
    - **应用**: **图像字幕生成**。例如，输入一张图片（经过CNN处理的特征向量），输出一句话“一只猫坐在垫子上”。

- **多对多 (Many-to-Many, 同步)**:
    - **结构**: 输入序列中的每一个元素，都对应产生一个输出。
    - **应用**: **序列标注**，如词性标注。例如，输入“猫 追 老鼠”，对应输出“名词 动词 名词”。

- **多对多 (Many-to-Many, 异步)**:
    - **结构**: 先完整读入一个输入序列，然后再开始生成一个输出序列。
    - **应用**: **[[论文/Week Ⅰ/3.《Attention is all you need》Transformer/Sequence Transduction|序列转导]]**，如**机器翻译**。例如，先完整输入英文句子 "Hello world"，再输出中文句子“你好 世界”。这通常由一个 **Encoder-Decoder** 架构实现。

---

## 4. RNN 的挑战：短期记忆问题

尽管 RNN 在理论上可以处理任意长度的序列，但在实践中，基础的 RNN 模型有一个致命的缺陷：**难以捕捉长期依赖关系**。

- **问题**: **[[梯度消失]]/[[梯度爆炸]]**。当网络按时间展开后，它变成了一个非常深的网络。在反向传播时，梯度需要从最后一个[[时间步]]一路传回第一个时间步。在这个漫长的过程中，梯度会因为连乘效应而变得极小（消失）或极大（爆炸）。
- **后果**: 梯度消失使得网络几乎无法将当前输出与很久之前的输入关联起来。它的“记忆”是**短期的**，可能只能记住序列中最近的几个元素。

为了解决这个问题，研究者们设计了更复杂的循环单元，如 **[[LSTM]]** 和 **[[GRU]]**，它们通过引入精巧的**[[Gating Function|门控机制]]**，来更好地控制信息的流动和记忆的存留，从而极大地增强了捕捉长期依赖的能力。