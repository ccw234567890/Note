下面给出一条以「最少改动」为原则、基于 **MindCV** 的 Vision Transformer (ViT) / Swin Transformer 的迁移学习方案。按步骤操作即可把原来的 MobileNetV2 替换成 Transformer 骨干，专注猫狗二分类。

────────────────────────
一、核心思路

1. 直接调用 MindCV 已实现并带有 ImageNet-22k / 1k 预训练权重的模型（`vit_base_patch16_224`, `swin_tiny_patch4_window7_224` …）。  
2. **复用预训练 Backbone**，仅重新训练最后的 `分类头 (head)`：  
   - 对小数据集更稳；  
   - 改动量小，训练速度快。  
3. 训练流程及余下代码（损失函数、学习率生成、Momentum 优化器、早停逻辑）保持不变。

────────────────────────
二、添加新建网函数（示例：ViT-Base）

在 `src/models.py` 末尾新增：

```python
from mindcv.models import vit_base_patch16_224                     # ① 导入 MindCV 模型

def define_net_vit(config, activation="None"):
    """Vision Transformer backbone + 新分类头"""
    # ② 获取预训练 ViT，去掉自带 head
    vit = vit_base_patch16_224(pretrained=True, num_classes=1000)  # 输出向量 dim=768
    # ③ 拆分：patch_embed + transformer encoder
    backbone_net = nn.SequentialCell(vit.patch_embed,
                                     vit.pos_drop,
                                     vit.blocks,
                                     vit.norm,
                                     nn.Flatten())                 # 输出 [B,768]

    # ④ 新建二分类 head
    head_net = nn.Dense(768, config.num_classes)

    net = nn.SequentialCell(backbone_net, head_net)
    if activation == "Softmax":
        net = nn.SequentialCell(net, nn.Softmax())

    return backbone_net, head_net, net
```

说明  
• ViT 输出 768 维 CLS token，经 `nn.Dense(768, num_classes)` 变成 logits。  
• `pretrained=True` 会自动下载官方 ImageNet-21k→1k 微调权重。  

────────────────────────
三、在 `train.py` 中切换 Backbone

```python
# --- 把原来 define_net() 改为 define_net_vit() ---
backbone_net, head_net, net = define_net_vit(config, activation="Softmax")
```

如果想做可选项，可在 `src/args.py` 增加 `--model vit`，然后在 `train.py`：

```python
if args_opt.model == "vit":
    define = define_net_vit
elif args_opt.model == "mobilenet":
    define = define_net
backbone_net, head_net, net = define(config, activation="Softmax")
```

────────────────────────
四、冻结 Transformer、加载权重

```python
load_ckpt(backbone_net, args_opt.pretrain_ckpt, trainable=False)
```

两种用法  
1. **自动下载**：不传 `--pretrain_ckpt`，让 `pretrained=True` 加载 ImageNet 权重；随后  
   ```python
   for p in backbone_net.get_parameters():
       p.requires_grad = False
   ```  
   (可放在 `define_net_vit` 里)  
2. **本地 ckpt**：若你已有 ViT 的 ckpt，沿用原逻辑加载后设置 `trainable=False`。

────────────────────────
五、保持其余训练逻辑不变

- `extract_features()` 能直接处理 ViT?  
  - 可以：本函数接受任意 backbone，只要 `construct()` 返回 `[B,feature_dim]`。  
- 损失函数、学习率表生成、Momentum 优化器 *均复用*。  
- 注意 ViT/SwIN 对输入尺寸要求 224×224，若你之前用的也是 224，这一步无需改；否则更新预处理或将 `config.image_size` 设为 224。

────────────────────────
六、推荐的超参调整

| 参数                | MobileNetV2 原值 | ViT/Swin 建议 |
|---------------------|-----------------|---------------|
| `lr_max`            | 0.01–0.1        | 1e-3 或更小   |
| `warmup_epochs`     | 1–5             | 5–10（更平滑）|
| `batch_size`        | 64              | 16–32（显存大）|
| `label_smooth`      | 0.1             | 0.1 可保持     |

可先用默认超参跑通，再逐一调优。

────────────────────────
七、替换为 Swin Transformer 的最小改动

将步骤二中的导入和维度改成：

```python
from mindcv.models import swin_tiny_patch4_window7_224

swin = swin_tiny_patch4_window7_224(pretrained=True, num_classes=1000)
backbone_net = nn.SequentialCell(swin.patch_embed,
                                 swin.pos_drop,
                                 swin.layers,
                                 swin.norm,
                                 nn.AvgPool1d(kernel_size=1),  # 取 CLS token
                                 nn.Flatten())                 # 输出 768
head_net = nn.Dense(768, config.num_classes)
```

────────────────────────
八、验证与调优

1. 先跑少量 epoch (`--epoch_size 3`)，确认能 forward 与 backward。  
2. 观察 `train total cost`，注意 Transformer 显存要求高；可适当调低 batch。  
3. 若发现过拟合：  
   - 解除部分 Backbone layer 的 `requires_grad`，做「部分解冻」；  
   - 增大数据增广：RandomErasing、RandAugment（MindCV `auto_augment.py`）；  
   - 使用 `Label Smoothing`、`Mixup`。

────────────────────────
总结

- 把 ViT/Swin 当作“新 Backbone”，接口保持“一致的输出向量 + 自建 Dense 头”即可。  
- 代码改动集中在 **新增 `define_net_vit/swin`、在 `train.py` 里调用**，其它流程原封不动。  
- 利用 MindCV 自带预训练权重 + 冻结策略，你可以在几分钟内完成 Transformer 微调并得到比 MobileNetV2 更强的分类效果。