# 函数：Softmax 函数

**标签**: #DeepLearning #ActivationFunction #Classification #Math

> [!info] 核心思想
> **Softmax 函数** 的主要作用是将一个包含任意实数（可正可负）的向量，转换为一个**概率分布向量**。
>
> 把它想象成一个**“分数转换器”**：
> 1.  **输入**: 神经网络对几个类别给出的原始、粗糙的“打分”（称为 **Logits**）。例如，对于一张图片，模型对 [猫, 狗, 鸟] 的打分可能是 `[2.0, 1.0, 0.1]`。
> 2.  **转换**: Softmax 函数接收这些分数。
> 3.  **输出**: 输出一个**概率**向量，其中：
>     - 每个元素都在 0 到 1 之间。
>     - 所有元素的总和恰好等于 1。
>     - 例如，`[2.0, 1.0, 0.1]` 可能会被转换为 `[0.7, 0.2, 0.1]`，意味着模型认为这张图有70%的概率是猫，20%是狗，10%是鸟。
>
> “Softmax” 这个名字里的 “Soft” 指的是，它是一种“软性”的最大值函数。它不像普通 `max` 函数那样只输出一个1（给最大值）和其余0，而是为每个元素都分配一个概率权重，但最大分数的元素会被不成比例地放大，获得最高的概率。

---

## 1. 数学公式

假设我们有一个 $K$ 维的实数向量 $\mathbf{z}$，它代表了模型对 $K$ 个不同类别的原始打分（Logits）。
$$ \mathbf{z} = (z_1, z_2, \dots, z_K) $$

Softmax 函数会输出一个同样为 $K$ 维的概率向量 $\mathbf{p}$。对于输出向量中的第 $i$ 个元素 $p_i$，其计算公式为：

$$
p_i = \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

- **$e^{z_i}$**: 分子部分。$e$ 是自然常数（约等于 2.718）。对输入的每一个分数 $z_i$ 取**指数**。
- **$\sum_{j=1}^{K} e^{z_j}$**: 分母部分。将**所有**输入分数取指数后的结果**相加**。
- **$p_i$**: 第 $i$ 个类别的最终概率。它是第 $i$ 个分数的指数值，除以所有分数的指数值之和。

---

## 2. 公式分步解析与计算示例

让我们用一个具体的例子，来一步步看懂这个公式是如何工作的。

**输入 Logits**: $\mathbf{z} = [3.2, 1.3, 0.2]$  (分别代表对类别A, B, C的打分)

#### **第1步：取指数 (Exponentiation)**
对输入的每一个分数，计算 $e$ 的幂。
- **目的**:
    1.  **非负化**: 无论输入是正数还是负数，`exp(z)` 的结果永远是正数，这符合概率的定义。
    2.  **放大差异**: 指数函数是一个增长极快的函数。它会不成比例地放大输入之间的差异。`3.2` 和 `1.3` 的差距，在取指数后会被拉得更大，使得最终的概率分布更“尖锐”。
- **计算**:
    - $e^{3.2} \approx 24.53$
    - $e^{1.3} \approx 3.67$
    - $e^{0.2} \approx 1.22$
- **中间结果**: `[24.53, 3.67, 1.22]`

#### **第2步：求和 (Summation)**
将上一步得到的所有指数值相加，作为归一化的分母。
- **目的**: 得到一个总的“权重池”，用于后续的归一化。
- **计算**:
    - $\sum_{j=1}^{3} e^{z_j} = 24.53 + 3.67 + 1.22 = 29.42$

#### **第3步：归一化 (Normalization)**
将第一步得到的每个指数值，分别除以第二步得到的总和。
- **目的**: 将所有值缩放到 `[0, 1]` 区间，并且保证它们的总和为 1，形成一个有效的概率分布。
- **计算**:
    - $p_A = \frac{24.53}{29.42} \approx 0.83$
    - $p_B = \frac{3.67}{29.42} \approx 0.12$
    - $p_C = \frac{1.22}{29.42} \approx 0.05$

**最终输出**:
$$ \text{Softmax}([3.2, 1.3, 0.2]) \approx [0.83, 0.12, 0.05] $$
*（注意：由于四舍五入，这里加起来可能是1.00，但精确计算下总和为1）*

**解读**: 模型以 83% 的置信度预测输入属于类别A，12% 属于类别B，5% 属于类别C。

---

## 3. 关键特性与应用

- **概率分布**: Softmax 的输出是一个标准的多项分布概率，非常适合用于多分类任务。
- **可微性**: Softmax 函数是处处可微的，这意味着梯度可以顺畅地通过它进行反向传播，这是模型能够通过[[梯度下降]]进行训练的前提。
- **与交叉熵损失 (Cross-Entropy Loss) 的关系**:
    - Softmax 通常作为多分类模型**输出层的激活函数**。
    - 它的输出概率会直接被送到**交叉熵损失函数**中，与真实标签（通常是一个 One-Hot 向量，如 `[1, 0, 0]`）进行比较，计算出损失值，从而指导模型学习。

- **数值稳定性问题 (Numerical Stability)**:
    - 当输入的 Logits 值非常大时，$e^{z_i}$ 可能会导致数值溢出（infinity）。
    - 实践中，通常会先对输入向量 $\mathbf{z}$ 的所有元素减去其中的最大值 $\max(\mathbf{z})$，再进行 Softmax 计算。这个操作在数学上等价，但可以保证指数函数的输入不会过大，从而保证计算的稳定性。
    $$ \frac{e^{z_i}}{\sum e^{z_j}} = \frac{e^{z_i - \max(\mathbf{z})}}{\sum e^{z_j - \max(\mathbf{z})}} $$

## 关联概念
- [[分类头 (Head Net)]]
- [[激活函数 (Activation Function)]]
- [[交叉熵损失 (Cross-Entropy Loss)]]
- [[Logits]]