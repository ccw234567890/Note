![image.png](https://cc-407-1376569927.cos.ap-guangzhou.myqcloud.com/cc-407-1376569927/images-obsidian/202509102010914.png)
# 图像解析：ResNet中的“瓶颈”残差块 (Bottleneck Block)

**标签**: #DeepLearning #ResNet #ResidualBlock #NetworkArchitecture #Optimization

> [!info] 核心思想
> 这张图对比了两种不同设计的[[残差块 (Residual Block)]]。
> - **左侧的基础块 (Basic Block)**：结构简单，用于 ResNet-18/34 等相对较浅的网络。
> - **右侧的瓶颈块 (Bottleneck Block)**：是一种**更深、但计算效率更高**的结构，专为 ResNet-50/101/152 这样非常深的网络设计。
>
> “瓶颈”设计的核心目的是：在不牺牲（甚至增强）模块表达能力的前提下，**显著减少参数量和计算成本**，从而让构建百层深的网络成为可能。

---

## 1. 左侧 - 基础残差块 (Basic Block)

- **适用网络**: ResNet-18, ResNet-34。
- **结构**:
    - 一个 `3x3, 64` 的卷积层。
    - 一个 `relu` 激活。
    - 一个 `3x3, 64` 的卷积层。
- **数据流**:
    - 输入是一个 `64-d` (64通道) 的特征图。
    - 经过两个主要的、拥有64个卷积核的 `3x3` 卷积层。
    - 输出的残差 $F(x)$ 也是 `64-d`。
- **特点**:
    - 结构直观、简单。
    - 在通道数不高时，计算效率尚可。
- **问题**:
    - 当网络的通道数变得很深时（例如256或512），连续两个 `3x3` 的卷积层会带来巨大的计算负担。例如，一个 `3x3` 卷积，如果输入和输出都是256通道，其参数量约为 `3*3*256*256 = 589,824`，成本非常高。

---

## 2. 右侧 - “瓶颈”残差块 (Bottleneck Block)

为了解决基础块在深度网络中的效率问题，作者设计了“瓶颈”结构。

- **适用网络**: ResNet-50, ResNet-101, ResNet-152。
- **结构 (三层结构)**:
    1.  一个 **`1x1, 64`** 的卷积层。
    2.  一个 **`3x3, 64`** 的卷积层。
    3.  一个 **`1x1, 256`** 的卷积层。
- **数据流与“瓶颈”效应**:
    - **输入**: 一个 `256-d` (256通道) 的高维特征图。
    1.  **降维 (Squeeze / 瓶颈)**: 第一个 `1x1` 卷积层将通道数从 **256** 压缩到 **64**。这就像一个瓶颈，大大减少了需要处理的数据深度。
    2.  **核心卷积**: 此时，计算成本最高的 `3x3` 卷积是在 **64** 通道的低维特征图上进行的，而不是在 256 通道上。**这是效率提升的关键**。
    3.  **升维 (Expand / 恢复)**: 最后一个 `1x1` 卷积层再将通道数从 **64** 恢复到 **256**，以确保最终输出的残差 $F(x)$ 的维度与初始输入 $x$ 的维度一致，从而可以进行相加操作。

---

## 3. 为什么需要“瓶颈”设计？—— 效率对比

让我们通过计算参数量来直观感受一下“瓶颈”设计的巨大优势。假设我们要处理一个 `256` 通道的特征图，并且希望残差块的输出也是 `256` 通道。

- **方案A：使用两个基础块的 `3x3` 卷积**
    - `Conv1`: `3x3`，输入256通道，输出256通道 -> 参数量: `3*3*256*256 = 589,824`
    - `Conv2`: `3x3`，输入256通道，输出256通道 -> 参数量: `3*3*256*256 = 589,824`
    - **总参数量**: ~**118万**

- **方案B：使用“瓶颈”块**
    - `Conv1 (1x1)`: 输入256通道，输出64通道 -> 参数量: `1*1*256*64 = 16,384`
    - `Conv2 (3x3)`: 输入64通道，输出64通道 -> 参数量: `3*3*64*64 = 36,864`
    - `Conv3 (1x1)`: 输入64通道，输出256通道 -> 参数量: `1*1*64*256 = 16,384`
    - **总参数量**: ~**6.9万**

**结论**: “瓶颈”块用**大约 1/17 的参数量**，实现了类似的功能。这种巨大的效率提升，使得构建和训练超过50层的超深网络在计算上成为可行。

---

## 4. 总结

- **图的含义**: 这张图展示了 ResNet 为了适应不同深度的网络而设计的两种核心“构建块”。
- **基础块 (左)**:
    - 简单，由两个 `3x3` 卷积构成。
    - 适用于 ResNet-34 及以下。
- **瓶颈块 (右)**:
    - 更深（3层卷积），但效率更高。
    - 采用 **`1x1` (降维) -> `3x3` (核心处理) -> `1x1` (升维)** 的策略。
    - 适用于 ResNet-50 及以上。
- **核心思想**: 通过“瓶颈”设计，可以用更少的计算资源构建出更深、表达能力可能更强的网络层。这是深度学习工程实践中，平衡模型性能与计算效率的经典范例。

## 关联概念
- [[残差块 (Residual Block)]]
- [[1x1 卷积]]
- [[残差网络 (ResNet)]]
- [[快捷连接 (Shortcut Connection)]]