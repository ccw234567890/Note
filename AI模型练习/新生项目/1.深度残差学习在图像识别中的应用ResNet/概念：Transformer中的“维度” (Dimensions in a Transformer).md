# 概念：Transformer中的“维度” (Dimensions in a Transformer)

**标签**: #DeepLearning #Transformer #NLP #CoreConcept

> [!info] 核心思想
> 在 Transformer 模型中，“维度”描述的是**将离散的文字符号（Token）转换为丰富的、可计算的向量表示**的各个方面。它定义了模型信息处理“高速公路”的宽度、并行计算的数量以及内部处理的复杂度。
>
> 想象一下为字典里的每一个词（如“猫”）创建一份**“详细档案”**：
> - **Token ID**: 只是一个编号（例如 `3052`），信息量很少。
> - **词向量 (Word Vector)**: 这份“详细档案”本身，是一个长长的向量。
>
> **Transformer 中的核心维度 `d_model` 就定义了这份档案的详细程度**。例如，`d_model=512` 意味着这份档案有 **512 个字段**，用来从不同角度描述这个词：
> - 字段1: “是名词吗？” (1.0)
> - 字段2: “是动物吗？” (0.98)
> - 字段3: “与‘坐’这个动作相关吗？” (0.23)
> - ...
> - 字段512: ...
>
> 整个 Transformer 模型就是在处理和更新这些包含丰富信息的“档案向量”。

---

## 1. 核心维度：`d_model` (模型维度)

`d_model` 是理解 Transformer 中所有其他维度的基础，也被称为**嵌入维度 (Embedding Dimension)**。

- **定义**: `d_model` 是在整个 Transformer 模型中，用来**表示单个词符（Token）的向量长度**。
- **作用**:
    1.  **信息承载量**: `d_model` 越大，每个词符的向量能携带的信息就越丰富和细致。
    2.  **“主干道”宽度**: 所有子层（如多头注意力、前馈网络）的输入和输出向量维度都必须是 `d_model`。它像一条贯穿整个模型的高速公路，所有数据流都在这条规定宽度的公路上行驶。
- **数据流**: 对于一个长度为 `seq_len` 的句子，输入到 Transformer 编码器层的数据，其形状是一个矩阵：`(seq_len, d_model)`。
- **典型值**:
    - 原始 Transformer 论文: `d_model = 512`
    - BERT-base: `d_model = 768`
    - BERT-large: `d_model = 1024`

---

## 2. 多头注意力中的维度

自注意力机制是 Transformer 的核心，其维度设计体现了“分而治之”的思想。

### `num_heads` (注意力头的数量)
- **定义**: 模型并行运行的“注意力机制”的数量。
- **作用**: 与其让一个大的注意力机制去理解句子中所有复杂的关系，不如把它拆分成多个“头”。每个头可以独立地、从不同角度去学习词与词之间的关系。
    - **一个比喻**: 就像一个专家委员会。与其让一个全才去分析所有问题，不如请 8 位（`num_heads=8`）专家，让他们分别关注语法关系、语义关联、指代关系等不同方面。
- **典型值**: 8, 12, 16

### `d_k`, `d_q`, `d_v` (键、查询、值的维度)
- **定义**: 在**每一个注意力头内部**，输入的 `d_model` 维向量会被投影（通过线性层）成三个更小的向量：**查询 (Query)**, **键 (Key)**, 和 **值 (Value)**。`d_q`, `d_k`, `d_v` 就是这些向量的维度。
- **关键关系**: 这些维度与 `d_model` 和 `num_heads` 密切相关，它们必须满足以下等式（通常 `d_q = d_k = d_v`）：
  $$ d_{model} = \text{num\_heads} \times d_k $$
- **示例**: 在 `d_model = 512`, `num_heads = 8` 的 Transformer Base 模型中：
  - 每个头的键（Key）和查询（Query）向量的维度是 $d_k = \frac{512}{8} = 64$。
  - 这意味着，8个注意力头，每个都在一个更低维（64维）的空间中并行计算注意力得分，这比在512维空间中直接计算更高效。
- **作用**:
    - **`Query`**: 代表当前词符，去“查询”句子中其他词符。
    - **`Key`**: 代表句子中的其他词符，用来响应 `Query` 的查询。
    - **`Value`**: 代表句子中其他词符的实际内容。一旦 `Query` 和 `Key` 计算出注意力权重，这个权重就会被用来加权求和所有的 `Value` 向量。

---

## 3. 前馈网络中的维度

### `d_ff` (前馈网络内部维度)
- **定义**: 在每个 Transformer 块的“前馈神经网络”（Feed-Forward Network, FFN）子层中，隐藏层的维度。
- **结构**: FFN 通常由两个线性层组成。
    1.  第一个线性层将输入向量从 `d_model` **扩展**到 `d_ff`。
    2.  第二个线性层再将其从 `d_ff` **压缩**回 `d_model`。
- **作用**:
    - 增加模型的**非线性能力**和**容量**。
    - 在注意力机制聚合了全局信息之后，FFN 对每个位置的表示进行一次独立的、更复杂的加工和提炼。
- **典型值**: 通常是 `d_model` 的 4 倍。
    - `d_model = 512` -> `d_ff = 2048`
    - `d_model = 768` -> `d_ff = 3072`

---

## 4. 其他重要维度

- **`sequence_length` (序列长度)**: 模型一次能处理的词符（Token）的最大数量。例如 512。
- **`vocab_size` (词汇表大小)**: 模型认识的所有唯一词符的总数。例如 30522 (BERT)。

---

## 总结图表

| 维度名称 | 常用符号 | 描述 | 典型值 |
| :--- | :--- | :--- | :--- |
| **模型维度** | `d_model` | 贯穿模型的**核心向量长度**，表示每个词符的丰富度。 | 512, 768, 1024 |
| **前馈网络维度** | `d_ff` | FFN子层中隐藏层的神经元数量。 | 2048, 3072 |
| **注意力头数量** | `num_heads` | 并行计算注意力的“专家”数量。 | 8, 12, 16 |
| **键/值维度** | `d_k`, `d_v` | **单个注意力头内部**的向量长度。 | 64 |
| **序列长度** | `seq_len` | 模型能处理的句子的最大长度。 | 512, 2048 |
| **词汇表大小** | `vocab_size` | 模型输入/输出层的Token总数。 | 30k, 50k |

## 关联概念
- [[Pytorch/Transformer]]
- [[自注意力机制 (Self-Attention)]]
- [[表示 (Representation)]]
- [[词嵌入 (Word Embedding)]]