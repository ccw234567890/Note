# 解析：“标签平滑交叉熵”的向量模拟

**标签**: #DeepLearning #LossFunction #Regularization #LabelSmoothing #CrossEntropy

> [!quote] 您的原始分析
> `CrossEntropyWithLabelSmooth` 是项目在 `src/models.py` 自定义的一种交叉熵损失，它在普通 one-hot 标签的基础上做了 **Label Smoothing（标签平滑）**。
> ...
> Label Smoothing 把“绝对 1”削成 `1−ε`，把“绝对 0”抬成 `ε/(K−1)`...约束模型不要把全部概率压在单一类别上...
> ... (完整分析)

---

## 1. 我们的目标
我们将模拟对于**一个样本**的分类过程，对比在**标准交叉熵**和**标签平滑交叉熵**两种情况下，最终计算出的损失值有何不同。

## 2. 设定场景

- **类别总数 (K)**: 4 (例如：猫, 狗, 鸟, 鱼)
- **真实标签 (Integer Label)**: `2` (代表“鸟”，索引从0开始)
- **平滑系数 (ε)**: `0.1`
- **模型的输出 (Logits 向量)**:
    - 我们的模型经过训练，对这张“鸟”的图片**非常自信**，输出了一个在第2个索引位上值极高的 Logits 向量。
    $$ \mathbf{z} = \begin{bmatrix} -1.5 & 0.5 & 5.0 & -0.5 \end{bmatrix} $$

---

## 3. 模拟开始：两条计算路径

### 路径 A: 标准交叉熵 (Standard Cross-Entropy) 的计算过程

#### **A1. Logits -> 概率向量 (Softmax)**
首先，`SoftmaxCrossEntropyWithLogits` 内部会对 Logits 向量 $\mathbf{z}$ 应用 [[函数：Softmax 函数|Softmax 函数]]，得到预测概率向量 $\mathbf{p}$。
$$ \mathbf{p} = \text{Softmax}(\mathbf{z}) = \text{Softmax}(\begin{bmatrix} -1.5 & 0.5 & 5.0 & -0.5 \end{bmatrix}) $$
$$ \mathbf{p} \approx \begin{bmatrix} 0.0015 & 0.0109 & \mathbf{0.9836} & 0.0040 \end{bmatrix} $$
*观察：模型几乎把 98.4% 的概率都压在了“鸟”这个类别上，表现得“过于自信”。*

#### **A2. 整数标签 -> One-Hot 目标向量**
标准的交叉熵需要一个 One-Hot 编码的真实标签向量 $\mathbf{y}$。
$$ \mathbf{y} = \begin{bmatrix} 0 & 0 & 1 & 0 \end{bmatrix} $$
*观察：这是一个“绝对”的标签，非黑即白，没有任何不确定性。*

#### **A3. 计算交叉熵损失**
损失的计算公式为 $L = -\sum_{i=1}^{K} y_i \log(p_i)$。
$$ L = - (0 \cdot \log(0.0015) + 0 \cdot \log(0.0109) + \mathbf{1 \cdot \log(0.9836)} + 0 \cdot \log(0.0040)) $$
由于 `0` 乘以任何数都为 0，公式简化为：
$$ L = - \log(0.9836) \approx -(-0.0165) \approx \mathbf{0.0165} $$
*结果：损失值非常非常小。模型因为自己的预测和“绝对正确”的标签高度一致，所以受到的“惩罚”微乎其微。这会鼓励它下一次变得更加自信。*

---
### 路径 B: 标签平滑交叉熵 (Label Smoothing) 的计算过程

#### **B1. Logits -> 概率向量 (Softmax)**
这一步与路径 A 完全相同，模型的预测概率 $\mathbf{p}$ 依然是：
$$ \mathbf{p} \approx \begin{bmatrix} 0.0015 & 0.0109 & 0.9836 & 0.0040 \end{bmatrix} $$

#### **B2. 整数标签 -> “平滑后”的目标向量**
这是核心区别。我们不再使用“绝对”的 One-Hot 向量，而是根据 `__init__` 中的定义创建一个平滑的标签向量 $\mathbf{y}'$。
- **正类的值 (on_value)**: $1 - \varepsilon = 1 - 0.1 = \mathbf{0.9}$
- **负类的值 (off_value)**: $\frac{\varepsilon}{K-1} = \frac{0.1}{4-1} \approx \mathbf{0.0333}$

因此，平滑后的目标向量为：
$$ \mathbf{y}' = \begin{bmatrix} 0.0333 & 0.0333 & 0.9 & 0.0333 \end{bmatrix} $$
*观察：这个标签引入了“不确定性”。它表示“我有90%的把握这是‘鸟’，但也有约3.3%的可能它是其他几类”。*

#### **B3. 计算交叉熵损失**
损失的计算公式 $L' = -\sum_{i=1}^{K} y'_i \log(p_i)$ 不再能简化，每一项都有贡献。
$$ L' = - [ (0.0333 \cdot \log(0.0015)) + (0.0333 \cdot \log(0.0109)) + (0.9 \cdot \log(0.9836)) + (0.0333 \cdot \log(0.0040)) ] $$
代入对数值（$\log(p)$ 对于 $p<1$ 是负数）：
$$ L' \approx - [ (0.0333 \cdot -6.5) + (0.0333 \cdot -4.52) + (0.9 \cdot -0.0165) + (0.0333 \cdot -5.52) ] $$
$$ L' \approx - [ -0.216 - 0.151 - 0.015 - 0.184 ] $$
$$ L' \approx -(-0.566) \approx \mathbf{0.566} $$

---
## 4. 结果对比与分析

| 损失类型 | 目标向量 `y` | 预测向量 `p` | 最终损失 L |
| :--- | :--- | :--- | :--- |
| **标准交叉熵** | `[0, 0, 1, 0]` | `[..., 0.98, ...]` | **~0.0165 (极小)** |
| **标签平滑** | `[0.03, 0.03, 0.9, 0.03]` | `[..., 0.98, ...]` | **~0.566 (显著)** |

**结论**:
面对同一个“过于自信”的预测 `p`，标签平滑计算出的**损失值远大于**标准交叉熵。

- **为什么会这样？**
    - 标准交叉熵看到 `p_2 ≈ 1` 和 `y_2 = 1`，认为模型做得“近乎完美”，所以几乎不给惩罚。
    - 标签平滑的目标 `y'` 认为“完美答案是90%”，而模型给出了“98.4%”的自信，这是一种“过度自信”。更重要的是，平滑标签认为其他类别也有 `3.33%` 的可能性，但模型的预测 `p_0, p_1, p_3` 都接近于0，**严重不符**。
    - 这个**“不符合”**体现在 `y'_i \log(p_i)` 的计算中：一个小的正数 `y'_i` 乘以一个大的负数 `log(p_i)`，产生了一个不可忽略的负值，累加起来就形成了显著的损失（惩罚）。

**效果**:
这个显著的惩罚会通过[[讲解一下什么是梯度（gt）|梯度]]反向传播，**迫使模型下一次不要再输出如此“尖锐”的概率分布**。它会鼓励模型稍微“谦虚”一点，把分配给正确类别的概率从 98.4% 降一点，分一些给其他类别，从而让模型的决策边界更平滑，**提高泛化能力**。