# 🧠 神经网络的层级连接：从向量到矩阵的跃迁

> [!note] 核心思想
> 通过从 **向量** 到 **矩阵** 的转变，我们让一层网络具备了同时 **提取** 和 **输出** 多种不同信息的能力，这是构建复杂深度神经网络模型至关重要的一步。

---

## 🔗 核心连接方式：全连接层 (Fully Connected Layer)

`#NeuralNetwork` `#DeepLearning` `#FullyConnected`

> [!info] 什么是全连接层？
> 全连接层（也称稠密层 `Dense Layer`）是神经网络中最基础的连接方式。其核心特点是：前一层网络中的 **每一个神经元** 都与后一层网络中的 **每一个神经元** 相连接。这种“完全连接”的结构使得信息能够被充分地交叉、组合与转换。

### 🔢 从向量到矩阵的数学解析

> [!tip] 颜色建议
> 您可以在 Obsidian 中使用 CSS Snippets 为以下元素自定义颜色：
> - **输入向量**: `#90CAF9` (淡蓝色)
> - **权重矩阵**: `#FFCC80` (淡橙色)
> - **输出向量**: `#A5D6A7` (淡绿色)

信息在两层之间的流动遵循以下步骤：

1.  **输入向量 (Input Vector)** `(维度: n x 1)`
    - 来自上一层的输出，每个元素 $x_i$ 代表一个特征。
    $$
    \vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
    $$

2.  **权重矩阵 (Weight Matrix)** `(维度: m x n)`
    - 连接的核心！矩阵中的每个元素 $w_{ij}$ 代表了输入层第 $j$ 个神经元到隐藏层第 $i$ 个神经元连接的 **强度** 或 **重要性**。
    > [!quote] 矩阵的每一行都是一个“特征提取器”。
    $$
    W = \begin{bmatrix} \color{#FF8A65}w_{11} & \color{#FF8A65}w_{12} & \cdots & \color{#FF8A65}w_{1n} \\ \color{#4DB6AC}w_{21} & \color{#4DB6AC}w_{22} & \cdots & \color{#4DB6AC}w_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ w_{m1} & w_{m2} & \cdots & w_{mn} \end{bmatrix}
    $$

3.  **线性变换 (Linear Transformation)**
    - 将输入向量 $\vec{x}$ 与权重矩阵 $W$ 进行矩阵乘法，本质上是对输入信息的一次 **“加权求和”**。
    $$
    \vec{z} = W \cdot \vec{x}
    $$

4.  **添加偏置项 (Bias)**
    - 为了增加模型的灵活性和拟合能力，为每个隐藏层神经元添加一个偏置项 $\vec{b}$。
    $$
    \vec{z}' = W \cdot \vec{x} + \vec{b}
    $$

5.  **激活函数 (Activation Function)**
    - 对线性变换的结果进行非线性处理（例如 `ReLU`, `Sigmoid`），使网络能够学习和拟合更加复杂的模式。这是产生“智能”的关键一步。
    $$
    \vec{a} = f(\vec{z}')
    $$
    - 这个输出向量 $\vec{a}$ 将作为下一层的输入，重复上述过程。

### 🎨 权重矩阵如何实现“提取多种信息”？

> [!example] 一个形象的比喻：做菜
> - **输入向量 $\vec{x}$**: 各种食材 (西红柿, 鸡蛋, 盐, 糖...)
> - **权重矩阵 $W$**: 一本菜谱
>
> 菜谱的 **每一行** 就是一道菜的做法（一个特征提取器）：
> - **`权重行 1` (炒鸡蛋风味)**: `2份`鸡蛋 + `1份`西红柿 + `少许`盐  =>  **`输出神经元 1`** (提取出咸鲜味)
> - **`权重行 2` (凉拌糖番茄)**: `1份`西红柿 + `3份`糖 => **`输出神经元 2`** (提取出酸甜味)
>
> 通过这本菜谱 (权重矩阵)，我们就能从 **相同** 的输入食材中，提取并组合出 **多种不同** 的风味信息 (更高层次的特征)。

---

## 🌐 其他重要的层连接方式

> [!abstract] 不同的连接方式适用于不同的任务场景，好比为不同的工作选择最合适的工具。

| 连接类型 | 连接方式 | 主要应用 | 优势 |
| :--- | :--- | :--- | :--- |
| **全连接层**<br>`Fully Connected` | 全局连接，一对多 | 表格数据、分类任务 | 能够学习特征之间的 **全局关系**。 |
| **卷积层**<br>`Convolutional` | 局部连接、权重共享 | 图像识别、计算机视觉 | **参数少**、对平移缩放不敏感，高效提取 **局部特征**（如边缘、纹理）。 |
| **循环层**<br>`Recurrent` | 带有“记忆”的循环连接 | 自然语言处理、时间序列 | 能够捕捉 **序列数据** 中的时序依赖关系。 |

---

## ✨ 总结

> [!done] 核心要点
> - **核心机制**: 层与层之间通过 **权重矩阵** 进行连接和信息变换。
> - **向量到矩阵**: 将输入 **向量** 与 **权重矩阵** 相乘，是实现特征提取和信息转换的关键。
> - **特征提取器**: 权重矩阵的 **每一行** 都是一个独立的特征检测器，负责从输入中组合并提取一种特定的高级特征。
> - **多样化连接**: 不同的网络架构（如 `CNN`, `RNN`）使用不同的连接策略来适应特定类型的数据，从而最大化模型效能。

---

----

----
# 神经网络核心概念：从前向传播到反向传播

> [!abstract] 核心思想
> 本笔记旨在梳理神经网络从**前向计算**（Forward Pass）到**梯度反向传播**（Backpropagation）的核心逻辑。我们将从单个神经元层开始，逐步深入到如何通过链式法则更新网络中的权重。

---

## Ⅰ. 前向传播 (Forward Propagation)

> [!note] 目标
> 从输入数据 $\mathbf{x}$ 开始，逐层计算，直到得到网络最终的预测输出 $\mathbf{a}^L$ 和损失值 $E$。

### 1. 输入 (Input)

> [!example] 特征向量
> 输入是一个特征向量 $\mathbf{x}$，代表一个数据样本。
> $$\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$$

### 2. 线性加权和 (Linear Combination)

> [!danger] 关键变化：从向量到矩阵
> 当我们从单个神经元扩展到一个神经元**层**时，权重从一个向量 $\mathbf{w}$ 升级为一个**矩阵** $\mathbf{W}$。

- **权重矩阵 $\mathbf{W}$**: 每一**行**代表一个输出神经元的所有输入权重。
- **计算**: 整个层的线性计算可以通过一次高效的**矩阵乘法**完成。
- **公式**:
    $$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$$
    ```latex
    % 矩阵形式
    \begin{pmatrix} z_1 \\ z_2 \\ \vdots \\ z_m \end{pmatrix} = 
    \begin{pmatrix}
    W_{11} & \cdots & W_{1n} \\
    \vdots & \ddots & \vdots \\
    W_{m1} & \cdots & W_{mn}
    \end{pmatrix}
    \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} +
    \begin{pmatrix} b_1 \\ \vdots \\ b_m \end{pmatrix}
    ```

### 3. 激活函数 (Activation)

> [!tip] 按元素应用 (Element-wise)
> 激活函数 $\sigma$ 被独立地应用于线性输出向量 $\mathbf{z}$ 的每一个元素上，得到最终的激活输出向量 $\mathbf{a}$。

$$\mathbf{a} = \sigma(\mathbf{z}) = \begin{pmatrix} \sigma(z_1) \\ \sigma(z_2) \\ \vdots \\ \sigma(z_m) \end{pmatrix}$$

### 4. 计算损失 (Loss)

> [!warning] 关键变化：误差求和
> 总误差 $E$ 是所有输出神经元误差 $E_k$ 的总和或平均，将多个输出的误差汇聚成一个**单一的标量值**，用于指导模型优化。

$$E = \sum_k E_k$$
例如，均方误差 (MSE) 为:
$$E = \sum_{k=1}^{m} (a_k - y_k)^2$$

---

## Ⅱ. 反向传播 (Backpropagation)

> [!todo] 目标
> 计算损失函数 $E$ 对网络中所有参数（权重 $\mathbf{W}$ 和偏置 $\mathbf{b}$）的**梯度**（即“责任”），以便更新这些参数。核心是**链式法则**的应用。

### 第 1 步: 计算输出层梯度 (初始误差)

> [!question] 我们从哪里开始？
> 从损失 $E$ 对网络**最终线性输出** $\mathbf{z}^L$ 的梯度开始。
> > [!cite] 结合Softmax与交叉熵损失
> > 这是一个非常优雅的简化结果，直接给出了预测与真实值之间的偏差。
> > $$\delta^L = \frac{\partial E}{\partial \mathbf{z}^L} = \mathbf{a}^L - \mathbf{y}$$
> > - $\mathbf{a}^L$: 网络的最终预测概率。
> > - $\mathbf{y}$: 真实的标签 (one-hot)。

### 第 2 步: 计算输出层权重梯度

> [!bug] 如何计算 $\mathbf{W}^L$ 的责任？
> 应用链式法则：`本层误差信号` $\times$ `本层输入`
> $$\frac{\partial E}{\partial \mathbf{W}^L} = \frac{\partial E}{\partial \mathbf{z}^L} \cdot \frac{\partial \mathbf{z}^L}{\partial \mathbf{W}^L} = \delta^L (\mathbf{a}^{L-1})^T$$
> - 这一步验证了通过**局部导数相乘**来更新当前层权重的有效性。

### 第 3 步: 将误差反向传播到隐藏层

> [!help] 如何将误差 $\delta^L$ 传给前一层？
> 将误差信号通过权重矩阵 $\mathbf{W}^L$ 的“反向通道”传播回去。
> $$\frac{\partial E}{\partial \mathbf{a}^{L-1}} = \frac{\partial E}{\partial \mathbf{z}^L} \cdot \frac{\partial \mathbf{z}^L}{\partial \mathbf{a}^{L-1}} = (\mathbf{W}^L)^T \delta^L$$
> - 这里的转置 $(\mathbf{W}^L)^T$ 是为了匹配维度，将误差正确分配给前一层的每个神经元。

### 第 4 步: 计算隐藏层梯度

> [!caution] 注意：必须穿过激活函数！
> 误差信号在进入上一层后，需要先“穿过”上一层的激活函数。
> $$\delta^{L-1} = \frac{\partial E}{\partial \mathbf{z}^{L-1}} = \frac{\partial E}{\partial \mathbf{a}^{L-1}} \cdot \frac{\partial \mathbf{a}^{L-1}}{\partial \mathbf{z}^{L-1}} = ((\mathbf{W}^L)^T \delta^L) \odot \sigma'(\mathbf{z}^{L-1})$$
> - $\odot$: 按元素乘积 (Hadamard Product)。
> - $\sigma'(\mathbf{z}^{L-1})$: 隐藏层激活函数的导数。

### 第 5 步: 计算隐藏层权重梯度

> [!success] 递归公式
> 一旦得到了隐藏层的误差信号 $\delta^{L-1}$，计算其权重梯度的方式与输出层**完全相同**。
> $$\frac{\partial E}{\partial \mathbf{W}^{L-1}} = \delta^{L-1} (\mathbf{a}^{L-2})^T$$
> - $\mathbf{a}^{L-2}$: 更前一层的激活输出 (或者是网络的原始输入 $\mathbf{x}$)。

---

## Ⅲ. 核心符号定义

> [!summary] TL;DR - 符号速查表
| 符号 | 完整名称 | 含义解释 | 计算方式 |
| :--- | :--- | :--- | :--- |
| $\mathbf{W}^L$ | **输出层权重矩阵** | 连接倒数第二层和输出层的“知识”或“判断标准”。 | 训练得到 |
| $\mathbf{z}^L$ | **输出层线性输出** | 激活函数作用前的“原始分数”或“草稿判决”。 | $\mathbf{z}^L = \mathbf{W}^L \mathbf{a}^{L-1} + \mathbf{b}^L$ |
| $\mathbf{a}^L$ | **输出层激活输出** | 网络的**最终预测结果**，即“最终判决书”。 | $\mathbf{a}^L = \sigma(\mathbf{z}^L)$ |

> [!check] 关系流程
> **前层输出 $\mathbf{a}^{L-1}$** $\xrightarrow{\text{加权求和}}$ **线性输出 $\mathbf{z}^L$** $\xrightarrow{\text{激活函数}}$ **最终预测 $\mathbf{a}^L$**

---

---

---
# 🌊 深度反向传播：一个多层神经网络的完整示例

> [!note] 目标
> 为我们之前讨论的“2个输入 -> 2个隐藏层 -> 1个输出”的网络，执行一次完整的反向传播计算。我们将计算出网络中 **所有** 权重和偏置的梯度。

---

## 🏗️ 第 0 步：场景回顾与设定

> [!example] 网络、数据与前向传播结果
>
> **1. 网络结构与激活函数**
> - 结构: Input(2) -> Hidden1(2) -> Hidden2(2) -> Output(1)
> - 隐藏层激活: **ReLU** (`max(0, x)`)
> - 输出层激活: **线性** (`f(x) = x`)
>
> **2. 初始权重与偏置**
> (与前例完全相同)
>
> **3. 前向传播回顾**
> - 输入 `X = [1.0, 2.0]`
> - 第1隐藏层输出 `A_h1 = [0.6, 1.2]` (其加权输入为 `Z_h1 = [0.6, 1.2]`)
> - 第2隐藏层输出 `A_h2 = [1.22, 1.58]` (其加权输入为 `Z_h2 = [1.22, 1.58]`)
> - 最终预测输出 `A_o = 2.978`
>
> **4. 设定目标值 (必须!)**
> - 为了计算误差，我们必须有一个“正确答案”。我们设定本次训练的目标值是：
> - **`Y_true = 3.5`**

> [!info] 需要用到的导数
> - **线性激活函数的导数**: 永远是 `1`。
> - **ReLU激活函数的导数**: 如果输入 `x > 0`，导数为 `1`；如果 `x <= 0`，导数为 `0`。

---

## ⬅️ 第 1 步：从输出层开始，反向计算

### 1.1 计算输出层的梯度

> [!abstract] 任务
> 计算 `W_o` 和 `b_o` 的梯度。这是反向传播的起点。

1.  **计算初始误差信号 `delta_o`**
    `delta_o = (预测值 A_o - 真实值 Y_true) * 线性激活的导数`
    `delta_o = (2.978 - 3.5) * 1`
    `delta_o = -0.522`

2.  **计算 `W_o` 的梯度**
    `Gradient of W_o = delta_o * A_h2的转置`
    `Gradient of W_o = -0.522 * [1.22, 1.58]`
    > [!success] 输出层权重梯度
    > ```
    > # Gradient of W_o
    > [-0.6368, -0.8248]
    > ```

3.  **计算 `b_o` 的梯度**
    `Gradient of b_o = delta_o`
    > [!success] 输出层偏置梯度
    > ```
    > # Gradient of b_o
    > -0.522
    > ```

---

## ⬅️ 第 2 步：将误差传播到第2隐藏层

### 2.1 计算第2隐藏层的梯度

> [!abstract] 任务
> 将来自输出层的误差信号 `delta_o` 向后传递，计算 `W_h2` 和 `b_h2` 的梯度。

1.  **将误差信号反向传播**
    `传入h2的误差 = W_o的转置 * delta_o`
    `传入h2的误差 = [0.9] * -0.522 = [-0.4698]`
                     `[1.0]           [-0.5220]`

2.  **计算 `delta_h2` (穿过ReLU的导数)**
    `delta_h2 = (传入h2的误差) * ReLU'(Z_h2)`
    - 首先，`Z_h2 = [1.22, 1.58]`，两个值都大于0，所以 `ReLU'(Z_h2) = [1, 1]`。
    - `delta_h2 = [-0.4698] * [1] = [-0.4698]`
                   `[-0.5220]   [1]   [-0.5220]`
    (这里的乘法是按元素相乘)

3.  **计算 `W_h2` 的梯度**
    `Gradient of W_h2 = delta_h2 * A_h1的转置`
    `Gradient of W_h2 = [-0.4698] * [0.6, 1.2]`
                     `[-0.5220]`
    > [!success] 第2隐藏层权重梯度
    > ```
    > # Gradient of W_h2
    > [(-0.4698 * 0.6), (-0.4698 * 1.2)] = [-0.2819, -0.5638]
    > [(-0.5220 * 0.6), (-0.5220 * 1.2)] = [-0.3132, -0.6264]
    > ```

4.  **计算 `b_h2` 的梯度**
    `Gradient of b_h2 = delta_h2`
    > [!success] 第2隐藏层偏置梯度
    > ```
    > # Gradient of b_h2
    > [-0.4698]
    > [-0.5220]
    > ```

---

## ⬅️ 第 3 步：将误差继续传播到第1隐藏层

### 3.1 计算第1隐藏层的梯度

> [!abstract] 任务
> 将来自第2隐藏层的误差信号 `delta_h2` 继续向后传递，计算 `W_h1` 和 `b_h1` 的梯度。

1.  **将误差信号再次反向传播**
    `传入h1的误差 = W_h2的转置 * delta_h2`
    `传入h1的误差 = [0.5, 0.7] * [-0.4698]`
                     `[0.6, 0.8]   [-0.5220]`
    `传入h1的误差 = [(0.5 * -0.4698) + (0.7 * -0.5220)] = [-0.2349 - 0.3654] = [-0.6003]`
                     `[(0.6 * -0.4698) + (0.8 * -0.5220)] = [-0.2819 - 0.4176] = [-0.6995]`

2.  **计算 `delta_h1` (穿过ReLU的导数)**
    `delta_h1 = (传入h1的误差) * ReLU'(Z_h1)`
    - 首先，`Z_h1 = [0.6, 1.2]`，两个值都大于0，所以 `ReLU'(Z_h1) = [1, 1]`。
    - `delta_h1 = [-0.6003] * [1] = [-0.6003]`
                   `[-0.6995]   [1]   [-0.6995]`

3.  **计算 `W_h1` 的梯度**
    `Gradient of W_h1 = delta_h1 * X的转置`
    `Gradient of W_h1 = [-0.6003] * [1.0, 2.0]`
                     `[-0.6995]`
    > [!success] 第1隐藏层权重梯度
    > ```
    > # Gradient of W_h1
    > [(-0.6003 * 1.0), (-0.6003 * 2.0)] = [-0.6003, -1.2006]
    > [(-0.6995 * 1.0), (-0.6995 * 2.0)] = [-0.6995, -1.3990]
    > ```

4.  **计算 `b_h1` 的梯度**
    `Gradient of b_h1 = delta_h1`
    > [!success] 第1隐藏层偏置梯度
    > ```
    > # Gradient of b_h1
    > [-0.6003]
    > [-0.6995]
    > ```

---

## ✨ 总结：我们得到了什么？

> [!done] 梯度计算完毕！
> 我们已经成功计算出了网络中 **所有** 参数的梯度。这些梯度告诉我们，为了减小误差（让预测值 `2.978` 更接近目标值 `3.5`），每个权重和偏置应该朝着哪个方向、以多大的“强度”进行调整。
>
> - **`Gradient of W_o`**: `[-0.6368, -0.8248]`
> - **`Gradient of b_o`**: `-0.522`
> - **`Gradient of W_h2`**: `[[-0.2819, -0.5638], [-0.3132, -0.6264]]`
> - **`Gradient of b_h2`**: `[-0.4698, -0.5220]`
> - **`Gradient of W_h1`**: `[[-0.6003, -1.2006], [-0.6995, -1.3990]]`
> - **`Gradient of b_h1`**: `[-0.6003, -0.6995]`
>
> 接下来，优化器（如梯度下降）会使用这些梯度和学习率来更新原始的权重，完成一次学习周期。